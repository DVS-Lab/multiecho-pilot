{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb61258-5769-499a-9538-54db6f4ec030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial separation:\n",
      "- SP subjects (spSubs=1): 14 subjects\n",
      "- Main subjects (spSubs=0): 50 subjects\n",
      "\n",
      "=== MAIN POPULATION (spSubs=0) - ALL SUBJECTS ===\n",
      "Total N (sum of OpenNeuro): 50\n",
      "Age: Mean = 28.8, SD = 13.4\n",
      "\n",
      "=== MAIN POPULATION (spSubs=0) - AFTER REMOVING SixRuns=0 ===\n",
      "Total N (sum of OpenNeuro): 41\n",
      "Age: Mean = 27.4, SD = 10.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects removed due to SixRuns=0: 9\n",
      "Final sample size: 41 subjects\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('~/Documents/GitHub/multiecho-pilot/code/multiecho-pilot_Demographics.xlsx')\n",
    "\n",
    "# Separate data based on spSubs column\n",
    "sp_df = df[df['spSubs'] == 1].copy()\n",
    "main_df = df[df['spSubs'] == 0].copy()\n",
    "\n",
    "print(f\"Initial separation (remove sp subs from main df):\")\n",
    "print(f\"- SP subjects (spSubs=1): {len(sp_df)} subjects\")\n",
    "print(f\"- Main subjects (spSubs=0): {len(main_df)} subjects\")\n",
    "print()\n",
    "\n",
    "# Print statistics for main_df (all subjects with spSubs=0)\n",
    "print(\"=== MAIN POPULATION (spSubs=0) - ALL SUBJECTS ===\")\n",
    "total_n = main_df['OpenNeuro'].sum()\n",
    "mean_age = main_df['Age'].mean()\n",
    "sd_age = main_df['Age'].std()\n",
    "\n",
    "print(f\"Total N (sum of OpenNeuro): {total_n}\")\n",
    "print(f\"Age: Mean = {mean_age:.1f}, SD = {sd_age:.1f}\")\n",
    "print()\n",
    "\n",
    "# Remove subjects with SixRuns = 0\n",
    "main_df_filtered = main_df[main_df['SixRuns'] != 0].copy()\n",
    "\n",
    "print(\"=== MAIN POPULATION (spSubs=0) - AFTER REMOVING SixRuns=0 ===\")\n",
    "filtered_n = main_df_filtered['OpenNeuro'].sum()\n",
    "filtered_mean_age = main_df_filtered['Age'].mean()\n",
    "filtered_sd_age = main_df_filtered['Age'].std()\n",
    "\n",
    "print(f\"Total N (sum of OpenNeuro): {filtered_n}\")\n",
    "print(f\"Age: Mean = {filtered_mean_age:.1f}, SD = {filtered_sd_age:.1f}\")\n",
    "print()\n",
    "\n",
    "# Create histogram of age for filtered main_df\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(main_df_filtered['Age'], bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Distribution - Main Population (After SixRuns Filter)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Subjects removed due to SixRuns=0: {len(main_df) - len(main_df_filtered)}\")\n",
    "print(f\"Final sample size: {len(main_df_filtered)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8364b42-eaf4-4357-b707-398243295833",
   "metadata": {},
   "source": [
    "# Smoothness Analysis Kernels Overview (Figure 3)\n",
    "### Complete Subjects Identification:\n",
    "Loads and processes a smoothness CSV to identify subjects with complete data for all six acquisition combinations, assigning headcoils and saving a table to CSV.\n",
    "### Statistical Analysis with LME:\n",
    "Filters complete subjects, fits an LME model for smoothness by headcoil, multiband, and multi-echo, and saves an APA-style ANOVA table to CSV.\n",
    "### Inline Smoothness Bar Plots:\n",
    "Generates inline Jupyter bar plots for smoothness by headcoil, multiband, and multi-echo using preprocessed data, with TSNR-style formatting, saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32de6170-541f-4a85-8e87-eccc18110f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects with Complete Data for All 6 Acquisitions (mb1me1, mb3me1, mb6me1, mb1me4, mb3me4, mb6me4):\n",
      "Total N: 41\n",
      "\n",
      "N per Headcoil:\n",
      "  Headcoil 20: 17\n",
      "  Headcoil 64: 24\n",
      "\n",
      "Found 41 subjects with complete data:\n",
      "['10017', '10024', '10035', '10041', '10043', '10054', '10059', '10069', '10074', '10078', '10080', '10085', '10094', '10108', '10125', '10130', '10136', '10137', '10142', '10150', '10154', '10166', '10185', '10186', '10188', '10203', '10221', '10223', '10234', '10296', '10318', '10319', '10320', '10321', '10363', '10382', '10391', '10416', '10422', '10438', '12042']\n",
      "\n",
      "Smoothness Values for Complete Subjects (with Headcoil):\n",
      "        headcoil  mb1me1  mb3me1  mb6me1  mb1me4  mb3me4  mb6me4\n",
      "subject                                                         \n",
      "10017         64   3.892   3.837   3.757   4.181   4.161   4.040\n",
      "10024         64   4.017   3.941   3.874   4.338   4.240   4.086\n",
      "10035         64   3.932   3.860   3.798   4.196   4.093   4.093\n",
      "10041         64   4.150   4.012   3.940   4.556   4.507   4.302\n",
      "10043         64   3.991   3.880   3.835   4.359   4.185   4.136\n",
      "10054         64   3.969   3.839   3.841   4.368   4.309   4.086\n",
      "10059         64   4.023   3.937   3.889   4.327   4.276   4.119\n",
      "10069         64   3.630   3.612   3.521   3.896   3.960   3.768\n",
      "10074         64   3.637   3.558   3.533   3.926   3.822   3.750\n",
      "10078         64   3.816   3.723   3.675   4.186   4.058   3.934\n",
      "10080         64   4.027   3.878   3.781   4.366   4.318   4.090\n",
      "10085         64   3.846   3.760   3.768   4.210   4.153   3.974\n",
      "10094         64   3.965   3.856   3.784   4.316   4.291   4.068\n",
      "10108         64   3.993   3.904   3.826   4.301   4.211   4.362\n",
      "10125         64   4.207   4.075   4.017   4.550   4.408   4.337\n",
      "10130         64   3.701   3.700   3.614   4.040   4.038   3.991\n",
      "10136         64   4.019   3.917   3.866   4.337   4.277   4.157\n",
      "10137         64   3.903   3.798   3.743   4.215   4.189   4.054\n",
      "10142         64   3.923   3.943   3.777   4.219   4.180   4.145\n",
      "10150         64   3.858   3.798   3.674   4.136   4.089   3.952\n",
      "10154         64   3.608   3.520   3.467   3.897   3.764   3.710\n",
      "10166         20   3.957   3.850   3.822   4.238   4.161   4.221\n",
      "10185         20   3.598   3.545   3.448   3.763   3.740   3.621\n",
      "10186         64   3.804   3.709   3.624   4.100   4.073   3.965\n",
      "10188         64   3.836   3.767   3.646   4.085   4.034   4.074\n",
      "10203         20   3.616   3.504   3.447   3.835   3.804   3.754\n",
      "10221         64   3.878   3.758   3.683   4.217   4.109   4.046\n",
      "10223         20   3.926   3.829   3.794   4.246   4.177   4.040\n",
      "10234         20   3.766   3.659   3.586   4.171   4.060   3.973\n",
      "10296         20   4.078   3.991   3.866   4.334   4.301   4.299\n",
      "10318         20   3.749   3.678   3.630   3.989   3.893   3.800\n",
      "10319         20   3.878   3.790   3.695   4.205   4.021   4.018\n",
      "10320         20   3.863   3.776   3.677   4.116   4.017   3.919\n",
      "10321         20   3.653   3.540   3.474   3.843   3.777   3.680\n",
      "10363         20   3.747   3.664   3.551   3.995   3.859   3.756\n",
      "10382         20   3.923   3.839   3.804   4.202   4.262   4.120\n",
      "10391         20   3.759   3.668   3.543   3.973   3.919   3.814\n",
      "10416         20   3.720   3.605   3.643   4.010   3.938   3.821\n",
      "10422         20   3.736   3.653   3.632   4.001   3.974   3.808\n",
      "10438         20   3.876   3.837   3.777   4.214   4.131   4.063\n",
      "12042         20   3.973   3.815   3.776   4.241   4.203   4.073\n"
     ]
    }
   ],
   "source": [
    "# Load and process smoothness data to identify complete subjects\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning from pandas if needed\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"DataFrame.applymap has been deprecated\"\n",
    ")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all-zero.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Load and process the CSV file\n",
    "try:\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    data = data.rename(columns={\n",
    "        data.columns[0]: 'path',\n",
    "        'Unnamed: 3': 'smoothness'\n",
    "    })\n",
    "    \n",
    "    # Apply shift up procedure to align smoothness values with correct acquisition\n",
    "    data['file_path'] = data['path'].shift(1)\n",
    "    \n",
    "    # Filter rows with non-null smoothness and file_path\n",
    "    data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "    \n",
    "    # Extract subject, mb, and me from file_path\n",
    "    def parse_path(path):\n",
    "        try:\n",
    "            if not isinstance(path, str):\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "            sub_match = re.search(r'sub-(\\d+)', path)\n",
    "            acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "            subject = sub_match.group(1) if sub_match else None\n",
    "            acq = acq_match.group(1) if acq_match else None\n",
    "            if acq:\n",
    "                mb = acq[:3]  # e.g., mb1\n",
    "                me = acq[3:]  # e.g., me1\n",
    "            else:\n",
    "                mb = None\n",
    "                me = None\n",
    "            return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "        except Exception as e:\n",
    "            return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "    \n",
    "    parsed_data = data['file_path'].apply(parse_path)\n",
    "    data = pd.concat([data, parsed_data], axis=1)\n",
    "    \n",
    "    # Assign headcoil\n",
    "    data['headcoil'] = data['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20' if x else None)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    data = data[['subject', 'headcoil', 'mb', 'me', 'smoothness']]\n",
    "    \n",
    "    # Convert to categorical\n",
    "    data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "    data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "    data['subject'] = data['subject'].astype(str)\n",
    "    \n",
    "    # Filter out invalid rows\n",
    "    data = data.dropna(subset=['subject', 'mb', 'me', 'smoothness'])\n",
    "    data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "    data = data[data['subject'] != 'nan']\n",
    "    \n",
    "    # Create a combined mb_me column for pivoting\n",
    "    data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "    \n",
    "    # Pivot the data to create a table with subjects as rows and mb_me combinations as columns\n",
    "    pivot_table = data.pivot_table(\n",
    "        values='smoothness',\n",
    "        index='subject',\n",
    "        columns='mb_me',\n",
    "        aggfunc='mean'  # In case of duplicates, take the mean\n",
    "    )\n",
    "    \n",
    "    # Ensure all expected mb_me combinations are present as columns\n",
    "    expected_columns = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    pivot_table = pivot_table.reindex(columns=expected_columns)\n",
    "    \n",
    "    # Identify subjects with no NaN values across all mb_me columns\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    # Sort the index (subjects) for consistency\n",
    "    complete_subjects = complete_subjects.sort_index()\n",
    "    \n",
    "    # Round smoothness values to 3 decimal places for readability\n",
    "    complete_subjects = complete_subjects.round(3)\n",
    "    \n",
    "    # Get headcoil information for complete subjects\n",
    "    headcoil_data = data[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "    complete_headcoil = headcoil_data.loc[complete_subjects.index].astype(str)\n",
    "    \n",
    "    # Calculate total N and N per headcoil\n",
    "    total_n = len(complete_subjects)\n",
    "    headcoil_counts = complete_headcoil['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "    \n",
    "    # Add headcoil as a column to the complete subjects table\n",
    "    complete_subjects = complete_subjects.reset_index().merge(\n",
    "        complete_headcoil[['headcoil']].reset_index(),\n",
    "        on='subject',\n",
    "        how='left'\n",
    "    ).set_index('subject')\n",
    "    \n",
    "    # Reorder columns to have headcoil first\n",
    "    cols = ['headcoil'] + expected_columns\n",
    "    complete_subjects = complete_subjects[cols]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Subjects with Complete Data for All 6 Acquisitions (mb1me1, mb3me1, mb6me1, mb1me4, mb3me4, mb6me4):\")\n",
    "    print(f\"Total N: {total_n}\")\n",
    "    print(\"\\nN per Headcoil:\")\n",
    "    print(f\"  Headcoil 20: {headcoil_counts['20']}\")\n",
    "    print(f\"  Headcoil 64: {headcoil_counts['64']}\")\n",
    "    \n",
    "    if complete_subjects.empty:\n",
    "        print(\"\\nNo subjects have complete data for all 6 acquisitions.\")\n",
    "    else:\n",
    "        print(f\"\\nFound {total_n} subjects with complete data:\")\n",
    "        print(complete_subjects.index.tolist())\n",
    "        print(\"\\nSmoothness Values for Complete Subjects (with Headcoil):\")\n",
    "        print(complete_subjects.to_string())\n",
    "    \n",
    "    # Save the complete subjects table to a CSV file for future use\n",
    "    complete_subjects.to_csv('complete_subjects_smoothness_with_headcoil.csv')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da255ba-e9b7-49e4-9fd7-285f01d99974",
   "metadata": {},
   "source": [
    "## Pre-smoothing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58b700-d097-40bb-8175-94b94b379a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LME analysis on complete subjects with shift-up corrected data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pymer4.models import Lmer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load the complete subjects data from the previous kernel\n",
    "try:\n",
    "    complete_subjects_table = pd.read_csv('complete_subjects_smoothness_with_headcoil.csv', index_col=0)\n",
    "    complete_subject_list = [str(subj) for subj in complete_subjects_table.index.tolist()]  # Ensure strings\n",
    "    print(f\"Loaded {len(complete_subject_list)} complete subjects for analysis\")\n",
    "    print(f\"Complete subjects (first 10): {complete_subject_list[:10]}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please run the data processing kernel first to generate complete_subjects_smoothness_with_headcoil.csv\")\n",
    "    exit()\n",
    "\n",
    "# Reload and process the original data with shift-up correction, filtering to complete subjects only\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all-zero.csv')\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    data = data.rename(columns={\n",
    "        data.columns[0]: 'path',\n",
    "        'Unnamed: 3': 'smoothness'\n",
    "    })\n",
    "    \n",
    "    # Apply shift up procedure to align smoothness values with correct acquisition\n",
    "    data['file_path'] = data['path'].shift(1)\n",
    "    \n",
    "    # Filter rows with non-null smoothness and file_path\n",
    "    data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "    print(f\"After filtering non-null: {len(data)} rows\")\n",
    "    \n",
    "    # Extract subject, mb, and me from file_path\n",
    "    def parse_path(path):\n",
    "        try:\n",
    "            if not isinstance(path, str):\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "            sub_match = re.search(r'sub-(\\d+)', path)\n",
    "            acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "            subject = sub_match.group(1) if sub_match else None\n",
    "            acq = acq_match.group(1) if acq_match else None\n",
    "            if acq:\n",
    "                mb = acq[:3]  # e.g., mb1\n",
    "                me = acq[3:]  # e.g., me1\n",
    "            else:\n",
    "                mb = None\n",
    "                me = None\n",
    "            return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "        except Exception as e:\n",
    "            return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "    \n",
    "    parsed_data = data['file_path'].apply(parse_path)\n",
    "    data = pd.concat([data, parsed_data], axis=1)\n",
    "    print(f\"After parsing: {len(data)} rows\")\n",
    "    print(f\"Unique subjects found: {data['subject'].dropna().unique()[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Filter to only complete subjects\n",
    "    print(f\"Parsed subjects (first 10): {data['subject'].dropna().unique()[:10].tolist()}\")\n",
    "    print(f\"Complete subjects (first 10): {complete_subject_list[:10]}\")\n",
    "    data = data[data['subject'].isin(complete_subject_list)]\n",
    "    print(f\"After filtering to complete subjects: {len(data)} rows\")\n",
    "    \n",
    "    # Assign headcoil information\n",
    "    HEADCOIL_64_SUBJECTS = [\n",
    "        \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "        \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "        \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "        \"10186\", \"10188\", \"10221\"\n",
    "    ]\n",
    "    data['headcoil'] = data['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20' if x else None)\n",
    "    \n",
    "    # Select relevant columns and clean data\n",
    "    data = data[['subject', 'headcoil', 'mb', 'me', 'smoothness']].dropna()\n",
    "    print(f\"After selecting columns and dropping NA: {len(data)} rows\")\n",
    "    data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "    data = data[data['subject'] != 'nan']\n",
    "    print(f\"After filtering out invalid subjects: {len(data)} rows\")\n",
    "    \n",
    "    # Convert to appropriate data types\n",
    "    data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "    data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "    data['subject'] = data['subject'].astype(str)\n",
    "    \n",
    "    # Prepare data for LME model with sum-to-zero contrasts\n",
    "    data_model = data.copy()\n",
    "    data_model['headcoil'] = data_model['headcoil'].cat.codes - 0.5  # 20=-0.5, 64=0.5\n",
    "    \n",
    "    print(f\"Running LME analysis on {len(data_model)} observations from {data_model['subject'].nunique()} subjects\")\n",
    "    \n",
    "    # Fit the LME model\n",
    "    model = Lmer('smoothness ~ headcoil * mb * me + (1 | subject)', data=data_model)\n",
    "    model.fit()\n",
    "    \n",
    "    # Get ANOVA table\n",
    "    anova_table = model.anova()\n",
    "    \n",
    "    # Define effect names and numerator df for APA table\n",
    "    effect_map = {\n",
    "        'headcoil': 'Head Coil',\n",
    "        'mb': 'Multiband',\n",
    "        'me': 'Multi-echo',\n",
    "        'headcoil:mb': 'Head Coil × Multiband',\n",
    "        'headcoil:me': 'Head Coil × Multi-echo',\n",
    "        'mb:me': 'Multiband × Multi-echo',\n",
    "        'headcoil:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "    }\n",
    "    \n",
    "    df_dict = {\n",
    "        'Head Coil': 1,\n",
    "        'Multiband': 2,\n",
    "        'Multi-echo': 1,\n",
    "        'Head Coil × Multiband': 2,\n",
    "        'Head Coil × Multi-echo': 1,\n",
    "        'Multiband × Multi-echo': 2,\n",
    "        'Head Coil × Multiband × Multi-echo': 2\n",
    "    }\n",
    "    \n",
    "    # Build APA table\n",
    "    apa_data = []\n",
    "    for effect in anova_table.index:\n",
    "        if effect in ['(Intercept)', 'Residuals']:\n",
    "            continue\n",
    "        effect_name = effect_map.get(effect, effect)\n",
    "        apa_data.append({\n",
    "            'Effect': effect_name,\n",
    "            'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "            'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "            'Num df': df_dict.get(effect_name, np.nan),\n",
    "            'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "            'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "            'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "            'Partial η²': np.nan  # Computed below\n",
    "        })\n",
    "    \n",
    "    # Compute partial eta-squared\n",
    "    try:\n",
    "        residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "    except (KeyError, IndexError):\n",
    "        residual_var = model.ranef_var.iloc[1]['Var']\n",
    "    \n",
    "    n_obs = len(data_model)\n",
    "    n_fixed = sum(df_dict.values())\n",
    "    n_subj = data_model['subject'].nunique()\n",
    "    ss_residual = residual_var * (n_obs - n_fixed - n_subj)\n",
    "    \n",
    "    for i, row in enumerate(apa_data):\n",
    "        ss_effect = row['Sum Sq']\n",
    "        if pd.notna(ss_effect):\n",
    "            apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "    \n",
    "    # Create APA table\n",
    "    apa_table = pd.DataFrame(apa_data)\n",
    "    apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "    apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "    apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "    apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "    apa_table['F'] = apa_table['F'].round(2)\n",
    "    apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "    apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "    print(\"Model: smoothness ~ headcoil * mb * me + (1 | subject)\")\n",
    "    print(f\"Data: Complete subjects only (N = {data_model['subject'].nunique()}), shift-up corrected\\n\")\n",
    "    print(apa_table.to_string(index=False))\n",
    "    \n",
    "    # Save APA table\n",
    "    apa_table.to_csv('smoothness_lme_anova_complete_subjects.csv', index=False)\n",
    "    print(f\"\\nAPA table saved to 'smoothness_lme_anova_complete_subjects.csv'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in LME analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba065b47-d638-4b78-a83c-0b0596e1d792",
   "metadata": {},
   "source": [
    "# Pre-smoothing Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751d086-4a6c-491d-b1c5-1499e1772102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel 3: Generate bar plots using shift-up corrected data from complete subjects\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def load_and_process_shifted_data():\n",
    "    \"\"\"\n",
    "    Load and process the CSV data with shift-up correction, filtering to complete subjects only\n",
    "    \"\"\"\n",
    "    print(\"Loading and processing shift-up corrected data...\")\n",
    "    \n",
    "    # Load complete subjects list\n",
    "    try:\n",
    "        complete_subjects_table = pd.read_csv('complete_subjects_smoothness_with_headcoil.csv', index_col=0)\n",
    "        complete_subject_list = [str(subj) for subj in complete_subjects_table.index.tolist()]\n",
    "        print(f\"Loaded {len(complete_subject_list)} complete subjects for plotting\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Please run kernels 1 and 2 first to generate complete subjects data\")\n",
    "        return None\n",
    "    \n",
    "    # Load and process original data with shift-up correction\n",
    "    csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all-zero.csv')\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift up procedure to align smoothness values with correct acquisition\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        \n",
    "        # Filter rows with non-null smoothness and file_path\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, and me from file_path\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]  # e.g., mb1\n",
    "                    me = acq[3:]  # e.g., me1\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception as e:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        \n",
    "        # Filter to only complete subjects\n",
    "        data = data[data['subject'].isin(complete_subject_list)]\n",
    "        \n",
    "        # Assign headcoil information\n",
    "        HEADCOIL_64_SUBJECTS = [\n",
    "            \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "            \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "            \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "            \"10186\", \"10188\", \"10221\"\n",
    "        ]\n",
    "        data['headcoil'] = data['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20' if x else None)\n",
    "        \n",
    "        # Select relevant columns and clean data\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'smoothness']].dropna()\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        \n",
    "        # Convert to appropriate data types\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        print(f\"Processed data: {len(data)} observations from {data['subject'].nunique()} subjects\")\n",
    "        print(\"Data columns:\", data.columns.tolist())\n",
    "        print(\"Unique headcoil values:\", data['headcoil'].unique())\n",
    "        print(\"Unique mb values:\", data['mb'].unique())\n",
    "        print(\"Unique me values:\", data['me'].unique())\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_data_for_plotting(data, value_column='smoothness'):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard error by multiband, multi-echo, and headcoil\n",
    "    \"\"\"\n",
    "    print(\"\\nProcessing data for plotting...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Debug: Check input data\n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects:\", data['subject'].nunique())\n",
    "    \n",
    "    data = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    agg_data = data.groupby(['mb', 'me', 'headcoil'], observed=True).agg({\n",
    "        value_column: 'mean',\n",
    "        'subject': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    std_error = data.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index()\n",
    "    std_error.columns = ['mb', 'me', 'headcoil', 'se']\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    # Debug: Check processed data\n",
    "    print(\"Processed data shape:\", result.shape)\n",
    "    print(\"Processed data:\\n\", result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_smoothness_bar_plots(data_processed, n_subjects_per_coil, save_files=True):\n",
    "    \"\"\"\n",
    "    Create bar plots for smoothness with identical y-axis scales across subplots\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data is empty. Cannot create plots.\")\n",
    "        return None\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 48})\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())  # Sort for consistency\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    width = 0.4\n",
    "    x1 = [0, 1.2, 2.4]\n",
    "    x2 = [x + width for x in x1]\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6']\n",
    "    \n",
    "    # Collect all y-values and errors to determine consistent y-axis limits\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits\n",
    "    if all_y_values:\n",
    "        #y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        #y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_max = 5\n",
    "        y_min = 3.5\n",
    "        margin = (y_max - y_min) * 0.1  # Add 10% margin\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin)\n",
    "        print(f\"Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1)\n",
    "        print(\"Warning: No valid y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with consistent y-axis\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            print(f\"No data for coil {coil}\")\n",
    "            continue\n",
    "        \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Smoothness', fontsize=48)\n",
    "        \n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05)\n",
    "    \n",
    "    if save_files:\n",
    "        plt.savefig('smoothness_bar_plot_shift_corrected.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Plot saved to 'smoothness_bar_plot_shift_corrected.png'\")\n",
    "    \n",
    "    # Explicitly show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting Kernel 3: Bar plot generation with shift-up corrected data\")\n",
    "\n",
    "# Load the shift-up corrected data for complete subjects only\n",
    "data_corrected = load_and_process_shifted_data()\n",
    "\n",
    "if data_corrected is not None and not data_corrected.empty:\n",
    "    # Compute n_subjects_per_coil from corrected data\n",
    "    n_subjects_per_coil = data_corrected.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "    print(\"Subjects per coil:\", n_subjects_per_coil)\n",
    "    \n",
    "    # Process data for plotting\n",
    "    smoothness_processed = process_data_for_plotting(data_corrected, 'smoothness')\n",
    "    \n",
    "    # Create bar plots\n",
    "    if not smoothness_processed.empty:\n",
    "        fig = create_smoothness_bar_plots(smoothness_processed, n_subjects_per_coil)\n",
    "        print(\"Bar plot generation complete with shift-up corrected data!\")\n",
    "    else:\n",
    "        print(\"Error: Failed to process data for plotting.\")\n",
    "else:\n",
    "    print(\"Error: Failed to load corrected data. Please ensure kernels 1 and 2 have been run successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb25834-2679-4820-a850-ca96e35a13b0",
   "metadata": {},
   "source": [
    "# Post-smoothing loading, stats, and plots\n",
    "Loads data, generates LME stats, and bar plots for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908832c-c890-4c13-9e8a-dd5fe8fc6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Kernel: Complete smoothness analysis pipeline\n",
    "# Processes data, identifies complete subjects, runs LME analysis, and generates plots\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SMOOTHNESS ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Output file names\n",
    "complete_subjects_file = 'complete_subjects_smoothness_all_with_headcoil.csv'\n",
    "anova_table_file = 'smoothness_all_lme_anova_complete_subjects.csv'\n",
    "plot_file = 'smoothness_all_bar_plot_shift_corrected.png'\n",
    "\n",
    "print(f\"Input file: {csv_path}\")\n",
    "print(f\"Processing smoothness-all.csv data...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: IDENTIFYING COMPLETE SUBJECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_and_process_data(csv_path):\n",
    "    \"\"\"Load and process the CSV file with shift-up correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift up procedure to align smoothness values with correct acquisition\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        \n",
    "        # Filter rows with non-null smoothness and file_path\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, and me from file_path\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]  # e.g., mb1\n",
    "                    me = acq[3:]  # e.g., me1\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception as e:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        \n",
    "        # Assign headcoil\n",
    "        data['headcoil'] = data['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20' if x else None)\n",
    "        \n",
    "        # Select relevant columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        # Filter out invalid rows\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    # Create a combined mb_me column for pivoting\n",
    "    data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "    \n",
    "    # Pivot the data to create a table with subjects as rows and mb_me combinations as columns\n",
    "    pivot_table = data.pivot_table(\n",
    "        values='smoothness',\n",
    "        index='subject',\n",
    "        columns='mb_me',\n",
    "        aggfunc='mean'  # In case of duplicates, take the mean\n",
    "    )\n",
    "    \n",
    "    # Ensure all expected mb_me combinations are present as columns\n",
    "    expected_columns = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    pivot_table = pivot_table.reindex(columns=expected_columns)\n",
    "    \n",
    "    # Identify subjects with no NaN values across all mb_me columns\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    # Sort the index (subjects) for consistency\n",
    "    complete_subjects = complete_subjects.sort_index()\n",
    "    \n",
    "    # Round smoothness values to 3 decimal places for readability\n",
    "    complete_subjects = complete_subjects.round(3)\n",
    "    \n",
    "    # Get headcoil information for complete subjects\n",
    "    headcoil_data = data[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "    complete_headcoil = headcoil_data.loc[complete_subjects.index].astype(str)\n",
    "    \n",
    "    # Add headcoil as a column to the complete subjects table\n",
    "    complete_subjects = complete_subjects.reset_index().merge(\n",
    "        complete_headcoil[['headcoil']].reset_index(),\n",
    "        on='subject',\n",
    "        how='left'\n",
    "    ).set_index('subject')\n",
    "    \n",
    "    # Reorder columns to have headcoil first\n",
    "    cols = ['headcoil'] + expected_columns\n",
    "    complete_subjects = complete_subjects[cols]\n",
    "    \n",
    "    return complete_subjects\n",
    "\n",
    "# Load and process data\n",
    "data = load_and_process_data(csv_path)\n",
    "\n",
    "if data is None:\n",
    "    raise Exception(\"Failed to load data. Stopping execution.\")\n",
    "\n",
    "# Identify complete subjects\n",
    "complete_subjects_table = identify_complete_subjects(data)\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_n = len(complete_subjects_table)\n",
    "headcoil_counts = complete_subjects_table['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "# Display results\n",
    "print(\"Subjects with Complete Data for All 6 Acquisitions (mb1me1, mb3me1, mb6me1, mb1me4, mb3me4, mb6me4):\")\n",
    "print(f\"Total N: {total_n}\")\n",
    "print(\"\\nN per Headcoil:\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts['64']}\")\n",
    "\n",
    "if complete_subjects_table.empty:\n",
    "    print(\"\\nNo subjects have complete data for all 6 acquisitions.\")\n",
    "    raise Exception(\"No complete subjects found. Stopping execution.\")\n",
    "else:\n",
    "    print(f\"\\nFound {total_n} subjects with complete data:\")\n",
    "    print(complete_subjects_table.index.tolist())\n",
    "    print(\"\\nSmoothness Values for Complete Subjects (with Headcoil):\")\n",
    "    print(complete_subjects_table.to_string())\n",
    "\n",
    "# Save the complete subjects table\n",
    "complete_subjects_table.to_csv(complete_subjects_file)\n",
    "print(f\"\\nComplete subjects table saved to '{complete_subjects_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter data to complete subjects only\n",
    "complete_subject_list = [str(subj) for subj in complete_subjects_table.index.tolist()]\n",
    "data_complete = data[data['subject'].isin(complete_subject_list)]\n",
    "\n",
    "print(f\"Running LME analysis on {len(data_complete)} observations from {data_complete['subject'].nunique()} subjects\")\n",
    "\n",
    "# Prepare data for LME model with sum-to-zero contrasts\n",
    "data_model = data_complete.copy()\n",
    "data_model['headcoil'] = data_model['headcoil'].cat.codes - 0.5  # 20=-0.5, 64=0.5\n",
    "\n",
    "# Fit the LME model\n",
    "model = Lmer('smoothness ~ headcoil * mb * me + (1 | subject)', data=data_model)\n",
    "model.fit()\n",
    "\n",
    "# Get ANOVA table\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Define effect names and numerator df for APA table\n",
    "effect_map = {\n",
    "    'headcoil': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'headcoil:mb': 'Head Coil × Multiband',\n",
    "    'headcoil:me': 'Head Coil × Multi-echo',\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'Head Coil': 1,\n",
    "    'Multiband': 2,\n",
    "    'Multi-echo': 1,\n",
    "    'Head Coil × Multiband': 2,\n",
    "    'Head Coil × Multi-echo': 1,\n",
    "    'Multiband × Multi-echo': 2,\n",
    "    'Head Coil × Multiband × Multi-echo': 2\n",
    "}\n",
    "\n",
    "# Build APA table\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect)\n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan  # Computed below\n",
    "    })\n",
    "\n",
    "# Compute partial eta-squared\n",
    "try:\n",
    "    residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "except (KeyError, IndexError):\n",
    "    residual_var = model.ranef_var.iloc[1]['Var']\n",
    "\n",
    "n_obs = len(data_model)\n",
    "n_fixed = sum(df_dict.values())\n",
    "n_subj = data_model['subject'].nunique()\n",
    "ss_residual = residual_var * (n_obs - n_fixed - n_subj)\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    if pd.notna(ss_effect):\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "\n",
    "# Create APA table\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "print(\"Model: smoothness ~ headcoil * mb * me + (1 | subject)\")\n",
    "print(f\"Data: Complete subjects only (N = {data_model['subject'].nunique()}), shift-up corrected\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save APA table\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BAR PLOT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def process_data_for_plotting(data, value_column='smoothness'):\n",
    "    \"\"\"Calculate mean and standard error by multiband, multi-echo, and headcoil\"\"\"\n",
    "    print(\"Processing data for plotting...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects:\", data['subject'].nunique())\n",
    "    \n",
    "    data = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    agg_data = data.groupby(['mb', 'me', 'headcoil'], observed=True).agg({\n",
    "        value_column: 'mean',\n",
    "        'subject': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    std_error = data.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index()\n",
    "    std_error.columns = ['mb', 'me', 'headcoil', 'se']\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    print(\"Processed data shape:\", result.shape)\n",
    "    print(\"Processed data:\\n\", result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_smoothness_bar_plots(data_processed, n_subjects_per_coil, save_files=True):\n",
    "    \"\"\"Create bar plots for smoothness with identical y-axis scales across subplots\"\"\"\n",
    "    print(\"Creating bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data is empty. Cannot create plots.\")\n",
    "        return None\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 48})\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    width = 0.4\n",
    "    x1 = [0, 1.2, 2.4]\n",
    "    x2 = [x + width for x in x1]\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6']\n",
    "    \n",
    "    # Collect all y-values and errors to determine consistent y-axis limits\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits\n",
    "    if all_y_values:\n",
    "        #y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        #y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_max = 5\n",
    "        y_min = 3.5\n",
    "        margin = (y_max - y_min) * 0.1\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin)\n",
    "        print(f\"Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1)\n",
    "        print(\"Warning: No valid y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with consistent y-axis\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            print(f\"No data for coil {coil}\")\n",
    "            continue\n",
    "        \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Smoothness', fontsize=48)\n",
    "        \n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05)\n",
    "    \n",
    "    if save_files:\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{plot_file}'\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate plots using complete subjects data\n",
    "n_subjects_per_coil = data_complete.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "print(\"Subjects per coil:\", n_subjects_per_coil)\n",
    "\n",
    "# Process data for plotting\n",
    "smoothness_processed = process_data_for_plotting(data_complete, 'smoothness')\n",
    "\n",
    "# Create bar plots\n",
    "if not smoothness_processed.empty:\n",
    "    fig = create_smoothness_bar_plots(smoothness_processed, n_subjects_per_coil)\n",
    "    print(\"Bar plot generation complete!\")\n",
    "else:\n",
    "    print(\"Error: Failed to process data for plotting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input file: {csv_path}\")\n",
    "print(f\"Complete subjects identified: {total_n}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts['64']}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - {complete_subjects_file}\")\n",
    "print(f\"  - {anova_table_file}\")\n",
    "print(f\"  - {plot_file}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632213b-646d-4705-8bb3-e17bca959444",
   "metadata": {},
   "source": [
    "# TSNR Data Analysis Pipeline (Figure 4)\n",
    "This pipeline analyzes TSNR (Temporal Signal-to-Noise Ratio), investigating how it's affected by MRI acquisition parameters and smoothness as a covariate.\n",
    "## 1. Purpose\n",
    "The pipeline statistically models and visualizes TSNR, examining the impact of multiband (MB), multi-echo (ME), head coil type, and data smoothness.\n",
    "## 2. Input Data\n",
    "- TSNR Data: combined_tsnr_coil_output.csv (contains TSNR values).\n",
    "- Smoothness Data: smoothness-all.csv (contains corresponding smoothness metrics).\n",
    "## 3. Key Analysis Steps\n",
    "### A. Data Preparation\n",
    "  Both TSNR and smoothness datasets are processed. Only common complete subjects (those with all 6 acquisition conditions present in both datasets) are selected for analysis.\n",
    "### B. Linear Mixed Effects (LME) Analysis\n",
    "A statistical model (tsnrMedian ~ headcoil * mb * me + smoothness + (1 | subject)) is fit. This assesses the main and interactive effects of head coil, MB, ME, and the direct influence of smoothness on TSNR, accounting for individual subject variability. An APA-style ANOVA table summarizes the results.\n",
    "### C. Bar Plot Generation\n",
    "Mean TSNR values are calculated and visualized using bar plots. These plots display average TSNR across different acquisition conditions and head coil types, maintaining consistent formatting and y-axis scales for clear comparison.\n",
    "## 4. Generated Output Files\n",
    "- `complete_subjects_tsnr_common_with_headcoil.csv`\n",
    "- `tsnr_lme_anova_complete_subjects_with_smoothness.csv`\n",
    "- `tsnr_bar_plot_median.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062087f-3049-45b9-8b49-0f5aa64b792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process TSNR data to identify complete subjects\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the TSNR CSV file\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/code/combined_tsnr_coil_output.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Load and process the CSV\n",
    "try:\n",
    "    tsnr_data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Ensure consistent string formatting\n",
    "    tsnr_data['Subject'] = tsnr_data['Subject'].astype(str)\n",
    "    tsnr_data['AcquisitionType'] = tsnr_data['AcquisitionType'].astype(str)\n",
    "    \n",
    "    # Extract mb and me from AcquisitionType\n",
    "    tsnr_data['mb'] = tsnr_data['AcquisitionType'].str.extract(r'(mb\\d)')\n",
    "    tsnr_data['me'] = tsnr_data['AcquisitionType'].str.extract(r'(me\\d)')\n",
    "    \n",
    "    # Add headcoil info\n",
    "    tsnr_data['headcoil'] = tsnr_data['Subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20')\n",
    "    \n",
    "    # Create combined mb_me label\n",
    "    tsnr_data['mb_me'] = tsnr_data['mb'] + tsnr_data['me']\n",
    "    \n",
    "    # Expected acquisition types\n",
    "    expected_columns = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    \n",
    "    # Pivot to wide format (using tsnrMedian; change to tsnrMean if needed)\n",
    "    pivot_table = tsnr_data.pivot_table(\n",
    "        values='tsnrMedian',\n",
    "        index='Subject',\n",
    "        columns='mb_me',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Ensure all expected columns are present\n",
    "    pivot_table = pivot_table.reindex(columns=expected_columns)\n",
    "    \n",
    "    # Identify complete subjects\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    complete_subjects = complete_subjects.sort_index()\n",
    "    complete_subjects = complete_subjects.round(3)\n",
    "    \n",
    "    # Add headcoil column\n",
    "    headcoil_df = tsnr_data[['Subject', 'headcoil']].drop_duplicates().set_index('Subject')\n",
    "    complete_headcoil = headcoil_df.loc[complete_subjects.index].astype(str)\n",
    "    \n",
    "    # Count totals\n",
    "    total_n = len(complete_subjects)\n",
    "    headcoil_counts = complete_headcoil['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "    \n",
    "    # Merge headcoil column into complete_subjects\n",
    "    complete_subjects = complete_subjects.reset_index().merge(\n",
    "        complete_headcoil.reset_index(),\n",
    "        on='Subject',\n",
    "        how='left'\n",
    "    ).set_index('Subject')\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = ['headcoil'] + expected_columns\n",
    "    complete_subjects = complete_subjects[cols]\n",
    "    \n",
    "    # Output results\n",
    "    print(\"Subjects with Complete TSNR Data for All 6 Acquisitions:\")\n",
    "    print(f\"Total N: {total_n}\")\n",
    "    print(\"\\nN per Headcoil:\")\n",
    "    print(f\"  Headcoil 20: {headcoil_counts['20']}\")\n",
    "    print(f\"  Headcoil 64: {headcoil_counts['64']}\")\n",
    "    \n",
    "    if complete_subjects.empty:\n",
    "        print(\"\\nNo subjects have complete TSNR data for all 6 acquisitions.\")\n",
    "    else:\n",
    "        print(f\"\\nFound {total_n} subjects with complete TSNR data:\")\n",
    "        print(complete_subjects.index.tolist())\n",
    "        print(\"\\nTSNR Median Values for Complete Subjects (with Headcoil):\")\n",
    "        print(complete_subjects.to_string())\n",
    "    \n",
    "    # Save results\n",
    "    complete_subjects.to_csv('complete_subjects_tsnr_mean_with_headcoil.csv')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing TSNR data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d962f-a36c-4e35-a4de-03c0778c8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Kernel: TSNR analysis pipeline with smoothness covariate and bar plots\n",
    "# Processes TSNR and smoothness data, identifies complete subjects, runs LME analysis, and generates plots\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Suppress common warnings that might clutter output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TSNR ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "# Ensure these paths correctly point to your data files\n",
    "tsnr_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/code/combined_tsnr_coil_output.csv')\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil (as per your provided list)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Output file names for the results\n",
    "complete_subjects_tsnr_file = 'complete_subjects_tsnr_common_with_headcoil.csv'\n",
    "anova_table_file = 'tsnr_lme_anova_complete_subjects_with_smoothness.csv'\n",
    "tsnr_plot_file = 'tsnr_bar_plot_median.png' # New output file for TSNR plot\n",
    "\n",
    "print(f\"TSNR input file: {tsnr_csv_path}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Initiating data processing for TSNR and smoothness...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMMON COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR TSNR AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_and_process_tsnr_data(csv_path, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the TSNR CSV file.\n",
    "    Extracts acquisition parameters (mb, me) and assigns headcoil type.\n",
    "    Ensures 'tsnrMedian' is available for analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tsnr_data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure 'Subject' and 'AcquisitionType' columns are strings for consistent processing\n",
    "        tsnr_data['Subject'] = tsnr_data['Subject'].astype(str)\n",
    "        tsnr_data['AcquisitionType'] = tsnr_data['AcquisitionType'].astype(str)\n",
    "        \n",
    "        # Extract multiband (mb) and multi-echo (me) factors using regular expressions\n",
    "        tsnr_data['mb'] = tsnr_data['AcquisitionType'].str.extract(r'(mb\\d)')\n",
    "        tsnr_data['me'] = tsnr_data['AcquisitionType'].str.extract(r'(me\\d)')\n",
    "        \n",
    "        # Add headcoil info\n",
    "        tsnr_data['headcoil'] = tsnr_data['Subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20'\n",
    "        )\n",
    "        \n",
    "        # Rename 'Subject' column to 'subject' for consistency with smoothness data\n",
    "        tsnr_data = tsnr_data.rename(columns={'Subject': 'subject'})\n",
    "\n",
    "        # Create combined mb_me label BEFORE selecting columns\n",
    "        tsnr_data['mb_me'] = tsnr_data['mb'].astype(str) + tsnr_data['me'].astype(str)\n",
    "        \n",
    "        # Select relevant columns for TSNR analysis (tsnrMean is used for complete subject identification,\n",
    "        # tsnrMedian for the LME as per your request).\n",
    "        # IMPORTANT: Added 'mb_me' to the required_tsnr_cols to fix the KeyError.\n",
    "        required_tsnr_cols = ['subject', 'headcoil', 'mb', 'me', 'mb_me', 'tsnrMean', 'tsnrMedian']\n",
    "        if not all(col in tsnr_data.columns for col in required_tsnr_cols):\n",
    "            missing_cols = [col for col in required_tsnr_cols if col not in tsnr_data.columns]\n",
    "            raise ValueError(f\"Missing required TSNR columns: {missing_cols}. Please ensure 'tsnrMedian' and others are in your CSV.\")\n",
    "            \n",
    "        tsnr_data = tsnr_data[required_tsnr_cols]\n",
    "        \n",
    "        # Convert relevant columns to categorical types for proper statistical modeling\n",
    "        tsnr_data['headcoil'] = pd.Categorical(tsnr_data['headcoil'], categories=['20', '64'])\n",
    "        tsnr_data['mb'] = pd.Categorical(tsnr_data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        tsnr_data['me'] = pd.Categorical(tsnr_data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop any rows with NaN values in the key columns, as these would be incomplete for analysis\n",
    "        tsnr_data = tsnr_data.dropna(subset=['subject', 'headcoil', 'mb', 'me', 'mb_me', 'tsnrMean', 'tsnrMedian'])\n",
    "        \n",
    "        return tsnr_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: TSNR file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Data processing error for TSNR: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing TSNR data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Smoothness CSV file, applying the 'shift-up' correction.\n",
    "    Extracts acquisition parameters and assigns headcoil type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename initial columns for clarity, consistent with the original smoothness kernel\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply the 'shift up' procedure to align smoothness values with their corresponding file paths.\n",
    "        # This is specific to how the smoothness-all.csv file is structured.\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        \n",
    "        # Filter for rows where both smoothness and file_path are valid\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Function to parse subject, multiband, and multi-echo from the file_path\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str): # Handle non-string paths if any\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]  # e.g., mb1\n",
    "                    me = acq[3:]  # e.g., me1\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception: # Catch any parsing errors for a specific path\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        # Apply the parsing function to create new columns\n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "\n",
    "        # Create combined mb_me label BEFORE selecting columns\n",
    "        data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Assign headcoil type\n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        \n",
    "        # Select relevant columns for smoothness analysis\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'mb_me', 'smoothness']] # Added mb_me here too\n",
    "        \n",
    "        # Convert to categorical types and ensure subject is string\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        # Filter out rows with incomplete or invalid subject/acquisition data\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'mb_me', 'smoothness']) # Added mb_me to dropna\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)] # Filter out 'sp' subjects\n",
    "        data = data[data['subject'] != 'nan'] # Filter out 'nan' string subjects if any\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_mb_me):\n",
    "    \"\"\"\n",
    "    Identifies subjects who have complete data (non-null values) for a given metric\n",
    "    across all expected multiband-multi-echo acquisition combinations.\n",
    "    \"\"\"\n",
    "    # Pivot the data to create a table where subjects are rows and mb_me combinations are columns.\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='mb_me', # Use the pre-existing 'mb_me' column\n",
    "        aggfunc='first' \n",
    "    )\n",
    "    \n",
    "    # Ensure all expected acquisition combinations are present as columns.\n",
    "    pivot_table = pivot_table.reindex(columns=expected_mb_me)\n",
    "    \n",
    "    # Identify subjects with no NaN values across all expected acquisition columns.\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "\n",
    "# Define expected acquisition combinations for both TSNR and Smoothness\n",
    "expected_mb_me_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Load and process TSNR data\n",
    "tsnr_data_preprocessed = load_and_process_tsnr_data(tsnr_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if tsnr_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: TSNR data loading failed. Aborting script.\")\n",
    "\n",
    "# Load and process Smoothness data\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "# Identify subjects with complete TSNR data\n",
    "complete_tsnr_subjects = identify_complete_subjects_for_metric(\n",
    "    tsnr_data_preprocessed.copy(), # Use a copy to avoid SettingWithCopyWarning\n",
    "    'tsnrMedian', \n",
    "    expected_mb_me_combinations\n",
    ")\n",
    "print(f\"Subjects with complete TSNR data: {len(complete_tsnr_subjects)}\")\n",
    "\n",
    "# Identify subjects with complete Smoothness data\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(), # Use a copy\n",
    "    'smoothness', \n",
    "    expected_mb_me_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "# Find the intersection of subjects who are complete in *both* datasets\n",
    "common_complete_subjects = list(set(complete_tsnr_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort() # Sort for consistent order\n",
    "print(f\"\\nTotal common complete subjects for combined TSNR and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    print(\"No subjects found with complete data for both TSNR and Smoothness across all acquisitions. Cannot proceed with LME analysis.\")\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "# Filter both TSNR and Smoothness dataframes to include only these common subjects\n",
    "tsnr_data_filtered = tsnr_data_preprocessed[\n",
    "    tsnr_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "# Ensure categorical types are consistent before merging, especially after filtering\n",
    "tsnr_data_filtered['headcoil'] = pd.Categorical(tsnr_data_filtered['headcoil'], categories=['20', '64'])\n",
    "tsnr_data_filtered['mb'] = pd.Categorical(tsnr_data_filtered['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "tsnr_data_filtered['me'] = pd.Categorical(tsnr_data_filtered['me'], categories=['me1', 'me4'])\n",
    "\n",
    "smoothness_data_filtered['headcoil'] = pd.Categorical(smoothness_data_filtered['headcoil'], categories=['20', '64'])\n",
    "smoothness_data_filtered['mb'] = pd.Categorical(smoothness_data_filtered['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "smoothness_data_filtered['me'] = pd.Categorical(smoothness_data_filtered['me'], categories=['me1', 'me4'])\n",
    "\n",
    "\n",
    "# Merge the two filtered dataframes.\n",
    "# Crucially, merge on 'subject', 'mb', and 'me' to ensure that the correct TSNR and smoothness\n",
    "# values are matched for each unique acquisition condition for each subject.\n",
    "merged_analysis_data = pd.merge(\n",
    "    tsnr_data_filtered[['subject', 'headcoil', 'mb', 'me', 'tsnrMedian']],\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner' # Use inner merge to keep only the precisely matched rows\n",
    ")\n",
    "\n",
    "# A final check for any NaNs after merge, which should be minimal if subjects were truly complete\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate and display summary statistics for the final analysis dataset\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "# Save a table of TSNR values for the common complete subjects\n",
    "# This replicates the output from your previous TSNR kernel, but with the refined subject list.\n",
    "# Now 'mb_me' column is guaranteed to be present in tsnr_data_filtered\n",
    "tsnr_complete_table_for_output = tsnr_data_filtered.pivot_table(\n",
    "    values='tsnrMean', # Using tsnrMean for this summary table as in your original TSNR script\n",
    "    index='subject',\n",
    "    columns='mb_me', # This column now exists\n",
    "    aggfunc='mean' # Use mean in case of any duplicate entries from processing (should be unique after dropna)\n",
    ").reindex(columns=expected_mb_me_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "# Add headcoil information to this output table\n",
    "headcoil_df_for_output = tsnr_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "tsnr_complete_table_for_output = tsnr_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[tsnr_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_mb_me_combinations\n",
    "tsnr_complete_table_for_output = tsnr_complete_table_for_output[cols_output]\n",
    "\n",
    "tsnr_complete_table_for_output.to_csv(complete_subjects_tsnr_file)\n",
    "print(f\"\\nComplete TSNR subjects table (for common subjects) saved to '{complete_subjects_tsnr_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "# Prepare data for LME model. Create a copy to avoid modifying the original dataframe\n",
    "# and ensure sum-to-zero contrasts for 'headcoil' as in your reference kernel.\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "\n",
    "# Manually encode 'headcoil' for sum-to-zero contrasts (-0.5 for 20-channel, 0.5 for 64-channel).\n",
    "# Pymer4 handles other categorical variables (mb, me) appropriately for ANOVA.\n",
    "data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# Fit the Linear Mixed Effects model using tsnrMedian as the dependent variable\n",
    "# and including smoothness as a continuous covariate.\n",
    "# Model formula: tsnrMedian ~ headcoil_encoded * mb * me + smoothness + (1 | subject)\n",
    "# (1 | subject) specifies a random intercept for each subject.\n",
    "model = Lmer('tsnrMedian ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "# Retrieve the ANOVA table from the fitted model\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Define mapping for effect names and numerator degrees of freedom (df) for APA style\n",
    "effect_map = {\n",
    "    'headcoil_encoded': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness', # Covariate\n",
    "    'headcoil_encoded:mb': 'Head Coil × Multiband',\n",
    "    'headcoil_encoded:me': 'Head Coil × Multi-echo',\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil_encoded:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "}\n",
    "\n",
    "# Numerator degrees of freedom for each effect (based on factor levels and covariate type)\n",
    "df_dict = {\n",
    "    'Head Coil': 1, # 2 levels (20, 64) - 1\n",
    "    'Multiband': 2, # 3 levels (mb1, mb3, mb6) - 1\n",
    "    'Multi-echo': 1, # 2 levels (me1, me4) - 1\n",
    "    'Smoothness': 1, # Continuous covariate has 1 degree of freedom\n",
    "    'Head Coil × Multiband': 2,  # 1 * 2\n",
    "    'Head Coil × Multi-echo': 1, # 1 * 1\n",
    "    'Multiband × Multi-echo': 2, # 2 * 1\n",
    "    'Head Coil × Multiband × Multi-echo': 2 # 1 * 2 * 1\n",
    "}\n",
    "\n",
    "# Build the APA-style ANOVA table structure\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    # Skip intercept and residuals rows from the pymer4 ANOVA output\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect) # Map to more readable names\n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan  # Placeholder, computed below\n",
    "    })\n",
    "\n",
    "# Compute Partial Eta-Squared ($\\eta_p^2$)\n",
    "# This calculation assumes Type III Sum of Squares, which pymer4 typically uses for ANOVA.\n",
    "# The `residual_var` is the variance of the level 1 residuals (observation-level error).\n",
    "try:\n",
    "    # Attempt to find residual variance by name 'Residual' first\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'] == 'Residual', 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        # Fallback: if 'Residual' is not explicitly named, assume it's the second row (index 1)\n",
    "        # in ranef_var, typically representing the lowest-level variance.\n",
    "        residual_var = model.ranef_var.iloc[1]['Var'] if len(model.ranef_var) > 1 else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan # Set to NaN if unable to find\n",
    "\n",
    "n_obs_lme = len(data_model_for_lme)\n",
    "# Sum of numerator degrees of freedom for all fixed effects (used in ss_residual approximation)\n",
    "n_fixed_df_sum = sum(v for k, v in df_dict.items() if k != 'Residuals') # Exclude residuals df\n",
    "n_subj_lme = data_model_for_lme['subject'].nunique()\n",
    "\n",
    "# Approximate residual sum of squares (SS_error)\n",
    "# This formula is based on the residual variance estimate and the degrees of freedom.\n",
    "# It aligns with the calculation used in your original smoothness kernel.\n",
    "ss_residual = residual_var * (n_obs_lme - n_fixed_df_sum - n_subj_lme) if pd.notna(residual_var) else np.nan\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    # Calculate Partial Eta-Squared: SS_effect / (SS_effect + SS_error)\n",
    "    if pd.notna(ss_effect) and pd.notna(ss_residual) and (ss_effect + ss_residual) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan # Set to NaN if calculation is not possible\n",
    "\n",
    "# Create the final APA table DataFrame and format numerical columns\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA) # Ensure integer type for df\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "# Format p-values: '< .001' for very small p-values, otherwise 3 decimal places\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "# Display the APA-style ANOVA table\n",
    "print(\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "print(\"Model: tsnrMedian ~ headcoil * mb * me + smoothness + (1 | subject)\")\n",
    "print(f\"Data: Common complete subjects only (N = {total_subjects_for_lme} subjects, {len(merged_analysis_data)} observations)\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save the APA table to a CSV file\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BAR PLOT GENERATION FOR TSNR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING BAR PLOTS FOR TSNR MEDIAN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def process_data_for_plotting(data, value_column):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard error by multiband, multi-echo, and headcoil\n",
    "    for a given value_column (e.g., 'tsnrMedian').\n",
    "    \"\"\"\n",
    "    print(f\"Processing data for plotting '{value_column}'...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process for plotting.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects in plotting data:\", data['subject'].nunique())\n",
    "    \n",
    "    # Ensure the value_column and grouping columns exist and are not NaN\n",
    "    data_for_agg = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data_for_agg.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs for plotting. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    agg_data = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True).agg(\n",
    "        mean_value=(value_column, 'mean'),\n",
    "        n_subjects=('subject', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error manually\n",
    "    std_error = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index(name='se')\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    print(\"Processed data shape for plotting:\", result.shape)\n",
    "    print(\"Processed data for plotting (first 5 rows):\\n\", result.head())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_tsnr_bar_plots(data_processed, n_subjects_per_coil, save_files=True):\n",
    "    \"\"\"\n",
    "    Create bar plots for TSNR Median with identical y-axis scales across subplots,\n",
    "    matching the style of the smoothness plots.\n",
    "    \"\"\"\n",
    "    print(\"Creating TSNR bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data for plotting is empty. Cannot create plots.\")\n",
    "        return None\n",
    "        \n",
    "    plt.rcParams.update({'font.size': 48}) # Set global font size\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    # Create subplots, one for each headcoil type\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    # If there's only one subplot, axes will not be an array, so wrap it.\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    width = 0.4 # Bar width\n",
    "    x1 = [0, 1.2, 2.4] # X-coordinates for the first set of bars (e.g., me1)\n",
    "    x2 = [x + width for x in x1] # X-coordinates for the second set of bars (e.g., me4)\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'} # Colors for multi-echo conditions\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6'] # Order of multiband levels for x-axis\n",
    "    \n",
    "    # Collect all y-values (tsnrMedian) and their errors across all data\n",
    "    # to determine a consistent y-axis range for all subplots.\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        # Reindex to ensure all mb_levels are present, filling missing with 0 for plotting\n",
    "        me1_means = me1_data.set_index('mb')['tsnrMedian'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['tsnrMedian'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        # Only add non-zero values/errors to the list for limit calculation\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits to make all plots comparable\n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1 # Add a 10% margin for better visualization\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin) # Ensure lower limit is not negative\n",
    "        print(f\"Calculated consistent Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1) # Default limits if no valid data is found\n",
    "        print(\"Warning: No valid TSNR y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with the consistent y-axis limits\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            # Handle cases where a coil type might have no data after processing\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "            ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "            print(f\"No data for coil {coil}. Skipping plot for this headcoil type.\")\n",
    "            continue\n",
    "            \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')['tsnrMedian'].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')['tsnrMedian'].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        # Plot bars for me1 and me4\n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        # Set x-axis ticks and labels\n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        # Set y-axis label only for the first subplot\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('TSNR Median', fontsize=48)\n",
    "        \n",
    "        # Set title with headcoil type and subject count\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        # Apply the consistent y-axis limits\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    # Add a single legend for both subplots (placed outside the last subplot)\n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    # Adjust layout to prevent labels/titles from overlapping\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85) # Adjust right margin to make space for the legend\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05) # Super x-label for the whole figure\n",
    "    \n",
    "    # Save the plot\n",
    "    if save_files:\n",
    "        plt.savefig(tsnr_plot_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"TSNR plot saved to '{tsnr_plot_file}'\")\n",
    "    \n",
    "    plt.show() # Display the plot\n",
    "    return fig\n",
    "\n",
    "# Generate plots using complete subjects TSNR data\n",
    "# Use tsnr_data_filtered (the TSNR data for common complete subjects) for plotting\n",
    "n_subjects_per_coil_plot = tsnr_data_filtered.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "print(\"Subjects per coil (for TSNR plotting):\", n_subjects_per_coil_plot)\n",
    "\n",
    "# Process data for plotting\n",
    "tsnr_processed_for_plot = process_data_for_plotting(tsnr_data_filtered, 'tsnrMedian')\n",
    "\n",
    "# Create bar plots for TSNR\n",
    "if not tsnr_processed_for_plot.empty:\n",
    "    fig = create_tsnr_bar_plots(tsnr_processed_for_plot, n_subjects_per_coil_plot)\n",
    "    print(\"TSNR bar plot generation complete!\")\n",
    "else:\n",
    "    print(\"Error: Failed to process TSNR data for plotting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"TSNR input file: {tsnr_csv_path}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Total common complete subjects identified and used for analysis: {total_subjects_for_lme}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_for_lme['64']}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - {complete_subjects_tsnr_file}\")\n",
    "print(f\"  - {anova_table_file}\")\n",
    "print(f\"  - {tsnr_plot_file}\") # Include the new plot file in the summary\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2e6d7-23a0-4b0d-836f-c99b206a1567",
   "metadata": {},
   "source": [
    "# TSNR in VS (Figure 5)\n",
    "# NOTICE FOR ALL ROI-Based Analyses! We're missing one 64-ch sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed071ba-3800-43dc-8756-5b60c3ff2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Kernel: TSNR ROI analysis pipeline with smoothness covariate and bar plots\n",
    "# Processes ROI TSNR and smoothness data, identifies common complete subjects,\n",
    "# runs LME analysis with smoothness as covariate, generates ANOVA table, and creates bar plots.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Suppress common warnings that might clutter output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TSNR ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths and parameters for ROI TSNR data extraction\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"tsnr\" # This will be the dependent variable in LME and for plotting\n",
    "MASK_VALUE = \"VSconstrained\" # Specific ROI\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define the path to the whole-brain smoothness CSV file (covariate)\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil (as per your provided list)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Output file names for the results\n",
    "complete_subjects_roi_tsnr_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "roi_tsnr_plot_file = f'{MASK_VALUE}_{IMG_VALUE}_bar_plot.png' # New output file for ROI TSNR plot\n",
    "\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing TSNR for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing for ROI TSNR and smoothness...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMMON COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMMON COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR ROI TSNR AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_tsnr_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Extracts ROI TSNR data from .txt files, parses filenames, and creates a DataFrame.\n",
    "    This function combines logic from your provided reference kernel's data extraction.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for ROI TSNR data...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip 'sp' subjects and files not matching current parameters\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            \n",
    "            # Check if acquisition type is one of the expected ones\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me from acquisition string\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq, # Keep this for pivoting, similar to mb_me\n",
    "                    img_value: value # Use IMG_VALUE as the column name for TSNR\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI TSNR data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Ensure correct data types for categorical variables\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "\n",
    "    # Drop rows with any critical missing data after initial parsing\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    \n",
    "    print(f\"Successfully extracted {len(df)} ROI TSNR data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Smoothness CSV file, applying the 'shift-up' correction.\n",
    "    Extracts acquisition parameters and assigns headcoil type.\n",
    "    (Copied from previous TSNR LME kernel for consistency)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str): \n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str) # Renamed to match ROI tsnr data structure\n",
    "        \n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        \n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        \n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    \"\"\"\n",
    "    Identifies subjects who have complete data (non-null values) for a given metric\n",
    "    across all expected acquisition combinations (acq_combined column).\n",
    "    \"\"\"\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined', # Use 'acq_combined' for pivoting\n",
    "        aggfunc='first' \n",
    "    )\n",
    "    \n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "\n",
    "# Define expected acquisition combinations for both ROI TSNR and Smoothness\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Load and process ROI TSNR data\n",
    "roi_tsnr_data_preprocessed = extract_roi_tsnr_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_tsnr_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI TSNR data loading failed. Aborting script.\")\n",
    "\n",
    "# Load and process Smoothness data\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "# Identify subjects with complete ROI TSNR data\n",
    "complete_roi_tsnr_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_tsnr_data_preprocessed.copy(), \n",
    "    IMG_VALUE, # Use the actual image value column name\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI TSNR data ({MASK_VALUE}): {len(complete_roi_tsnr_subjects)}\")\n",
    "\n",
    "# Identify subjects with complete Smoothness data\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(), \n",
    "    'smoothness', \n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "# --- NEW ADDITION: Displaying complete subject lists side-by-side ---\n",
    "all_unique_subjects = sorted(list(set(complete_roi_tsnr_subjects) | set(complete_smoothness_subjects)))\n",
    "\n",
    "subject_comparison_data = []\n",
    "for sub in all_unique_subjects:\n",
    "    in_roi_tsnr = sub if sub in complete_roi_tsnr_subjects else np.nan\n",
    "    in_smoothness = sub if sub in complete_smoothness_subjects else np.nan\n",
    "    subject_comparison_data.append({\n",
    "        f'{MASK_VALUE} TSNR Subjects': in_roi_tsnr,\n",
    "        'Smoothness Subjects': in_smoothness\n",
    "    })\n",
    "\n",
    "subject_comparison_df = pd.DataFrame(subject_comparison_data)\n",
    "\n",
    "print(\"\\nComparison of Complete Subjects (ROI TSNR vs. Smoothness):\")\n",
    "print(subject_comparison_df.to_string(index=False))\n",
    "# --- END NEW ADDITION ---\n",
    "\n",
    "# Find the intersection of subjects who are complete in *both* datasets\n",
    "common_complete_subjects = list(set(complete_roi_tsnr_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort() \n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} TSNR and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    print(\"No subjects found with complete data for both ROI TSNR and Smoothness across all acquisitions. Cannot proceed with LME analysis.\")\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "# Filter both DataFrames to include only these common subjects\n",
    "roi_tsnr_data_filtered = roi_tsnr_data_preprocessed[\n",
    "    roi_tsnr_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "# Ensure categorical types are consistent after filtering\n",
    "for df in [roi_tsnr_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "# Merge the two filtered dataframes.\n",
    "# Crucially, merge on 'subject', 'mb', and 'me' to ensure that the correct TSNR and smoothness\n",
    "# values are matched for each unique acquisition condition for each subject.\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_tsnr_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]], # Use IMG_VALUE for TSNR column\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner' \n",
    ")\n",
    "\n",
    "# A final check for any NaNs after merge\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate and display descriptive statistics for the final analysis dataset\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "# Generate and save a table of ROI TSNR values for the common complete subjects\n",
    "# This replicates the output from your previous TSNR kernel's complete subject table.\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE, # Use the dynamic IMG_VALUE here\n",
    "    index='subject',\n",
    "    columns='acq_combined', # Use acq_combined for pivoting\n",
    "    aggfunc='mean' \n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "# Add headcoil information to this output table\n",
    "headcoil_df_for_output = roi_tsnr_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_tsnr_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output[cols_output]\n",
    "\n",
    "roi_tsnr_complete_table_for_output.to_csv(complete_subjects_roi_tsnr_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table (for common subjects) saved to '{complete_subjects_roi_tsnr_file}'\")\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "# Prepare data for LME model. Create a copy to avoid modifying the original dataframe\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "\n",
    "# Manually encode 'headcoil' for sum-to-zero contrasts (-0.5 for 20-channel, 0.5 for 64=0.5).\n",
    "data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# Fit the Linear Mixed Effects model\n",
    "# Model formula: tsnr ~ headcoil * mb * me + smoothness + (1 | subject)\n",
    "model = Lmer(f'{IMG_VALUE} ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "# Retrieve the ANOVA table from the fitted model\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Define mapping for effect names and numerator degrees of freedom (df) for APA style\n",
    "effect_map = {\n",
    "    'headcoil_encoded': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness', # Covariate\n",
    "    'headcoil_encoded:mb': 'Head Coil × Multiband',\n",
    "    'headcoil_encoded:me': 'Head Coil × Multi-echo',\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil_encoded:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "}\n",
    "\n",
    "# Numerator degrees of freedom for each effect (based on factor levels and covariate type)\n",
    "df_dict = {\n",
    "    'Head Coil': 1, \n",
    "    'Multiband': 2, \n",
    "    'Multi-echo': 1, \n",
    "    'Smoothness': 1, # Continuous covariate\n",
    "    'Head Coil × Multiband': 2, \n",
    "    'Head Coil × Multi-echo': 1,\n",
    "    'Multiband × Multi-echo': 2,\n",
    "    'Head Coil × Multiband × Multi-echo': 2\n",
    "}\n",
    "\n",
    "# Build the APA-style ANOVA table structure\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect) \n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan  \n",
    "    })\n",
    "\n",
    "# Compute Partial Eta-Squared ($\\eta_p^2$)\n",
    "try:\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'] == 'Residual', 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        residual_var = model.ranef_var.iloc[1]['Var'] if len(model.ranef_var) > 1 else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan \n",
    "\n",
    "n_obs_lme = len(data_model_for_lme)\n",
    "n_fixed_df_sum = sum(v for k, v in df_dict.items() if k != 'Residuals')\n",
    "n_subj_lme = data_model_for_lme['subject'].nunique()\n",
    "\n",
    "ss_residual = residual_var * (n_obs_lme - n_fixed_df_sum - n_subj_lme) if pd.notna(residual_var) else np.nan\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    if pd.notna(ss_effect) and pd.notna(ss_residual) and (ss_effect + ss_residual) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan \n",
    "\n",
    "# Create the final APA table DataFrame and format numerical columns\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "# Display the APA-style ANOVA table\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save the APA table to a CSV file\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BAR PLOT GENERATION FOR ROI TSNR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: GENERATING BAR PLOTS FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def process_data_for_plotting(data, value_column):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard error by multiband, multi-echo, and headcoil\n",
    "    for a given value_column (e.g., 'tsnrMedian').\n",
    "    \"\"\"\n",
    "    print(f\"Processing data for plotting '{value_column}'...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process for plotting.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects in plotting data:\", data['subject'].nunique())\n",
    "    \n",
    "    # Ensure the value_column and grouping columns exist and are not NaN\n",
    "    data_for_agg = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data_for_agg.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs for plotting. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    agg_data = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True).agg(\n",
    "        mean_value=(value_column, 'mean'),\n",
    "        n_subjects=('subject', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error manually\n",
    "    std_error = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index(name='se')\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    print(\"Processed data shape for plotting:\", result.shape)\n",
    "    print(\"Processed data for plotting (first 5 rows):\\n\", result.head())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_tsnr_roi_bar_plots(data_processed, n_subjects_per_coil, plot_value_column_name, plot_file_name, mask_name, save_files=True):\n",
    "    \"\"\"\n",
    "    Create bar plots for ROI TSNR with identical y-axis scales across subplots,\n",
    "    matching the style of the smoothness plots.\n",
    "    \"\"\"\n",
    "    print(f\"Creating {mask_name} {plot_value_column_name} bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data for plotting is empty. Cannot create plots.\")\n",
    "        return None\n",
    "        \n",
    "    plt.rcParams.update({'font.size': 48}) # Set global font size\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    # Create subplots, one for each headcoil type\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    # If there's only one subplot, axes will not be an array, so wrap it.\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    width = 0.4 # Bar width\n",
    "    x1 = [0, 1.2, 2.4] # X-coordinates for the first set of bars (e.g., me1)\n",
    "    x2 = [x + width for x in x1] # X-coordinates for the second set of bars (e.g., me4)\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'} # Colors for multi-echo conditions\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6'] # Order of multiband levels for x-axis\n",
    "    \n",
    "    # Collect all y-values (tsnr) and their errors across all data\n",
    "    # to determine a consistent y-axis range for all subplots.\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        # Reindex to ensure all mb_levels are present, filling missing with 0 for plotting\n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        # Only add non-zero values/errors to the list for limit calculation\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits to make all plots comparable\n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1 # Add a 10% margin for better visualization\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin) # Ensure lower limit is not negative\n",
    "        print(f\"Calculated consistent Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1) # Default limits if no valid data is found\n",
    "        print(f\"Warning: No valid {plot_value_column_name} y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with the consistent y-axis limits\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            # Handle cases where a coil type might have no data after processing\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "            ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "            print(f\"No data for coil {coil}. Skipping plot for this headcoil type.\")\n",
    "            continue\n",
    "            \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        # Plot bars for me1 and me4\n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        # Set x-axis ticks and labels\n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        # Set y-axis label only for the first subplot\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(plot_value_column_name.replace('tsnr', 'TSNR ').replace('Median', ' Median'), fontsize=48)\n",
    "        \n",
    "        # Set title with headcoil type and subject count\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        # Apply the consistent y-axis limits\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    # Add a single legend for both subplots (placed outside the last subplot)\n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    # Adjust layout to prevent labels/titles from overlapping\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85) # Adjust right margin to make space for the legend\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05) # Super x-label for the whole figure\n",
    "    \n",
    "    # Save the plot\n",
    "    if save_files:\n",
    "        plt.savefig(plot_file_name, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{plot_file_name}'\")\n",
    "    \n",
    "    plt.show() # Display the plot\n",
    "    return fig\n",
    "\n",
    "# Generate plots using complete subjects ROI TSNR data\n",
    "# Use roi_tsnr_data_filtered (the TSNR data for common complete subjects) for plotting\n",
    "n_subjects_per_coil_plot = roi_tsnr_data_filtered.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "print(f\"Subjects per coil (for {MASK_VALUE} {IMG_VALUE} plotting):\", n_subjects_per_coil_plot)\n",
    "\n",
    "# Process data for plotting\n",
    "tsnr_roi_processed_for_plot = process_data_for_plotting(roi_tsnr_data_filtered, IMG_VALUE)\n",
    "\n",
    "# Create bar plots for ROI TSNR\n",
    "if not tsnr_roi_processed_for_plot.empty:\n",
    "    fig = create_tsnr_roi_bar_plots(tsnr_roi_processed_for_plot, n_subjects_per_coil_plot, IMG_VALUE, roi_tsnr_plot_file, MASK_VALUE)\n",
    "    print(f\"{MASK_VALUE} {IMG_VALUE} bar plot generation complete!\")\n",
    "else:\n",
    "    print(f\"Error: Failed to process {MASK_VALUE} {IMG_VALUE} data for plotting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Total common complete subjects identified and used for analysis: {total_subjects_for_lme}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_for_lme['64']}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - {complete_subjects_roi_tsnr_file}\")\n",
    "print(f\"  - {anova_table_file}\")\n",
    "print(f\"  - {roi_tsnr_plot_file}\") # Include the new plot file in the summary\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606104e-6583-47c0-a9df-dab426473081",
   "metadata": {},
   "source": [
    "# TSNR in rFFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0fddc-bb4c-42b6-8b2d-ee920e9e63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Kernel: TSNR rFFA ROI analysis pipeline with smoothness covariate and bar plots\n",
    "# Processes rFFA ROI TSNR and smoothness data, identifies common complete subjects,\n",
    "# runs LME analysis with smoothness as covariate, generates ANOVA table, and creates bar plots.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Suppress common warnings that might clutter output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TSNR rFFA ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths and parameters for ROI TSNR data extraction\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"tsnr\" # This will be the dependent variable in LME and for plotting\n",
    "MASK_VALUE = \"rFFA\" # Specific ROI: rFFA\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define the path to the whole-brain smoothness CSV file (covariate)\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil (as per your provided list)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Output file names for the results, updated for rFFA\n",
    "complete_subjects_roi_tsnr_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "roi_tsnr_plot_file = f'{MASK_VALUE}_{IMG_VALUE}_bar_plot.png' # New output file for ROI TSNR plot\n",
    "\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing TSNR for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing for ROI TSNR and smoothness...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMMON COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR {MASK_VALUE} TSNR AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_tsnr_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Extracts ROI TSNR data from .txt files, parses filenames, and creates a DataFrame.\n",
    "    This function combines logic from your provided reference kernel's data extraction.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for ROI TSNR data...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip 'sp' subjects and files not matching current parameters\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            \n",
    "            # Check if acquisition type is one of the expected ones\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me from acquisition string\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq, # Keep this for pivoting, similar to mb_me\n",
    "                    img_value: value # Use IMG_VALUE as the column name for TSNR\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI TSNR data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Ensure correct data types for categorical variables\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "\n",
    "    # Drop rows with any critical missing data after initial parsing\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    \n",
    "    print(f\"Successfully extracted {len(df)} ROI TSNR data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Smoothness CSV file, applying the 'shift-up' correction.\n",
    "    Extracts acquisition parameters and assigns headcoil type.\n",
    "    (Copied from previous TSNR LME kernel for consistency)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str): \n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str) # Renamed to match ROI tsnr data structure\n",
    "        \n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        \n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        \n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    \"\"\"\n",
    "    Identifies subjects who have complete data (non-null values) for a given metric\n",
    "    across all expected acquisition combinations (acq_combined column).\n",
    "    \"\"\"\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined', # Use 'acq_combined' for pivoting\n",
    "        aggfunc='first' \n",
    "    )\n",
    "    \n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "\n",
    "# Define expected acquisition combinations for both ROI TSNR and Smoothness\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Load and process ROI TSNR data\n",
    "roi_tsnr_data_preprocessed = extract_roi_tsnr_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_tsnr_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI TSNR data loading failed. Aborting script.\")\n",
    "\n",
    "# Load and process Smoothness data\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "# Identify subjects with complete ROI TSNR data\n",
    "complete_roi_tsnr_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_tsnr_data_preprocessed.copy(), \n",
    "    IMG_VALUE, # Use the actual image value column name\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI TSNR data ({MASK_VALUE}): {len(complete_roi_tsnr_subjects)}\")\n",
    "\n",
    "# Identify subjects with complete Smoothness data\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(), \n",
    "    'smoothness', \n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "# Find the intersection of subjects who are complete in *both* datasets\n",
    "common_complete_subjects = list(set(complete_roi_tsnr_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort() \n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} TSNR and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    print(\"No subjects found with complete data for both ROI TSNR and Smoothness across all acquisitions. Cannot proceed with LME analysis.\")\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "# Filter both DataFrames to include only these common subjects\n",
    "roi_tsnr_data_filtered = roi_tsnr_data_preprocessed[\n",
    "    roi_tsnr_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "# Ensure categorical types are consistent after filtering\n",
    "for df in [roi_tsnr_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "# Merge the two filtered dataframes.\n",
    "# Crucially, merge on 'subject', 'mb', and 'me' to ensure that the correct TSNR and smoothness\n",
    "# values are matched for each unique acquisition condition for each subject.\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_tsnr_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]], # Use IMG_VALUE for TSNR column\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner' \n",
    ")\n",
    "\n",
    "# A final check for any NaNs after merge\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate and display descriptive statistics for the final analysis dataset\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "# Generate and save a table of ROI TSNR values for the common complete subjects\n",
    "# This replicates the output from your previous TSNR kernel's complete subject table.\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE, # Use the dynamic IMG_VALUE here\n",
    "    index='subject',\n",
    "    columns='acq_combined', # Use acq_combined for pivoting\n",
    "    aggfunc='mean' \n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "# Add headcoil information to this output table\n",
    "headcoil_df_for_output = roi_tsnr_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_tsnr_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output[cols_output]\n",
    "\n",
    "roi_tsnr_complete_table_for_output.to_csv(complete_subjects_roi_tsnr_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table (for common subjects) saved to '{complete_subjects_roi_tsnr_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE FOR {MASK_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "# Prepare data for LME model. Create a copy to avoid modifying the original dataframe\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "\n",
    "# Manually encode 'headcoil' for sum-to-zero contrasts (-0.5 for 20-channel, 0.5 for 64=0.5).\n",
    "data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# Fit the Linear Mixed Effects model\n",
    "# Model formula: tsnr ~ headcoil * mb * me + smoothness + (1 | subject)\n",
    "model = Lmer(f'{IMG_VALUE} ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "# Retrieve the ANOVA table from the fitted model\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Define mapping for effect names and numerator degrees of freedom (df) for APA style\n",
    "effect_map = {\n",
    "    'headcoil_encoded': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness', # Covariate\n",
    "    'headcoil_encoded:mb': 'Head Coil × Multiband',\n",
    "    'headcoil_encoded:me': 'Head Coil × Multi-echo',\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil_encoded:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "}\n",
    "\n",
    "# Numerator degrees of freedom for each effect (based on factor levels and covariate type)\n",
    "df_dict = {\n",
    "    'Head Coil': 1, \n",
    "    'Multiband': 2, \n",
    "    'Multi-echo': 1, \n",
    "    'Smoothness': 1, # Continuous covariate\n",
    "    'Head Coil × Multiband': 2, \n",
    "    'Head Coil × Multi-echo': 1,\n",
    "    'Multiband × Multi-echo': 2,\n",
    "    'Head Coil × Multiband × Multi-echo': 2\n",
    "}\n",
    "\n",
    "# Build the APA-style ANOVA table structure\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect) \n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan  \n",
    "    })\n",
    "\n",
    "# Compute Partial Eta-Squared ($\\eta_p^2$)\n",
    "try:\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'] == 'Residual', 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        residual_var = model.ranef_var.iloc[1]['Var'] if len(model.ranef_var) > 1 else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan \n",
    "\n",
    "n_obs_lme = len(data_model_for_lme)\n",
    "n_fixed_df_sum = sum(v for k, v in df_dict.items() if k != 'Residuals')\n",
    "n_subj_lme = data_model_for_lme['subject'].nunique()\n",
    "\n",
    "ss_residual = residual_var * (n_obs_lme - n_fixed_df_sum - n_subj_lme) if pd.notna(residual_var) else np.nan\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    if pd.notna(ss_effect) and pd.notna(ss_residual) and (ss_effect + ss_residual) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan \n",
    "\n",
    "# Create the final APA table DataFrame and format numerical columns\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "# Display the APA-style ANOVA table\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save the APA table to a CSV file\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BAR PLOT GENERATION FOR ROI TSNR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: GENERATING BAR PLOTS FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def process_data_for_plotting(data, value_column):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard error by multiband, multi-echo, and headcoil\n",
    "    for a given value_column (e.g., 'tsnrMedian').\n",
    "    \"\"\"\n",
    "    print(f\"Processing data for plotting '{value_column}'...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process for plotting.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects in plotting data:\", data['subject'].nunique())\n",
    "    \n",
    "    # Ensure the value_column and grouping columns exist and are not NaN\n",
    "    data_for_agg = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data_for_agg.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs for plotting. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    agg_data = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True).agg(\n",
    "        mean_value=(value_column, 'mean'),\n",
    "        n_subjects=('subject', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error manually\n",
    "    std_error = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index(name='se')\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    print(\"Processed data shape for plotting:\", result.shape)\n",
    "    print(\"Processed data for plotting (first 5 rows):\\n\", result.head())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_tsnr_roi_bar_plots(data_processed, n_subjects_per_coil, plot_value_column_name, plot_file_name, mask_name, save_files=True):\n",
    "    \"\"\"\n",
    "    Create bar plots for ROI TSNR with identical y-axis scales across subplots,\n",
    "    matching the style of the smoothness plots.\n",
    "    \"\"\"\n",
    "    print(f\"Creating {mask_name} {plot_value_column_name} bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data for plotting is empty. Cannot create plots.\")\n",
    "        return None\n",
    "        \n",
    "    plt.rcParams.update({'font.size': 48}) # Set global font size\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    # Create subplots, one for each headcoil type\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    # If there's only one subplot, axes will not be an array, so wrap it.\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    width = 0.4 # Bar width\n",
    "    x1 = [0, 1.2, 2.4] # X-coordinates for the first set of bars (e.g., me1)\n",
    "    x2 = [x + width for x in x1] # X-coordinates for the second set of bars (e.g., me4)\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'} # Colors for multi-echo conditions\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6'] # Order of multiband levels for x-axis\n",
    "    \n",
    "    # Collect all y-values (tsnr) and their errors across all data\n",
    "    # to determine a consistent y-axis range for all subplots.\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        # Reindex to ensure all mb_levels are present, filling missing with 0 for plotting\n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        # Only add non-zero values/errors to the list for limit calculation\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits to make all plots comparable\n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1 # Add a 10% margin for better visualization\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin) # Ensure lower limit is not negative\n",
    "        print(f\"Calculated consistent Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1) # Default limits if no valid data is found\n",
    "        print(f\"Warning: No valid {plot_value_column_name} y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with the consistent y-axis limits\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            # Handle cases where a coil type might have no data after processing\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "            ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "            print(f\"No data for coil {coil}. Skipping plot for this headcoil type.\")\n",
    "            continue\n",
    "            \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        # Plot bars for me1 and me4\n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        # Set x-axis ticks and labels\n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        # Set y-axis label only for the first subplot\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(plot_value_column_name.replace('tsnr', 'TSNR ').replace('Median', ' Median').replace('tsnr', 'TSNR'), fontsize=48)\n",
    "        \n",
    "        # Set title with headcoil type and subject count\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        # Apply the consistent y-axis limits\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    # Add a single legend for both subplots (placed outside the last subplot)\n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    # Adjust layout to prevent labels/titles from overlapping\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85) # Adjust right margin to make space for the legend\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05) # Super x-label for the whole figure\n",
    "    \n",
    "    # Save the plot\n",
    "    if save_files:\n",
    "        plt.savefig(plot_file_name, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{plot_file_name}'\")\n",
    "    \n",
    "    plt.show() # Display the plot\n",
    "    return fig\n",
    "\n",
    "# Generate plots using complete subjects ROI TSNR data\n",
    "n_subjects_per_coil_plot = roi_tsnr_data_filtered.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "print(f\"Subjects per coil (for {MASK_VALUE} {IMG_VALUE} plotting):\", n_subjects_per_coil_plot)\n",
    "\n",
    "# Process data for plotting\n",
    "tsnr_roi_processed_for_plot = process_data_for_plotting(roi_tsnr_data_filtered, IMG_VALUE)\n",
    "\n",
    "# Create bar plots for ROI TSNR\n",
    "if not tsnr_roi_processed_for_plot.empty:\n",
    "    fig = create_tsnr_roi_bar_plots(tsnr_roi_processed_for_plot, n_subjects_per_coil_plot, IMG_VALUE, roi_tsnr_plot_file, MASK_VALUE)\n",
    "    print(f\"{MASK_VALUE} {IMG_VALUE} bar plot generation complete!\")\n",
    "else:\n",
    "    print(f\"Error: Failed to process {MASK_VALUE} {IMG_VALUE} data for plotting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Total common complete subjects identified and used for analysis: {total_subjects_for_lme}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_for_lme['64']}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - {complete_subjects_roi_tsnr_file}\")\n",
    "print(f\"  - {anova_table_file}\")\n",
    "print(f\"  - {roi_tsnr_plot_file}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461fcba-1a08-4ef5-96b1-8e5279eb2c9f",
   "metadata": {},
   "source": [
    "# TSNR in Motor Cortex (beginning Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9c107-3490-4b10-89c5-679632dac442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Kernel: TSNR bilateralMotor ROI analysis pipeline with smoothness covariate and bar plots\n",
    "# Processes bilateralMotor ROI TSNR and smoothness data, identifies common complete subjects,\n",
    "# runs LME analysis with smoothness as covariate, generates ANOVA table, and creates bar plots.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Suppress common warnings that might clutter output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TSNR bilateralMotor ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths and parameters for ROI TSNR data extraction\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"tsnr\" # This will be the dependent variable in LME and for plotting\n",
    "MASK_VALUE = \"bilateralMotor\" # Specific ROI: bilateralMotor\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define the path to the whole-brain smoothness CSV file (covariate)\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "# Define subjects with 64-channel headcoil (as per your provided list)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Output file names for the results, updated for bilateralMotor\n",
    "complete_subjects_roi_tsnr_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "roi_tsnr_plot_file = f'{MASK_VALUE}_{IMG_VALUE}_bar_plot.png' # New output file for ROI TSNR plot\n",
    "\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing TSNR for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing for ROI TSNR and smoothness...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA PROCESSING AND COMMON COMPLETE SUBJECT IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR {MASK_VALUE} TSNR AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_tsnr_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Extracts ROI TSNR data from .txt files, parses filenames, and creates a DataFrame.\n",
    "    This function combines logic from your provided reference kernel's data extraction.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for ROI TSNR data...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip 'sp' subjects and files not matching current parameters\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            \n",
    "            # Check if acquisition type is one of the expected ones\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me from acquisition string\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq, # Keep this for pivoting, similar to mb_me\n",
    "                    img_value: value # Use IMG_VALUE as the column name for TSNR\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI TSNR data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Ensure correct data types for categorical variables\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "\n",
    "    # Drop rows with any critical missing data after initial parsing\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    \n",
    "    print(f\"Successfully extracted {len(df)} ROI TSNR data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Smoothness CSV file, applying the 'shift-up' correction.\n",
    "    Extracts acquisition parameters and assigns headcoil type.\n",
    "    (Copied from previous TSNR LME kernel for consistency)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str): \n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str) # Renamed to match ROI tsnr data structure\n",
    "        \n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        \n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        \n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    \"\"\"\n",
    "    Identifies subjects who have complete data (non-null values) for a given metric\n",
    "    across all expected acquisition combinations (acq_combined column).\n",
    "    \"\"\"\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined', # Use 'acq_combined' for pivoting\n",
    "        aggfunc='first' \n",
    "    )\n",
    "    \n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "\n",
    "# Define expected acquisition combinations for both ROI TSNR and Smoothness\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Load and process ROI TSNR data\n",
    "roi_tsnr_data_preprocessed = extract_roi_tsnr_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_tsnr_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI TSNR data loading failed. Aborting script.\")\n",
    "\n",
    "# Load and process Smoothness data\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "# Identify subjects with complete ROI TSNR data\n",
    "complete_roi_tsnr_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_tsnr_data_preprocessed.copy(), \n",
    "    IMG_VALUE, # Use the actual image value column name\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI TSNR data ({MASK_VALUE}): {len(complete_roi_tsnr_subjects)}\")\n",
    "\n",
    "# Identify subjects with complete Smoothness data\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(), \n",
    "    'smoothness', \n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "# Find the intersection of subjects who are complete in *both* datasets\n",
    "common_complete_subjects = list(set(complete_roi_tsnr_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort() \n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} TSNR and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    print(\"No subjects found with complete data for both ROI TSNR and Smoothness across all acquisitions. Cannot proceed with LME analysis.\")\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "# Filter both DataFrames to include only these common subjects\n",
    "roi_tsnr_data_filtered = roi_tsnr_data_preprocessed[\n",
    "    roi_tsnr_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "# Ensure categorical types are consistent after filtering\n",
    "for df in [roi_tsnr_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "# Merge the two filtered dataframes.\n",
    "# Crucially, merge on 'subject', 'mb', and 'me' to ensure that the correct TSNR and smoothness\n",
    "# values are matched for each unique acquisition condition for each subject.\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_tsnr_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]], # Use IMG_VALUE for TSNR column\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner' \n",
    ")\n",
    "\n",
    "# A final check for any NaNs after merge\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "# Calculate and display descriptive statistics for the final analysis dataset\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "# Generate and save a table of ROI TSNR values for the common complete subjects\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE, # Use the dynamic IMG_VALUE here\n",
    "    index='subject',\n",
    "    columns='acq_combined', # Use acq_combined for pivoting\n",
    "    aggfunc='mean' \n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "# Add headcoil information to this output table\n",
    "headcoil_df_for_output = roi_tsnr_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_tsnr_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_tsnr_complete_table_for_output = roi_tsnr_complete_table_for_output[cols_output]\n",
    "\n",
    "roi_tsnr_complete_table_for_output.to_csv(complete_subjects_roi_tsnr_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table (for common subjects) saved to '{complete_subjects_roi_tsnr_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE FOR {MASK_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "# Prepare data for LME model. Create a copy to avoid modifying the original dataframe\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "\n",
    "# Manually encode 'headcoil' for sum-to-zero contrasts (-0.5 for 20-channel, 0.5 for 64=0.5).\n",
    "data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# Fit the Linear Mixed Effects model\n",
    "# Model formula: tsnr ~ headcoil * mb * me + smoothness + (1 | subject)\n",
    "model = Lmer(f'{IMG_VALUE} ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "# Retrieve the ANOVA table from the fitted model\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Define mapping for effect names and numerator degrees of freedom (df) for APA style\n",
    "effect_map = {\n",
    "    'headcoil_encoded': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness', # Covariate\n",
    "    'headcoil_encoded:mb': 'Head Coil × Multiband',\n",
    "    'headcoil_encoded:me': 'Head Coil × Multi-echo',\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil_encoded:mb:me': 'Head Coil × Multiband × Multi-echo'\n",
    "}\n",
    "\n",
    "# Numerator degrees of freedom for each effect (based on factor levels and covariate type)\n",
    "df_dict = {\n",
    "    'Head Coil': 1, \n",
    "    'Multiband': 2, \n",
    "    'Multi-echo': 1, \n",
    "    'Smoothness': 1, # Continuous covariate\n",
    "    'Head Coil × Multiband': 2, \n",
    "    'Head Coil × Multi-echo': 1,\n",
    "    'Multiband × Multi-echo': 2,\n",
    "    'Head Coil × Multiband × Multi-echo': 2\n",
    "}\n",
    "\n",
    "# Build the APA-style ANOVA table structure\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect) \n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan  \n",
    "    })\n",
    "\n",
    "# Compute Partial Eta-Squared ($\\eta_p^2$)\n",
    "try:\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'] == 'Residual', 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        residual_var = model.ranef_var.iloc[1]['Var'] if len(model.ranef_var) > 1 else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan \n",
    "\n",
    "n_obs_lme = len(data_model_for_lme)\n",
    "n_fixed_df_sum = sum(v for k, v in df_dict.items() if k != 'Residuals')\n",
    "n_subj_lme = data_model_for_lme['subject'].nunique()\n",
    "\n",
    "ss_residual = residual_var * (n_obs_lme - n_fixed_df_sum - n_subj_lme) if pd.notna(residual_var) else np.nan\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    if pd.notna(ss_effect) and pd.notna(ss_residual) and (ss_effect + ss_residual) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + ss_residual)\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan \n",
    "\n",
    "# Create the final APA table DataFrame and format numerical columns\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "# Display the APA-style ANOVA table\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save the APA table to a CSV file\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BAR PLOT GENERATION FOR ROI TSNR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: GENERATING BAR PLOTS FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Debug: Verify Matplotlib backend\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "\n",
    "def process_data_for_plotting(data, value_column):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard error by multiband, multi-echo, and headcoil\n",
    "    for a given value_column (e.g., 'tsnrMedian').\n",
    "    \"\"\"\n",
    "    print(f\"Processing data for plotting '{value_column}'...\")\n",
    "    \n",
    "    if data is None or data.empty:\n",
    "        print(\"Error: Data is None or empty. Cannot process for plotting.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(\"Input data shape:\", data.shape)\n",
    "    print(\"Number of unique subjects in plotting data:\", data['subject'].nunique())\n",
    "    \n",
    "    # Ensure the value_column and grouping columns exist and are not NaN\n",
    "    data_for_agg = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data_for_agg.empty:\n",
    "        print(\"Error: Data is empty after dropping NaNs for plotting. Cannot process.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    agg_data = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True).agg(\n",
    "        mean_value=(value_column, 'mean'),\n",
    "        n_subjects=('subject', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error manually\n",
    "    std_error = data_for_agg.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index(name='se')\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    print(\"Processed data shape for plotting:\", result.shape)\n",
    "    print(\"Processed data for plotting (first 5 rows):\\n\", result.head())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_tsnr_roi_bar_plots(data_processed, n_subjects_per_coil, plot_value_column_name, plot_file_name, mask_name, save_files=True):\n",
    "    \"\"\"\n",
    "    Create bar plots for ROI TSNR with identical y-axis scales across subplots,\n",
    "    matching the style of the smoothness plots.\n",
    "    \"\"\"\n",
    "    print(f\"Creating {mask_name} {plot_value_column_name} bar plots...\")\n",
    "    if data_processed.empty:\n",
    "        print(\"Error: Processed data for plotting is empty. Cannot create plots.\")\n",
    "        return None\n",
    "        \n",
    "    plt.rcParams.update({'font.size': 48}) # Set global font size\n",
    "    coil_types = sorted(data_processed['headcoil'].unique())\n",
    "    print(f\"Plotting for coil types: {coil_types}\")\n",
    "    \n",
    "    # Create subplots, one for each headcoil type\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    # If there's only one subplot, axes will not be an array, so wrap it.\n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    width = 0.4 # Bar width\n",
    "    x1 = [0, 1.2, 2.4] # X-coordinates for the first set of bars (e.g., me1)\n",
    "    x2 = [x + width for x in x1] # X-coordinates for the second set of bars (e.g., me4)\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'} # Colors for multi-echo conditions\n",
    "    mb_levels = ['mb1', 'mb3', 'mb6'] # Order of multiband levels for x-axis\n",
    "    \n",
    "    # Collect all y-values (tsnr) and their errors across all data\n",
    "    # to determine a consistent y-axis range for all subplots.\n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        # Reindex to ensure all mb_levels are present, filling missing with 0 for plotting\n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        y_values = list(me1_means) + list(me4_means)\n",
    "        y_errors = list(me1_errors) + list(me4_errors)\n",
    "        # Only add non-zero values/errors to the list for limit calculation\n",
    "        all_y_values.extend([v for v in y_values if v != 0])\n",
    "        all_y_errors.extend([e for e in y_errors if e != 0])\n",
    "    \n",
    "    # Calculate global y-axis limits to make all plots comparable\n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1 # Add a 10% margin for better visualization\n",
    "        y_limits = (max(0, y_min - margin), y_max + margin) # Ensure lower limit is not negative\n",
    "        print(f\"Calculated consistent Y-axis limits: {y_limits}\")\n",
    "    else:\n",
    "        y_limits = (0, 1) # Default limits if no valid data is found\n",
    "        print(f\"Warning: No valid {plot_value_column_name} y-values found. Using default y-limits:\", y_limits)\n",
    "    \n",
    "    # Plot each subplot with the consistent y-axis limits\n",
    "    for i, (coil, ax) in enumerate(zip(coil_types, axes)):\n",
    "        coil_data = data_processed[data_processed['headcoil'] == coil]\n",
    "        if coil_data.empty:\n",
    "            # Handle cases where a coil type might have no data after processing\n",
    "            ax.set_title(f\"{coil}-Channel\\nn=0\", fontsize=48, fontweight='bold')\n",
    "            ax.set_ylim(y_limits)\n",
    "            ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "            ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "            print(f\"No data for coil {coil}. Skipping plot for this headcoil type.\")\n",
    "            continue\n",
    "            \n",
    "        me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "        me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "        \n",
    "        me1_means = me1_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        me4_means = me4_data.set_index('mb')[plot_value_column_name].reindex(mb_levels).fillna(0)\n",
    "        me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "        \n",
    "        # Plot bars for me1 and me4\n",
    "        ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "               yerr=me1_errors, capsize=5)\n",
    "        ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "               yerr=me4_errors, capsize=5)\n",
    "        \n",
    "        # Set x-axis ticks and labels\n",
    "        ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "        ax.set_xticklabels(mb_levels, fontsize=48)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        \n",
    "        # Set y-axis label only for the first subplot\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(plot_value_column_name.replace('tsnr', 'TSNR ').replace('Median', ' Median').replace('tsnr', 'TSNR'), fontsize=48)\n",
    "        \n",
    "        # Set title with headcoil type and subject count\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"{coil}-Channel\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        # Apply the consistent y-axis limits\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    # Add a single legend for both subplots (placed outside the last subplot)\n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    # Adjust layout to prevent labels/titles from overlapping\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85) # Adjust right margin to make space for the legend\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05) # Super x-label for the whole figure\n",
    "    \n",
    "    # Save the plot\n",
    "    if save_files:\n",
    "        plt.savefig(plot_file_name, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{plot_file_name}'\")\n",
    "    \n",
    "    plt.show() # Display the plot\n",
    "    return fig\n",
    "\n",
    "# Generate plots using complete subjects ROI TSNR data\n",
    "n_subjects_per_coil_plot = roi_tsnr_data_filtered.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "print(f\"Subjects per coil (for {MASK_VALUE} {IMG_VALUE} plotting):\", n_subjects_per_coil_plot)\n",
    "\n",
    "# Process data for plotting\n",
    "tsnr_roi_processed_for_plot = process_data_for_plotting(roi_tsnr_data_filtered, IMG_VALUE)\n",
    "\n",
    "# Create bar plots for ROI TSNR\n",
    "if not tsnr_roi_processed_for_plot.empty:\n",
    "    fig = create_tsnr_roi_bar_plots(tsnr_roi_processed_for_plot, n_subjects_per_coil_plot, IMG_VALUE, roi_tsnr_plot_file, MASK_VALUE)\n",
    "    print(f\"{MASK_VALUE} {IMG_VALUE} bar plot generation complete!\")\n",
    "else:\n",
    "    print(f\"Error: Failed to process {MASK_VALUE} {IMG_VALUE} data for plotting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROI TSNR extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Total common complete subjects identified and used for analysis: {total_subjects_for_lme}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_for_lme['64']}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - {complete_subjects_roi_tsnr_file}\")\n",
    "print(f\"  - {anova_table_file}\")\n",
    "print(f\"  - {roi_tsnr_plot_file}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75892dca-0e06-460f-abbb-9f1c3390cbb7",
   "metadata": {},
   "source": [
    "# BETAs: Estimated Marginal Means (EMMs) Line Plots (Figure 6)\n",
    "This kernel is dedicated to visualizing the Estimated Marginal Means (EMMs) derived from the Linear Mixed Effects (LME) model fitted in the previous section. EMMs, also known as least-squares means, are predictions from a statistical model over a regular grid of predictor values, often adjusted for covariates. They're especially useful for interpreting effects in complex models like LMEs, particularly when interactions between factors are present.\n",
    "\n",
    "The code here uses the rpy2 library to connect with R's powerful statistical packages, specifically lme4 for the mixed-effects model and emmeans for calculating these estimated marginal means.\n",
    "\n",
    "## How it Works:\n",
    "- Input Data: It takes your preprocessed and merged analysis data (which includes subject, headcoil, mb, me, your beta values, and smoothness) as its main input. It also uses the subject counts per head coil to display in the plot titles.\n",
    "- LME Model Fitting: A simplified LME model is re-fitted internally within the plotting function using rpy2 and R's lme4. This model includes the headcoil, multiband, multi-echo factors, their interactions, and the smoothness covariate, with subject as a random effect. This ensures the EMMs are properly adjusted for all relevant variables.\n",
    "- EMM Calculation: After the model is fitted, rpy2 calls R's emmeans package to compute the estimated marginal means of your beta values across the multiband factor, conditioned on headcoil and multi-echo levels.\n",
    "- Visualization: Using matplotlib, the kernel generates clear line plots. Each subplot represents a head coil type, showing the EMMs for different multiband factors, with distinct lines for multi-echo types. Error bars indicate the standard error of the means. The y-axis is kept consistent across subplots for easy comparison.\n",
    "- Output: The generated plot is saved as a PNG image (rFFA_beta_emm_plot.png) and displayed right in your notebook.\n",
    "\n",
    "This visualization offers an intuitive way to understand the main effects and interactions of your experimental factors on the beta values, all while accounting for the smoothness covariate and individual subject variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf3cc5-8f33-4e28-b590-9b770429e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from patsy import dmatrix\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import conversion\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "\n",
    "# --- Add rpy2 specific imports and setup for EMM plots ---\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# Activate numpy2ri conversion (already present)\n",
    "numpy2ri.activate()\n",
    "# Activate pandas2ri conversion (NEW)\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import R packages required for EMM plotting (NEW)\n",
    "lme4 = importr('lme4')\n",
    "lmerTest = importr('lmerTest') # Useful if you run LME directly in R, though pymer4 handles it\n",
    "emmeans = importr('emmeans')\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BETA VS ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND EMMs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"beta\" # This variable holds the name of your value column (e.g., 'beta')\n",
    "MASK_VALUE = \"VSconstrained\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "complete_subjects_roi_beta_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "emm_table_file = f'{MASK_VALUE}_{IMG_VALUE}_emm_table.csv'\n",
    "\n",
    "print(f\"ROI Beta extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing Beta for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing...\")\n",
    "\n",
    "# --- NEW: Function to create EMM Line Plots, adapted for this kernel's data ---\n",
    "def create_emm_line_plots(data, n_subjects_per_coil, img_value_col_name):\n",
    "    \"\"\"\n",
    "    Create line plots for estimated marginal means of beta values with consistent y-axis across subplots.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing ROI beta and smoothness data.\n",
    "                             Expected columns: 'subject', 'headcoil', 'mb', 'me', img_value_col_name, 'smoothness'.\n",
    "        n_subjects_per_coil (dict): A dictionary mapping coil names ('20', '64') to the number of subjects for each coil.\n",
    "        img_value_col_name (str): The name of the column in 'data' that contains the values to plot EMMs for (e.g., 'beta').\n",
    "    \"\"\"\n",
    "    # Prepare data for R\n",
    "    # Rename columns to match R-friendly names (e.g., 'BetaValue' for the outcome)\n",
    "    data_r = data.rename(columns={'subject': 'Subj', 'headcoil': 'HC', 'mb': 'MB', 'me': 'ME',\n",
    "                                  img_value_col_name: 'BetaValue'})\n",
    "    \n",
    "    # Drop rows with any NaN in critical columns before sending to R\n",
    "    data_r = data_r.dropna(subset=['BetaValue', 'MB', 'ME', 'HC', 'Subj', 'smoothness'])\n",
    "    \n",
    "    # Ensure relevant columns are of string type for R factor conversion\n",
    "    data_r['Subj'] = data_r['Subj'].astype(str)\n",
    "    data_r['MB'] = data_r['MB'].astype(str)\n",
    "    data_r['ME'] = data_r['ME'].astype(str)\n",
    "    data_r['HC'] = data_r['HC'].astype(str)\n",
    "    \n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df_temp = pandas2ri.py2rpy(data_r) # Renamed to avoid confusion with R object name\n",
    "\n",
    "    # Push the R DataFrame object to the R global environment with a temporary name\n",
    "    r_df_name = \"temp_emm_data_df\" # A unique temporary name for the R object\n",
    "    ro.globalenv[r_df_name] = r_df_temp\n",
    "\n",
    "    # Define factor levels in R to ensure correct ordering in plots/models\n",
    "    ro.r('''\n",
    "    levels_MB <- c(\"mb1\", \"mb3\", \"mb6\")\n",
    "    levels_ME <- c(\"me1\", \"me4\")\n",
    "    levels_HC <- c(\"20\", \"64\") # Explicitly define headcoil levels\n",
    "    ''')\n",
    "\n",
    "    # Apply factor conversions in R to the DataFrame\n",
    "    # Now, explicitly use the name we gave the DataFrame in R\n",
    "    r_df = ro.r(f'''\n",
    "    transform({r_df_name}, MB = factor(MB, levels = levels_MB), ME = factor(ME, levels = levels_ME), HC = factor(HC, levels = levels_HC))\n",
    "    ''') # No .format() needed here because we use f-string and the R name\n",
    "\n",
    "    # Clean up the temporary R object (optional)\n",
    "    del ro.globalenv[r_df_name]\n",
    "    \n",
    "    # Fit the Linear Mixed Effects Model using lme4\n",
    "    # IMPORTANT: Added 'smoothness' covariate to the formula\n",
    "    formula = Formula(f'BetaValue ~ HC * MB * ME + smoothness + (1 | Subj)')\n",
    "    model = lme4.lmer(formula, data=r_df)\n",
    "    \n",
    "    # Compute Estimated Marginal Means (EMMs) using emmeans\n",
    "    emm = emmeans.emmeans(model, 'MB', by=ro.StrVector(['HC', 'ME']))\n",
    "    \n",
    "    # Convert the R EMMs object to a pandas DataFrame\n",
    "    emm_r_df = ro.r('as.data.frame')(emm)\n",
    "    emm_df = pandas2ri.rpy2py(emm_r_df)\n",
    "    \n",
    "    # Map numeric factor indices back to original string labels from R\n",
    "    mb_map = {1: 'mb1', 2: 'mb3', 3: 'mb6'}\n",
    "    me_map = {1: 'me1', 2: 'me4'}\n",
    "    hc_levels = sorted(data['headcoil'].unique()) # Get actual headcoil levels from data\n",
    "    hc_map = {i + 1: hc for i, hc in enumerate(hc_levels)}\n",
    "    \n",
    "    emm_df['MB'] = emm_df['MB'].map(mb_map)\n",
    "    emm_df['ME'] = emm_df['ME'].map(me_map)\n",
    "    emm_df['HC'] = emm_df['HC'].map(hc_map)\n",
    "    \n",
    "    # --- Plotting Section ---\n",
    "    plt.rcParams.update({'font.size': 48})\n",
    "    \n",
    "    coil_types = emm_df['HC'].unique()\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    \n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    \n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            if not me_data.empty:\n",
    "                y_values = me_data['emmean'].values\n",
    "                y_errors = me_data['SE'].values\n",
    "                all_y_values.extend(y_values)\n",
    "                all_y_errors.extend(y_errors)\n",
    "    \n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1\n",
    "        y_limits = (y_min - margin, y_max + margin) # Allow negative if data goes there\n",
    "    else:\n",
    "        y_limits = (-1, 1) # Default limits if no data\n",
    "    \n",
    "    for i, coil in enumerate(coil_types):\n",
    "        ax = axes[i]\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        \n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            me_data = me_data.sort_values('MB', key=lambda x: x.str.extract(r'mb(\\d+)')[0].astype(int))\n",
    "            \n",
    "            ax.plot(me_data['MB'], me_data['emmean'], marker='o', color=me_colors[me], label=me, linewidth=5, markersize=15)\n",
    "            ax.errorbar(me_data['MB'], me_data['emmean'], yerr=me_data['SE'], fmt='none', color=me_colors[me], capsize=8, capthick=4, elinewidth=3)\n",
    "        \n",
    "        # Adjusted title to use 'Headcoil' instead of 'Channel' if '20' or '64' are your nominal headcoil types\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"Headcoil {coil}\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f'Estimated\\nMarginal Mean {img_value_col_name.capitalize()}', fontsize=48)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05)\n",
    "    plt.savefig(f'{MASK_VALUE}_{IMG_VALUE}_emm_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Part 1: Data Processing and Common Complete Subject Identification\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR {MASK_VALUE} BETA AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for {mask_value} {img_value} data...\")\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "    df = pd.DataFrame(data_records)\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    print(f\"Successfully extracted {len(df)} {mask_value} {img_value} data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        data = data.rename(columns={data.columns[0]: 'path', 'Unnamed: 3': 'smoothness'})\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        # Corrected: Filter to select only relevant columns and remove any 'nan' from subject\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "roi_beta_data_preprocessed = extract_roi_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_beta_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI Beta data loading failed. Aborting script.\")\n",
    "\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "complete_roi_beta_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_beta_data_preprocessed.copy(),\n",
    "    IMG_VALUE,\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI Beta data ({MASK_VALUE}): {len(complete_roi_beta_subjects)}\")\n",
    "\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(),\n",
    "    'smoothness',\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "common_complete_subjects = list(set(complete_roi_beta_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort()\n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} {IMG_VALUE} and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "roi_beta_data_filtered = roi_beta_data_preprocessed[\n",
    "    roi_beta_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "for df in [roi_beta_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_beta_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner'\n",
    ")\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "roi_beta_complete_table_for_output = roi_beta_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE,\n",
    "    index='subject',\n",
    "    columns='acq_combined',\n",
    "    aggfunc='mean'\n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "headcoil_df_for_output = roi_beta_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_beta_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output[cols_output]\n",
    "roi_beta_complete_table_for_output.to_csv(complete_subjects_roi_beta_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table saved to '{complete_subjects_roi_beta_file}'\")\n",
    "\n",
    "# Part 2: Linear Mixed Effects Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE FOR {MASK_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "# pymer4 automatically handles categorical encoding, so headcoil_encoded might not be strictly needed for pymer4,\n",
    "# but if you use dmatrix explicitly or found it necessary, keep it.\n",
    "# For R's lmer, it prefers factors directly, which we handle in the create_emm_line_plots.\n",
    "# data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# LME model using pymer4 (this part remains unchanged)\n",
    "lme_formula = f'{IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)' # Use 'headcoil' directly if pymer4 handles factors\n",
    "model = Lmer(lme_formula, data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Map pymer4 effect names to APA-style names\n",
    "effect_map = {\n",
    "    'headcoil': 'Head Coil', # Changed from headcoil_encoded\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness',\n",
    "    'headcoil:mb': 'Head Coil × Multiband', # Changed from headcoil_encoded:mb\n",
    "    'headcoil:me': 'Head Coil × Multi-echo', # Changed from headcoil_encoded:me\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil:mb:me': 'Head Coil × Multiband × Multi-echo' # Changed from headcoil_encoded:mb:me\n",
    "}\n",
    "\n",
    "# The NumDF values for the main effects and interactions are fixed based on levels:\n",
    "# Head Coil (2 levels) - 1 df\n",
    "# Multiband (3 levels) - 2 df\n",
    "# Multi-echo (2 levels) - 1 df\n",
    "# Smoothness (continuous) - 1 df\n",
    "# Interactions are product of main effect DFs\n",
    "df_dict = {\n",
    "    'Head Coil': 1,\n",
    "    'Multiband': 2,\n",
    "    'Multi-echo': 1,\n",
    "    'Smoothness': 1,\n",
    "    'Head Coil × Multiband': 1 * 2,\n",
    "    'Head Coil × Multi-echo': 1 * 1,\n",
    "    'Multiband × Multi-echo': 2 * 1,\n",
    "    'Head Coil × Multiband × Multi-echo': 1 * 2 * 1\n",
    "}\n",
    "\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect)\n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan\n",
    "    })\n",
    "\n",
    "try:\n",
    "    # Attempt to get residual variance from pymer4 model's ranef_var\n",
    "    # pymer4's residual variance is often labelled as '__sigma__' or 'residual'\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'].isin(['Residual', '__sigma__']), 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        # Fallback if specific residual label not found, often last entry\n",
    "        residual_var = model.ranef_var.iloc[-1]['Var'] if not model.ranef_var.empty else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan\n",
    "\n",
    "# Calculate partial eta-squared (assuming type III SS from pymer4)\n",
    "# For partial eta-squared, we need SS_effect and SS_error (residual SS)\n",
    "# pymer4's SS are Type 3 by default, suitable for this calculation.\n",
    "# The total sum of squares for the effect and error needs to be sum of SS_effect and SS_Residual.\n",
    "# The `model.ranef_var` contains variance components, not necessarily SS_residual directly for fixed effects.\n",
    "# A common simplification for partial eta-squared in mixed models is SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, without direct SS_error from pymer4's ANOVA for each effect, we rely on total residual variance from the model.\n",
    "# This approximation might not be perfectly aligned with more complex mixed model eta-squared calculations\n",
    "# (e.g., those considering denominator degrees of freedom).\n",
    "# For now, keeping the previous logic of ss_effect / (ss_effect + SS_residual_overall_from_model) as an approximation.\n",
    "\n",
    "# Re-evaluating residual SS calculation:\n",
    "# For lmer/lmerTest, the residual variance (sigma^2) is reported.\n",
    "# SS_residual = residual_variance * residual_df.\n",
    "# From anova_table, DenomDF is often the error df for that effect.\n",
    "# For fixed effects, a simpler approximation of partial eta-squared is SS_effect / (SS_effect + SS_error)\n",
    "# where SS_error could be the sum of squares for the residual from the overall ANOVA table.\n",
    "# pymer4's anova doesn't directly report an 'Error' row with SS.\n",
    "# Let's use the total residual variance from the model for a common (though approximate) partial eta-squared.\n",
    "# The original code's calculation of ss_residual was more of an estimation based on total observations.\n",
    "\n",
    "# A more robust calculation for partial eta-squared from pymer4's output would involve:\n",
    "# For each effect, Partial η² = SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, pymer4's ANOVA table doesn't directly provide SS_error_for_that_effect.\n",
    "# We'll use the residual variance `residual_var` from `model.ranef_var` as an overall residual variance estimate.\n",
    "# The calculation in the original code for `ss_residual` was:\n",
    "# `residual_var * anova_df['Den Df']` from the R `anova` table.\n",
    "# Since we are using pymer4 here, we have `anova_table['DenomDF']`.\n",
    "# Let's adapt the calculation to be closer to what `lmerTest` does for approximate partial eta-squared.\n",
    "\n",
    "# If DenomDF is available from pymer4's anova, we can use it to derive an approximate SS_error per effect.\n",
    "# This assumes DenomDF represents the error degrees of freedom associated with that F-test.\n",
    "# SS_error_approx = residual_var * anova_table.loc[effect, 'DenomDF']\n",
    "# For simplicity, let's keep the existing loop logic for now, using the overall residual_var\n",
    "# and assuming it's a common denominator for the partial eta-squared.\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    denom_df = row['Den df'] # Denominator DF for this effect\n",
    "    # The true SS_error is complex in mixed models.\n",
    "    # An approximation for partial eta-squared for fixed effects is often\n",
    "    # SS_effect / (SS_effect + SS_Residual from overall model).\n",
    "    # Since pymer4 doesn't give a direct SS_Residual for the fixed effects ANOVA,\n",
    "    # and the 'smoothness' is added, we use the `residual_var` as an overall error component.\n",
    "    # A more precise partial eta^2 for lmer models might require more specific formulas or another R package.\n",
    "    \n",
    "    # Using the logic from the original kernel for the effect size calculation:\n",
    "    # partial_eta2 = SS_effect / (SS_effect + SS_residual)\n",
    "    # The SS_residual was approximated from model deviance or variance components (sigma^2 * Den Df).\n",
    "    \n",
    "    # If we assume `residual_var` (sigma^2) is the common error variance:\n",
    "    # SS_error for a given effect is approximately residual_var * DenomDF for that effect.\n",
    "    if pd.notna(ss_effect) and pd.notna(residual_var) and pd.notna(denom_df) and (ss_effect + (residual_var * denom_df)) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + (residual_var * denom_df))\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan\n",
    "\n",
    "\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "\n",
    "# Part 3: Estimated Marginal Means (EMMs) Calculation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: ESTIMATED MARGINAL MEANS (EMMs) FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    print(\"Skipping EMM plot generation: Merged analysis data is empty.\")\n",
    "else:\n",
    "    # Prepare n_subjects_per_coil for the plotting function\n",
    "    # It should map '20' to count and '64' to count\n",
    "    n_subjects_per_coil_for_plot = headcoil_counts_for_lme.to_dict()\n",
    "    \n",
    "    # Call the EMM plotting function\n",
    "    print(\"Generating EMM line plots...\")\n",
    "    emm_plot_fig = create_emm_line_plots(merged_analysis_data, n_subjects_per_coil_for_plot, IMG_VALUE)\n",
    "    print(f\"EMM line plot saved to '{MASK_VALUE}_{IMG_VALUE}_emm_plot.png'\")\n",
    "    plt.show() # Display the plot in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afc273-3daf-4cbe-b0ee-8de463072013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from patsy import dmatrix\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import conversion\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "\n",
    "# --- Add rpy2 specific imports and setup for EMM plots ---\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# Activate numpy2ri conversion (already present)\n",
    "numpy2ri.activate()\n",
    "# Activate pandas2ri conversion (NEW)\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import R packages required for EMM plotting (NEW)\n",
    "lme4 = importr('lme4')\n",
    "lmerTest = importr('lmerTest') # Useful if you run LME directly in R, though pymer4 handles it\n",
    "emmeans = importr('emmeans')\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BETA rFFA ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND EMMs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"beta\" # This variable holds the name of your value column (e.g., 'beta')\n",
    "MASK_VALUE = \"rFFA\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "complete_subjects_roi_beta_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "emm_table_file = f'{MASK_VALUE}_{IMG_VALUE}_emm_table.csv'\n",
    "\n",
    "print(f\"ROI Beta extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing Beta for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing...\")\n",
    "\n",
    "# --- NEW: Function to create EMM Line Plots, adapted for this kernel's data ---\n",
    "def create_emm_line_plots(data, n_subjects_per_coil, img_value_col_name):\n",
    "    \"\"\"\n",
    "    Create line plots for estimated marginal means of beta values with consistent y-axis across subplots.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing ROI beta and smoothness data.\n",
    "                             Expected columns: 'subject', 'headcoil', 'mb', 'me', img_value_col_name, 'smoothness'.\n",
    "        n_subjects_per_coil (dict): A dictionary mapping coil names ('20', '64') to the number of subjects for each coil.\n",
    "        img_value_col_name (str): The name of the column in 'data' that contains the values to plot EMMs for (e.g., 'beta').\n",
    "    \"\"\"\n",
    "    # Prepare data for R\n",
    "    # Rename columns to match R-friendly names (e.g., 'BetaValue' for the outcome)\n",
    "    data_r = data.rename(columns={'subject': 'Subj', 'headcoil': 'HC', 'mb': 'MB', 'me': 'ME',\n",
    "                                  img_value_col_name: 'BetaValue'})\n",
    "    \n",
    "    # Drop rows with any NaN in critical columns before sending to R\n",
    "    data_r = data_r.dropna(subset=['BetaValue', 'MB', 'ME', 'HC', 'Subj', 'smoothness'])\n",
    "    \n",
    "    # Ensure relevant columns are of string type for R factor conversion\n",
    "    data_r['Subj'] = data_r['Subj'].astype(str)\n",
    "    data_r['MB'] = data_r['MB'].astype(str)\n",
    "    data_r['ME'] = data_r['ME'].astype(str)\n",
    "    data_r['HC'] = data_r['HC'].astype(str)\n",
    "    \n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df_temp = pandas2ri.py2rpy(data_r) # Renamed to avoid confusion with R object name\n",
    "\n",
    "    # Push the R DataFrame object to the R global environment with a temporary name\n",
    "    r_df_name = \"temp_emm_data_df\" # A unique temporary name for the R object\n",
    "    ro.globalenv[r_df_name] = r_df_temp\n",
    "\n",
    "    # Define factor levels in R to ensure correct ordering in plots/models\n",
    "    ro.r('''\n",
    "    levels_MB <- c(\"mb1\", \"mb3\", \"mb6\")\n",
    "    levels_ME <- c(\"me1\", \"me4\")\n",
    "    levels_HC <- c(\"20\", \"64\") # Explicitly define headcoil levels\n",
    "    ''')\n",
    "\n",
    "    # Apply factor conversions in R to the DataFrame\n",
    "    # Now, explicitly use the name we gave the DataFrame in R\n",
    "    r_df = ro.r(f'''\n",
    "    transform({r_df_name}, MB = factor(MB, levels = levels_MB), ME = factor(ME, levels = levels_ME), HC = factor(HC, levels = levels_HC))\n",
    "    ''') # No .format() needed here because we use f-string and the R name\n",
    "\n",
    "    # Clean up the temporary R object (optional)\n",
    "    del ro.globalenv[r_df_name]\n",
    "    \n",
    "    # Fit the Linear Mixed Effects Model using lme4\n",
    "    # IMPORTANT: Added 'smoothness' covariate to the formula\n",
    "    formula = Formula(f'BetaValue ~ HC * MB * ME + smoothness + (1 | Subj)')\n",
    "    model = lme4.lmer(formula, data=r_df)\n",
    "    \n",
    "    # Compute Estimated Marginal Means (EMMs) using emmeans\n",
    "    emm = emmeans.emmeans(model, 'MB', by=ro.StrVector(['HC', 'ME']))\n",
    "    \n",
    "    # Convert the R EMMs object to a pandas DataFrame\n",
    "    emm_r_df = ro.r('as.data.frame')(emm)\n",
    "    emm_df = pandas2ri.rpy2py(emm_r_df)\n",
    "    \n",
    "    # Map numeric factor indices back to original string labels from R\n",
    "    mb_map = {1: 'mb1', 2: 'mb3', 3: 'mb6'}\n",
    "    me_map = {1: 'me1', 2: 'me4'}\n",
    "    hc_levels = sorted(data['headcoil'].unique()) # Get actual headcoil levels from data\n",
    "    hc_map = {i + 1: hc for i, hc in enumerate(hc_levels)}\n",
    "    \n",
    "    emm_df['MB'] = emm_df['MB'].map(mb_map)\n",
    "    emm_df['ME'] = emm_df['ME'].map(me_map)\n",
    "    emm_df['HC'] = emm_df['HC'].map(hc_map)\n",
    "    \n",
    "    # --- Plotting Section ---\n",
    "    plt.rcParams.update({'font.size': 48})\n",
    "    \n",
    "    coil_types = emm_df['HC'].unique()\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    \n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    \n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            if not me_data.empty:\n",
    "                y_values = me_data['emmean'].values\n",
    "                y_errors = me_data['SE'].values\n",
    "                all_y_values.extend(y_values)\n",
    "                all_y_errors.extend(y_errors)\n",
    "    \n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1\n",
    "        y_limits = (y_min - margin, y_max + margin) # Allow negative if data goes there\n",
    "    else:\n",
    "        y_limits = (-1, 1) # Default limits if no data\n",
    "    \n",
    "    for i, coil in enumerate(coil_types):\n",
    "        ax = axes[i]\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        \n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            me_data = me_data.sort_values('MB', key=lambda x: x.str.extract(r'mb(\\d+)')[0].astype(int))\n",
    "            \n",
    "            ax.plot(me_data['MB'], me_data['emmean'], marker='o', color=me_colors[me], label=me, linewidth=5, markersize=15)\n",
    "            ax.errorbar(me_data['MB'], me_data['emmean'], yerr=me_data['SE'], fmt='none', color=me_colors[me], capsize=8, capthick=4, elinewidth=3)\n",
    "        \n",
    "        # Adjusted title to use 'Headcoil' instead of 'Channel' if '20' or '64' are your nominal headcoil types\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"Headcoil {coil}\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f'Estimated\\nMarginal Mean {img_value_col_name.capitalize()}', fontsize=48)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05)\n",
    "    plt.savefig(f'{MASK_VALUE}_{IMG_VALUE}_emm_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Part 1: Data Processing and Common Complete Subject Identification\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR {MASK_VALUE} BETA AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for {mask_value} {img_value} data...\")\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "    df = pd.DataFrame(data_records)\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    print(f\"Successfully extracted {len(df)} {mask_value} {img_value} data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        data = data.rename(columns={data.columns[0]: 'path', 'Unnamed: 3': 'smoothness'})\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        # Corrected: Filter to select only relevant columns and remove any 'nan' from subject\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "roi_beta_data_preprocessed = extract_roi_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_beta_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI Beta data loading failed. Aborting script.\")\n",
    "\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "complete_roi_beta_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_beta_data_preprocessed.copy(),\n",
    "    IMG_VALUE,\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI Beta data ({MASK_VALUE}): {len(complete_roi_beta_subjects)}\")\n",
    "\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(),\n",
    "    'smoothness',\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "common_complete_subjects = list(set(complete_roi_beta_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort()\n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} {IMG_VALUE} and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "roi_beta_data_filtered = roi_beta_data_preprocessed[\n",
    "    roi_beta_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "for df in [roi_beta_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_beta_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner'\n",
    ")\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "roi_beta_complete_table_for_output = roi_beta_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE,\n",
    "    index='subject',\n",
    "    columns='acq_combined',\n",
    "    aggfunc='mean'\n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "headcoil_df_for_output = roi_beta_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_beta_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output[cols_output]\n",
    "roi_beta_complete_table_for_output.to_csv(complete_subjects_roi_beta_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table saved to '{complete_subjects_roi_beta_file}'\")\n",
    "\n",
    "# Part 2: Linear Mixed Effects Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE FOR {MASK_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "# pymer4 automatically handles categorical encoding, so headcoil_encoded might not be strictly needed for pymer4,\n",
    "# but if you use dmatrix explicitly or found it necessary, keep it.\n",
    "# For R's lmer, it prefers factors directly, which we handle in the create_emm_line_plots.\n",
    "# data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# LME model using pymer4 (this part remains unchanged)\n",
    "lme_formula = f'{IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)' # Use 'headcoil' directly if pymer4 handles factors\n",
    "model = Lmer(lme_formula, data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Map pymer4 effect names to APA-style names\n",
    "effect_map = {\n",
    "    'headcoil': 'Head Coil', # Changed from headcoil_encoded\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness',\n",
    "    'headcoil:mb': 'Head Coil × Multiband', # Changed from headcoil_encoded:mb\n",
    "    'headcoil:me': 'Head Coil × Multi-echo', # Changed from headcoil_encoded:me\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil:mb:me': 'Head Coil × Multiband × Multi-echo' # Changed from headcoil_encoded:mb:me\n",
    "}\n",
    "\n",
    "# The NumDF values for the main effects and interactions are fixed based on levels:\n",
    "# Head Coil (2 levels) - 1 df\n",
    "# Multiband (3 levels) - 2 df\n",
    "# Multi-echo (2 levels) - 1 df\n",
    "# Smoothness (continuous) - 1 df\n",
    "# Interactions are product of main effect DFs\n",
    "df_dict = {\n",
    "    'Head Coil': 1,\n",
    "    'Multiband': 2,\n",
    "    'Multi-echo': 1,\n",
    "    'Smoothness': 1,\n",
    "    'Head Coil × Multiband': 1 * 2,\n",
    "    'Head Coil × Multi-echo': 1 * 1,\n",
    "    'Multiband × Multi-echo': 2 * 1,\n",
    "    'Head Coil × Multiband × Multi-echo': 1 * 2 * 1\n",
    "}\n",
    "\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect)\n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan\n",
    "    })\n",
    "\n",
    "try:\n",
    "    # Attempt to get residual variance from pymer4 model's ranef_var\n",
    "    # pymer4's residual variance is often labelled as '__sigma__' or 'residual'\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'].isin(['Residual', '__sigma__']), 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        # Fallback if specific residual label not found, often last entry\n",
    "        residual_var = model.ranef_var.iloc[-1]['Var'] if not model.ranef_var.empty else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan\n",
    "\n",
    "# Calculate partial eta-squared (assuming type III SS from pymer4)\n",
    "# For partial eta-squared, we need SS_effect and SS_error (residual SS)\n",
    "# pymer4's SS are Type 3 by default, suitable for this calculation.\n",
    "# The total sum of squares for the effect and error needs to be sum of SS_effect and SS_Residual.\n",
    "# The `model.ranef_var` contains variance components, not necessarily SS_residual directly for fixed effects.\n",
    "# A common simplification for partial eta-squared in mixed models is SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, without direct SS_error from pymer4's ANOVA for each effect, we rely on total residual variance from the model.\n",
    "# This approximation might not be perfectly aligned with more complex mixed model eta-squared calculations\n",
    "# (e.g., those considering denominator degrees of freedom).\n",
    "# For now, keeping the previous logic of ss_effect / (ss_effect + SS_residual_overall_from_model) as an approximation.\n",
    "\n",
    "# Re-evaluating residual SS calculation:\n",
    "# For lmer/lmerTest, the residual variance (sigma^2) is reported.\n",
    "# SS_residual = residual_variance * residual_df.\n",
    "# From anova_table, DenomDF is often the error df for that effect.\n",
    "# For fixed effects, a simpler approximation of partial eta-squared is SS_effect / (SS_effect + SS_error)\n",
    "# where SS_error could be the sum of squares for the residual from the overall ANOVA table.\n",
    "# pymer4's anova doesn't directly report an 'Error' row with SS.\n",
    "# Let's use the total residual variance from the model for a common (though approximate) partial eta-squared.\n",
    "# The original code's calculation of ss_residual was more of an estimation based on total observations.\n",
    "\n",
    "# A more robust calculation for partial eta-squared from pymer4's output would involve:\n",
    "# For each effect, Partial η² = SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, pymer4's ANOVA table doesn't directly provide SS_error_for_that_effect.\n",
    "# We'll use the residual variance `residual_var` from `model.ranef_var` as an overall residual variance estimate.\n",
    "# The calculation in the original code for `ss_residual` was:\n",
    "# `residual_var * anova_df['Den Df']` from the R `anova` table.\n",
    "# Since we are using pymer4 here, we have `anova_table['DenomDF']`.\n",
    "# Let's adapt the calculation to be closer to what `lmerTest` does for approximate partial eta-squared.\n",
    "\n",
    "# If DenomDF is available from pymer4's anova, we can use it to derive an approximate SS_error per effect.\n",
    "# This assumes DenomDF represents the error degrees of freedom associated with that F-test.\n",
    "# SS_error_approx = residual_var * anova_table.loc[effect, 'DenomDF']\n",
    "# For simplicity, let's keep the existing loop logic for now, using the overall residual_var\n",
    "# and assuming it's a common denominator for the partial eta-squared.\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    denom_df = row['Den df'] # Denominator DF for this effect\n",
    "    # The true SS_error is complex in mixed models.\n",
    "    # An approximation for partial eta-squared for fixed effects is often\n",
    "    # SS_effect / (SS_effect + SS_Residual from overall model).\n",
    "    # Since pymer4 doesn't give a direct SS_Residual for the fixed effects ANOVA,\n",
    "    # and the 'smoothness' is added, we use the `residual_var` as an overall error component.\n",
    "    # A more precise partial eta^2 for lmer models might require more specific formulas or another R package.\n",
    "    \n",
    "    # Using the logic from the original kernel for the effect size calculation:\n",
    "    # partial_eta2 = SS_effect / (SS_effect + SS_residual)\n",
    "    # The SS_residual was approximated from model deviance or variance components (sigma^2 * Den Df).\n",
    "    \n",
    "    # If we assume `residual_var` (sigma^2) is the common error variance:\n",
    "    # SS_error for a given effect is approximately residual_var * DenomDF for that effect.\n",
    "    if pd.notna(ss_effect) and pd.notna(residual_var) and pd.notna(denom_df) and (ss_effect + (residual_var * denom_df)) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + (residual_var * denom_df))\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan\n",
    "\n",
    "\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "\n",
    "# Part 3: Estimated Marginal Means (EMMs) Calculation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: ESTIMATED MARGINAL MEANS (EMMs) FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    print(\"Skipping EMM plot generation: Merged analysis data is empty.\")\n",
    "else:\n",
    "    # Prepare n_subjects_per_coil for the plotting function\n",
    "    # It should map '20' to count and '64' to count\n",
    "    n_subjects_per_coil_for_plot = headcoil_counts_for_lme.to_dict()\n",
    "    \n",
    "    # Call the EMM plotting function\n",
    "    print(\"Generating EMM line plots...\")\n",
    "    emm_plot_fig = create_emm_line_plots(merged_analysis_data, n_subjects_per_coil_for_plot, IMG_VALUE)\n",
    "    print(f\"EMM line plot saved to '{MASK_VALUE}_{IMG_VALUE}_emm_plot.png'\")\n",
    "    plt.show() # Display the plot in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d500cb-7e85-4c9d-ba75-8b661a5cbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from patsy import dmatrix\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import conversion\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "\n",
    "# --- Add rpy2 specific imports and setup for EMM plots ---\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# Activate numpy2ri conversion (already present)\n",
    "numpy2ri.activate()\n",
    "# Activate pandas2ri conversion (NEW)\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import R packages required for EMM plotting (NEW)\n",
    "lme4 = importr('lme4')\n",
    "lmerTest = importr('lmerTest') # Useful if you run LME directly in R, though pymer4 handles it\n",
    "emmeans = importr('emmeans')\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BETA Motor Cortex ROI ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND EMMs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/extractions\")\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"beta\" # This variable holds the name of your value column (e.g., 'beta')\n",
    "MASK_VALUE = \"bilateralMotor\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "smoothness_csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "complete_subjects_roi_beta_file = f'complete_subjects_{MASK_VALUE}_{IMG_VALUE}_common_with_smoothness.csv'\n",
    "anova_table_file = f'{MASK_VALUE}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "emm_table_file = f'{MASK_VALUE}_{IMG_VALUE}_emm_table.csv'\n",
    "\n",
    "print(f\"ROI Beta extraction directory: {BASE_DIR}\")\n",
    "print(f\"Smoothness input file: {smoothness_csv_path}\")\n",
    "print(f\"Processing Beta for ROI: {MASK_VALUE}\")\n",
    "print(f\"Initiating data processing...\")\n",
    "\n",
    "# --- NEW: Function to create EMM Line Plots, adapted for this kernel's data ---\n",
    "def create_emm_line_plots(data, n_subjects_per_coil, img_value_col_name):\n",
    "    \"\"\"\n",
    "    Create line plots for estimated marginal means of beta values with consistent y-axis across subplots.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing ROI beta and smoothness data.\n",
    "                             Expected columns: 'subject', 'headcoil', 'mb', 'me', img_value_col_name, 'smoothness'.\n",
    "        n_subjects_per_coil (dict): A dictionary mapping coil names ('20', '64') to the number of subjects for each coil.\n",
    "        img_value_col_name (str): The name of the column in 'data' that contains the values to plot EMMs for (e.g., 'beta').\n",
    "    \"\"\"\n",
    "    # Prepare data for R\n",
    "    # Rename columns to match R-friendly names (e.g., 'BetaValue' for the outcome)\n",
    "    data_r = data.rename(columns={'subject': 'Subj', 'headcoil': 'HC', 'mb': 'MB', 'me': 'ME',\n",
    "                                  img_value_col_name: 'BetaValue'})\n",
    "    \n",
    "    # Drop rows with any NaN in critical columns before sending to R\n",
    "    data_r = data_r.dropna(subset=['BetaValue', 'MB', 'ME', 'HC', 'Subj', 'smoothness'])\n",
    "    \n",
    "    # Ensure relevant columns are of string type for R factor conversion\n",
    "    data_r['Subj'] = data_r['Subj'].astype(str)\n",
    "    data_r['MB'] = data_r['MB'].astype(str)\n",
    "    data_r['ME'] = data_r['ME'].astype(str)\n",
    "    data_r['HC'] = data_r['HC'].astype(str)\n",
    "    \n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df_temp = pandas2ri.py2rpy(data_r) # Renamed to avoid confusion with R object name\n",
    "\n",
    "    # Push the R DataFrame object to the R global environment with a temporary name\n",
    "    r_df_name = \"temp_emm_data_df\" # A unique temporary name for the R object\n",
    "    ro.globalenv[r_df_name] = r_df_temp\n",
    "\n",
    "    # Define factor levels in R to ensure correct ordering in plots/models\n",
    "    ro.r('''\n",
    "    levels_MB <- c(\"mb1\", \"mb3\", \"mb6\")\n",
    "    levels_ME <- c(\"me1\", \"me4\")\n",
    "    levels_HC <- c(\"20\", \"64\") # Explicitly define headcoil levels\n",
    "    ''')\n",
    "\n",
    "    # Apply factor conversions in R to the DataFrame\n",
    "    # Now, explicitly use the name we gave the DataFrame in R\n",
    "    r_df = ro.r(f'''\n",
    "    transform({r_df_name}, MB = factor(MB, levels = levels_MB), ME = factor(ME, levels = levels_ME), HC = factor(HC, levels = levels_HC))\n",
    "    ''') # No .format() needed here because we use f-string and the R name\n",
    "\n",
    "    # Clean up the temporary R object (optional)\n",
    "    del ro.globalenv[r_df_name]\n",
    "    \n",
    "    # Fit the Linear Mixed Effects Model using lme4\n",
    "    # IMPORTANT: Added 'smoothness' covariate to the formula\n",
    "    formula = Formula(f'BetaValue ~ HC * MB * ME + smoothness + (1 | Subj)')\n",
    "    model = lme4.lmer(formula, data=r_df)\n",
    "    \n",
    "    # Compute Estimated Marginal Means (EMMs) using emmeans\n",
    "    emm = emmeans.emmeans(model, 'MB', by=ro.StrVector(['HC', 'ME']))\n",
    "    \n",
    "    # Convert the R EMMs object to a pandas DataFrame\n",
    "    emm_r_df = ro.r('as.data.frame')(emm)\n",
    "    emm_df = pandas2ri.rpy2py(emm_r_df)\n",
    "    \n",
    "    # Map numeric factor indices back to original string labels from R\n",
    "    mb_map = {1: 'mb1', 2: 'mb3', 3: 'mb6'}\n",
    "    me_map = {1: 'me1', 2: 'me4'}\n",
    "    hc_levels = sorted(data['headcoil'].unique()) # Get actual headcoil levels from data\n",
    "    hc_map = {i + 1: hc for i, hc in enumerate(hc_levels)}\n",
    "    \n",
    "    emm_df['MB'] = emm_df['MB'].map(mb_map)\n",
    "    emm_df['ME'] = emm_df['ME'].map(me_map)\n",
    "    emm_df['HC'] = emm_df['HC'].map(hc_map)\n",
    "    \n",
    "    # --- Plotting Section ---\n",
    "    plt.rcParams.update({'font.size': 48})\n",
    "    \n",
    "    coil_types = emm_df['HC'].unique()\n",
    "    fig, axes = plt.subplots(1, len(coil_types), figsize=(8 * len(coil_types), 8))\n",
    "    \n",
    "    if len(coil_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    \n",
    "    all_y_values = []\n",
    "    all_y_errors = []\n",
    "    \n",
    "    for coil in coil_types:\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            if not me_data.empty:\n",
    "                y_values = me_data['emmean'].values\n",
    "                y_errors = me_data['SE'].values\n",
    "                all_y_values.extend(y_values)\n",
    "                all_y_errors.extend(y_errors)\n",
    "    \n",
    "    if all_y_values:\n",
    "        y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "        margin = (y_max - y_min) * 0.1\n",
    "        y_limits = (y_min - margin, y_max + margin) # Allow negative if data goes there\n",
    "    else:\n",
    "        y_limits = (-1, 1) # Default limits if no data\n",
    "    \n",
    "    for i, coil in enumerate(coil_types):\n",
    "        ax = axes[i]\n",
    "        coil_data = emm_df[emm_df['HC'] == coil]\n",
    "        \n",
    "        for me in ['me1', 'me4']:\n",
    "            me_data = coil_data[coil_data['ME'] == me]\n",
    "            me_data = me_data.sort_values('MB', key=lambda x: x.str.extract(r'mb(\\d+)')[0].astype(int))\n",
    "            \n",
    "            ax.plot(me_data['MB'], me_data['emmean'], marker='o', color=me_colors[me], label=me, linewidth=5, markersize=15)\n",
    "            ax.errorbar(me_data['MB'], me_data['emmean'], yerr=me_data['SE'], fmt='none', color=me_colors[me], capsize=8, capthick=4, elinewidth=3)\n",
    "        \n",
    "        # Adjusted title to use 'Headcoil' instead of 'Channel' if '20' or '64' are your nominal headcoil types\n",
    "        n_subjects = n_subjects_per_coil.get(coil, 0)\n",
    "        ax.set_title(f\"Headcoil {coil}\\nn={n_subjects}\", fontsize=48, fontweight='bold')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f'Estimated\\nMarginal Mean {img_value_col_name.capitalize()}', fontsize=48)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=48)\n",
    "        ax.set_ylim(y_limits)\n",
    "    \n",
    "    if len(coil_types) > 0:\n",
    "        axes[-1].legend(title='Multi-echo', fontsize=36, title_fontsize=36, \n",
    "                        loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    fig.supxlabel('Multiband Factor', fontsize=48, y=-0.05)\n",
    "    plt.savefig(f'{MASK_VALUE}_{IMG_VALUE}_emm_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Part 1: Data Processing and Common Complete Subject Identification\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS FOR {MASK_VALUE} BETA AND SMOOTHNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_roi_data(base_dir, type_value, img_value, mask_value, denoise_value, headcoil_64_subjects):\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Scanning {len(file_paths)} files in {base_dir} for {mask_value} {img_value} data...\")\n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            if 'sp' in sub_id or (file_type != type_value or img != img_value or mask != mask_value or denoise != denoise_value):\n",
    "                continue\n",
    "            if acq not in acq_params_list:\n",
    "                continue\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "                headcoil = '64' if sub_id in headcoil_64_subjects else '20'\n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except (ValueError, IOError) as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for file {filename}: {e}\")\n",
    "    if not data_records:\n",
    "        print(f\"Warning: No valid ROI data records found for type={type_value}, img={img_value}, mask={mask_value}, denoise={denoise_value}.\")\n",
    "        return None\n",
    "    df = pd.DataFrame(data_records)\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    df = df.dropna(subset=['subject', 'headcoil', 'mb', 'me', img_value])\n",
    "    print(f\"Successfully extracted {len(df)} {mask_value} {img_value} data points.\")\n",
    "    return df\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, headcoil_64_subjects):\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        data = data.rename(columns={data.columns[0]: 'path', 'Unnamed: 3': 'smoothness'})\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]\n",
    "                    me = acq[3:]\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        data['headcoil'] = data['subject'].apply(\n",
    "            lambda x: '64' if x in headcoil_64_subjects else '20' if x else None\n",
    "        )\n",
    "        # Corrected: Filter to select only relevant columns and remove any 'nan' from subject\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'acq_combined', 'smoothness'])\n",
    "        data = data[~data['subject'].str.contains('sp', na=False)]\n",
    "        data = data[data['subject'] != 'nan']\n",
    "        print(f\"Successfully extracted {len(data)} smoothness data points.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Please check the path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects_for_metric(data_df, value_col, expected_acq_combinations):\n",
    "    pivot_table = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    pivot_table = pivot_table.reindex(columns=expected_acq_combinations)\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    return complete_subjects.index.tolist()\n",
    "\n",
    "expected_acq_combinations = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "roi_beta_data_preprocessed = extract_roi_data(BASE_DIR, TYPE_VALUE, IMG_VALUE, MASK_VALUE, DENOISE_VALUE, HEADCOIL_64_SUBJECTS)\n",
    "if roi_beta_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: ROI Beta data loading failed. Aborting script.\")\n",
    "\n",
    "smoothness_data_preprocessed = load_and_process_smoothness_data(smoothness_csv_path, HEADCOIL_64_SUBJECTS)\n",
    "if smoothness_data_preprocessed is None:\n",
    "    raise Exception(\"Critical: Smoothness data loading failed. Aborting script.\")\n",
    "\n",
    "complete_roi_beta_subjects = identify_complete_subjects_for_metric(\n",
    "    roi_beta_data_preprocessed.copy(),\n",
    "    IMG_VALUE,\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete ROI Beta data ({MASK_VALUE}): {len(complete_roi_beta_subjects)}\")\n",
    "\n",
    "complete_smoothness_subjects = identify_complete_subjects_for_metric(\n",
    "    smoothness_data_preprocessed.copy(),\n",
    "    'smoothness',\n",
    "    expected_acq_combinations\n",
    ")\n",
    "print(f\"Subjects with complete Smoothness data: {len(complete_smoothness_subjects)}\")\n",
    "\n",
    "# --- NEW ADDITION: Displaying complete subject lists side-by-side ---\n",
    "all_unique_subjects = sorted(list(set(complete_roi_beta_subjects) | set(complete_smoothness_subjects)))\n",
    "\n",
    "subject_comparison_data = []\n",
    "for sub in all_unique_subjects:\n",
    "    in_roi_beta = sub if sub in complete_roi_beta_subjects else np.nan\n",
    "    in_smoothness = sub if sub in complete_smoothness_subjects else np.nan\n",
    "    subject_comparison_data.append({\n",
    "        f'{MASK_VALUE} Beta Subjects': in_roi_beta,\n",
    "        'Smoothness Subjects': in_smoothness\n",
    "    })\n",
    "\n",
    "subject_comparison_df = pd.DataFrame(subject_comparison_data)\n",
    "\n",
    "print(\"\\nComparison of Complete Subjects (ROI Beta vs. Smoothness):\")\n",
    "print(subject_comparison_df.to_string(index=False))\n",
    "# --- END NEW ADDITION ---\n",
    "\n",
    "common_complete_subjects = list(set(complete_roi_beta_subjects) & set(complete_smoothness_subjects))\n",
    "common_complete_subjects.sort()\n",
    "print(f\"\\nTotal common complete subjects for combined {MASK_VALUE} {IMG_VALUE} and Smoothness analysis: {len(common_complete_subjects)}\")\n",
    "\n",
    "if not common_complete_subjects:\n",
    "    raise Exception(\"No common complete subjects for analysis.\")\n",
    "\n",
    "roi_beta_data_filtered = roi_beta_data_preprocessed[\n",
    "    roi_beta_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "smoothness_data_filtered = smoothness_data_preprocessed[\n",
    "    smoothness_data_preprocessed['subject'].isin(common_complete_subjects)\n",
    "].copy()\n",
    "\n",
    "for df in [roi_beta_data_filtered, smoothness_data_filtered]:\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "\n",
    "merged_analysis_data = pd.merge(\n",
    "    roi_beta_data_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "    smoothness_data_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner'\n",
    ")\n",
    "merged_analysis_data.dropna(inplace=True)\n",
    "\n",
    "total_subjects_for_lme = merged_analysis_data['subject'].nunique()\n",
    "headcoil_counts_for_lme = merged_analysis_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nFinal dataset for LME analysis contains {total_subjects_for_lme} subjects for {MASK_VALUE} {IMG_VALUE}.\")\n",
    "print(\"Subject count per Headcoil (for LME analysis dataset):\")\n",
    "print(f\"  Headcoil 20: {headcoil_counts_for_lme['20']}\")\n",
    "print(f\"  Headcoil 64: {headcoil_counts_for_lme['64']}\")\n",
    "print(\"\\nFirst 5 rows of the merged data (used for LME):\")\n",
    "print(merged_analysis_data.head())\n",
    "\n",
    "roi_beta_complete_table_for_output = roi_beta_data_filtered.pivot_table(\n",
    "    values=IMG_VALUE,\n",
    "    index='subject',\n",
    "    columns='acq_combined',\n",
    "    aggfunc='mean'\n",
    ").reindex(columns=expected_acq_combinations).dropna().sort_index().round(3)\n",
    "\n",
    "headcoil_df_for_output = roi_beta_data_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output.reset_index().merge(\n",
    "    headcoil_df_for_output.loc[roi_beta_complete_table_for_output.index].reset_index(),\n",
    "    on='subject',\n",
    "    how='left'\n",
    ").set_index('subject')\n",
    "cols_output = ['headcoil'] + expected_acq_combinations\n",
    "roi_beta_complete_table_for_output = roi_beta_complete_table_for_output[cols_output]\n",
    "roi_beta_complete_table_for_output.to_csv(complete_subjects_roi_beta_file)\n",
    "print(f\"\\nComplete {MASK_VALUE} {IMG_VALUE} subjects table saved to '{complete_subjects_roi_beta_file}'\")\n",
    "\n",
    "# Part 2: Linear Mixed Effects Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 2: LINEAR MIXED EFFECTS ANALYSIS WITH SMOOTHNESS COVARIATE FOR {MASK_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    raise Exception(\"Error: Merged data for LME analysis is empty. Cannot run LME.\")\n",
    "\n",
    "print(f\"Running LME analysis on {len(merged_analysis_data)} observations from {merged_analysis_data['subject'].nunique()} subjects.\")\n",
    "\n",
    "data_model_for_lme = merged_analysis_data.copy()\n",
    "# pymer4 automatically handles categorical encoding, so headcoil_encoded might not be strictly needed for pymer4,\n",
    "# but if you use dmatrix explicitly or found it necessary, keep it.\n",
    "# For R's lmer, it prefers factors directly, which we handle in the create_emm_line_plots.\n",
    "# data_model_for_lme['headcoil_encoded'] = data_model_for_lme['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# LME model using pymer4 (this part remains unchanged)\n",
    "lme_formula = f'{IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)' # Use 'headcoil' directly if pymer4 handles factors\n",
    "model = Lmer(lme_formula, data=data_model_for_lme)\n",
    "print(\"Fitting LME model... This may take a moment.\")\n",
    "model.fit()\n",
    "print(\"LME model fitting complete.\")\n",
    "\n",
    "anova_table = model.anova()\n",
    "\n",
    "# Map pymer4 effect names to APA-style names\n",
    "effect_map = {\n",
    "    'headcoil': 'Head Coil', # Changed from headcoil_encoded\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness',\n",
    "    'headcoil:mb': 'Head Coil × Multiband', # Changed from headcoil_encoded:mb\n",
    "    'headcoil:me': 'Head Coil × Multi-echo', # Changed from headcoil_encoded:me\n",
    "    'mb:me': 'Multiband × Multi-echo',\n",
    "    'headcoil:mb:me': 'Head Coil × Multiband × Multi-echo' # Changed from headcoil_encoded:mb:me\n",
    "}\n",
    "\n",
    "# The NumDF values for the main effects and interactions are fixed based on levels:\n",
    "# Head Coil (2 levels) - 1 df\n",
    "# Multiband (3 levels) - 2 df\n",
    "# Multi-echo (2 levels) - 1 df\n",
    "# Smoothness (continuous) - 1 df\n",
    "# Interactions are product of main effect DFs\n",
    "df_dict = {\n",
    "    'Head Coil': 1,\n",
    "    'Multiband': 2,\n",
    "    'Multi-echo': 1,\n",
    "    'Smoothness': 1,\n",
    "    'Head Coil × Multiband': 1 * 2,\n",
    "    'Head Coil × Multi-echo': 1 * 1,\n",
    "    'Multiband × Multi-echo': 2 * 1,\n",
    "    'Head Coil × Multiband × Multi-echo': 1 * 2 * 1\n",
    "}\n",
    "\n",
    "apa_data = []\n",
    "for effect in anova_table.index:\n",
    "    if effect in ['(Intercept)', 'Residuals']:\n",
    "        continue\n",
    "    effect_name = effect_map.get(effect, effect)\n",
    "    apa_data.append({\n",
    "        'Effect': effect_name,\n",
    "        'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "        'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "        'Num df': df_dict.get(effect_name, np.nan),\n",
    "        'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "        'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "        'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "        'Partial η²': np.nan\n",
    "    })\n",
    "\n",
    "try:\n",
    "    # Attempt to get residual variance from pymer4 model's ranef_var\n",
    "    # pymer4's residual variance is often labelled as '__sigma__' or 'residual'\n",
    "    residual_var_row = model.ranef_var.loc[model.ranef_var['Name'].isin(['Residual', '__sigma__']), 'Var']\n",
    "    if not residual_var_row.empty:\n",
    "        residual_var = residual_var_row.iloc[0]\n",
    "    else:\n",
    "        # Fallback if specific residual label not found, often last entry\n",
    "        residual_var = model.ranef_var.iloc[-1]['Var'] if not model.ranef_var.empty else np.nan\n",
    "        print(f\"Warning: 'Residual' variance not explicitly found by name. Using fallback value: {residual_var}.\")\n",
    "except (KeyError, IndexError):\n",
    "    print(\"Warning: Could not determine residual variance from model.ranef_var. Partial eta-squared might be inaccurate.\")\n",
    "    residual_var = np.nan\n",
    "\n",
    "# Calculate partial eta-squared (assuming type III SS from pymer4)\n",
    "# For partial eta-squared, we need SS_effect and SS_error (residual SS)\n",
    "# pymer4's SS are Type 3 by default, suitable for this calculation.\n",
    "# The total sum of squares for the effect and error needs to be sum of SS_effect and SS_Residual.\n",
    "# The `model.ranef_var` contains variance components, not necessarily SS_residual directly for fixed effects.\n",
    "# A common simplification for partial eta-squared in mixed models is SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, without direct SS_error from pymer4's ANOVA for each effect, we rely on total residual variance from the model.\n",
    "# This approximation might not be perfectly aligned with more complex mixed model eta-squared calculations\n",
    "# (e.g., those considering denominator degrees of freedom).\n",
    "# For now, keeping the previous logic of ss_effect / (ss_effect + SS_residual_overall_from_model) as an approximation.\n",
    "\n",
    "# Re-evaluating residual SS calculation:\n",
    "# For lmer/lmerTest, the residual variance (sigma^2) is reported.\n",
    "# SS_residual = residual_variance * residual_df.\n",
    "# From anova_table, DenomDF is often the error df for that effect.\n",
    "# For fixed effects, a simpler approximation of partial eta-squared is SS_effect / (SS_effect + SS_error)\n",
    "# where SS_error could be the sum of squares for the residual from the overall ANOVA table.\n",
    "# pymer4's anova doesn't directly report an 'Error' row with SS.\n",
    "# Let's use the total residual variance from the model for a common (though approximate) partial eta-squared.\n",
    "# The original code's calculation of ss_residual was more of an estimation based on total observations.\n",
    "\n",
    "# A more robust calculation for partial eta-squared from pymer4's output would involve:\n",
    "# For each effect, Partial η² = SS_effect / (SS_effect + SS_error_for_that_effect)\n",
    "# However, pymer4's ANOVA table doesn't directly provide SS_error_for_that_effect.\n",
    "# We'll use the residual variance `residual_var` from `model.ranef_var` as an overall residual variance estimate.\n",
    "# The calculation in the original code for `ss_residual` was:\n",
    "# `residual_var * anova_df['Den Df']` from the R `anova` table.\n",
    "# Since we are using pymer4 here, we have `anova_table['DenomDF']`.\n",
    "# Let's adapt the calculation to be closer to what `lmerTest` does for approximate partial eta-squared.\n",
    "\n",
    "# If DenomDF is available from pymer4's anova, we can use it to derive an approximate SS_error per effect.\n",
    "# This assumes DenomDF represents the error degrees of freedom associated with that F-test.\n",
    "# SS_error_approx = residual_var * anova_table.loc[effect, 'DenomDF']\n",
    "# For simplicity, let's keep the existing loop logic for now, using the overall residual_var\n",
    "# and assuming it's a common denominator for the partial eta-squared.\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    denom_df = row['Den df'] # Denominator DF for this effect\n",
    "    # The true SS_error is complex in mixed models.\n",
    "    # An approximation for partial eta-squared for fixed effects is often\n",
    "    # SS_effect / (SS_effect + SS_Residual from overall model).\n",
    "    # Since pymer4 doesn't give a direct SS_Residual for the fixed effects ANOVA,\n",
    "    # and the 'smoothness' is added, we use the `residual_var` as an overall error component.\n",
    "    # A more precise partial eta^2 for lmer models might require more specific formulas or another R package.\n",
    "    \n",
    "    # Using the logic from the original kernel for the effect size calculation:\n",
    "    # partial_eta2 = SS_effect / (SS_effect + SS_residual)\n",
    "    # The SS_residual was approximated from model deviance or variance components (sigma^2 * Den Df).\n",
    "    \n",
    "    # If we assume `residual_var` (sigma^2) is the common error variance:\n",
    "    # SS_error for a given effect is approximately residual_var * DenomDF for that effect.\n",
    "    if pd.notna(ss_effect) and pd.notna(residual_var) and pd.notna(denom_df) and (ss_effect + (residual_var * denom_df)) > 0:\n",
    "        apa_data[i]['Partial η²'] = ss_effect / (ss_effect + (residual_var * denom_df))\n",
    "    else:\n",
    "        apa_data[i]['Partial η²'] = np.nan\n",
    "\n",
    "\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(\n",
    "    lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A'\n",
    ")\n",
    "apa_table['Partial η²'] = apa_table['Partial η²'].round(3)\n",
    "\n",
    "print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model ({IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)):\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "\n",
    "# Part 3: Estimated Marginal Means (EMMs) Calculation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PART 3: ESTIMATED MARGINAL MEANS (EMMs) FOR {MASK_VALUE} {IMG_VALUE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if merged_analysis_data.empty:\n",
    "    print(\"Skipping EMM plot generation: Merged analysis data is empty.\")\n",
    "else:\n",
    "    # Prepare n_subjects_per_coil for the plotting function\n",
    "    # It should map '20' to count and '64' to count\n",
    "    n_subjects_per_coil_for_plot = headcoil_counts_for_lme.to_dict()\n",
    "    \n",
    "    # Call the EMM plotting function\n",
    "    print(\"Generating EMM line plots...\")\n",
    "    emm_plot_fig = create_emm_line_plots(merged_analysis_data, n_subjects_per_coil_for_plot, IMG_VALUE)\n",
    "    print(f\"EMM line plot saved to '{MASK_VALUE}_{IMG_VALUE}_emm_plot.png'\")\n",
    "    plt.show() # Display the plot in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104ec25-b869-4621-b50f-4f8a26ff59bf",
   "metadata": {},
   "source": [
    "# Base vs. Tedana Confounds: Generating Group-Level Design Matrices for FSL `randomise` (Figure 8)\n",
    "\n",
    "## Make sure this pulls from the full OpenNeuro dataset; largest N possible for each acq\n",
    "\n",
    "This kernel prepares the behavioral and nuisance covariate data for a group-level (L3) analysis using FSL's randomise. It constructs three separate Pandas DataFrames, one for each multi-echo multiband (MB/ME) acquisition type (`mb1me4`, `mb3me4`, `mb6me4`). These DataFrames serve as the \"right side\" of the `randomise` equation.\n",
    "\n",
    "For each acquisition type, the corresponding DataFrame will contain the following columns:\n",
    "- `ones`: A constant regressor (a column of 1s).\n",
    "- `fd_mean_demeaned`: Mean framewise displacement, de-meaned across subjects for that specific acquisition type.\n",
    "- `headcoil_demeaned`: Headcoil type (encoded as 0 for 20-channel and 1 for 64-channel), de-meaned across subjects for that acquisition type.\n",
    "- `fd_mean_x_headcoil_demeaned`: The interaction term between the de-meaned fd_mean and de-meaned headcoil regressors, with this interaction term also de-meaned.\n",
    "- `smoothness_demeaned`: Voxel-wise smoothness, de-meaned across subjects for that specific acquisition type.\n",
    "\n",
    "The script ensures that all subjects from `sublist-complete.txt` are represented in each DataFrame. If a subject is missing `fd_mean` or `smoothness` data for a particular acquisition type, `NaN` values will be inserted into the corresponding DataFrame columns instead of omitting the subject. This allows `randomise` (or other statistical software) to handle missing data appropriately during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cd23d-f3f2-4e35-bdd7-62d005aed423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress SyntaxWarning for '\\s' as r'\\s+' is used below\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING AND TRUNCATING NEW FD_MEAN DATA SOURCE WITH OUTLIER SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update this path if your GitHub repo is not in Documents/GitHub\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-complete.txt\")\n",
    "\n",
    "# >>>>> CORRECTED PATH FOR FD TSV FILE <<<<<\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "\n",
    "print(f\"New FD TSV data path: {FD_TSV_PATH}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load Master Subject List ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    # Format subjects from sublist-complete.txt to match the 'sub-XXXXX' format in the TSV\n",
    "    all_subjects_formatted = [f\"sub-{s}\" for s in all_subjects_raw]\n",
    "    all_subjects_formatted.sort() # Keep it sorted for consistent output\n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH} (formatted to 'sub-XXXXX').\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Please check the path. Exiting.\")\n",
    "    exit() # Exit if the subject list isn't found\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit() # Exit for other errors\n",
    "\n",
    "# --- Step 2: Load the new FD Mean TSV data ---\n",
    "fd_new_df_raw = None # Initialize to None so it exists even if read_csv fails\n",
    "try:\n",
    "    # Using a raw string r'\\s+' for the separator to handle variable whitespace\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Please check the path to your TSV file. Exiting.\")\n",
    "    exit() # Exit if the FD data file isn't found\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit() # Exit for other errors\n",
    "\n",
    "# --- Step 3: Truncate the DataFrame to only include subjects from the sublist ---\n",
    "# This step will only execute if fd_new_df_raw was successfully loaded due to the exit() calls above.\n",
    "# The 'Sub' column in the TSV should match the 'sub-XXXXX' format.\n",
    "# We will filter based on the formatted subject list.\n",
    "filtered_fd_df = fd_new_df_raw[fd_new_df_raw['Sub'].isin(all_subjects_formatted)].copy()\n",
    "\n",
    "print(f\"\\nTruncated FD DataFrame to {len(filtered_fd_df)} records, including only subjects from {SUBLIST_PATH}.\")\n",
    "\n",
    "# --- Step 4: Summarize and list Outlier Runs ---\n",
    "# Ensure the 'outlier_acq_Custom1' column is boolean type for accurate filtering\n",
    "# Coercing errors to NaN and then filling with False to handle any non-boolean values gracefully\n",
    "filtered_fd_df['outlier_acq_Custom1'] = pd.to_numeric(filtered_fd_df['outlier_acq_Custom1'], errors='coerce')\n",
    "filtered_fd_df['outlier_acq_Custom1'] = filtered_fd_df['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "\n",
    "outlier_runs = filtered_fd_df[filtered_fd_df['outlier_acq_Custom1'] == True].copy()\n",
    "num_outlier_runs = len(outlier_runs)\n",
    "\n",
    "print(f\"\\n--- Outlier Run Summary ---\")\n",
    "print(f\"Total number of runs classified as Outlier: {num_outlier_runs}\")\n",
    "\n",
    "if num_outlier_runs > 0:\n",
    "    print(\"\\nDetails of Outlier Runs:\")\n",
    "    # Display all columns for outlier runs, reset display options if needed\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(outlier_runs)\n",
    "    # Reset to default if desired after printing\n",
    "    # pd.reset_option('display.max_columns')\n",
    "    # pd.reset_option('display.width')\n",
    "else:\n",
    "    print(\"No runs classified as Outlier (i.e., 'True' for outlier_acq_Custom1).\")\n",
    "\n",
    "\n",
    "# --- Step 5: Print the truncated DataFrame (head/tail) as before ---\n",
    "print(\"\\n--- Truncated FD Mean DataFrame (Head) ---\")\n",
    "print(filtered_fd_df.head())\n",
    "\n",
    "print(\"\\n--- Truncated FD Mean DataFrame (Tail) ---\")\n",
    "print(filtered_fd_df.tail())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FD DATA TRUNCATION AND OUTLIER SUMMARY COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966f987-b956-41fe-b94a-ea770d4f433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress SyntaxWarning for '\\s' as r'\\s+' is used below\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONSOLIDATING MULTI-ECHO DATA BY AVERAGING ECHOES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration (Copied from previous kernel for self-containment) ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-complete.txt\")\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "print(f\"FD TSV data path: {FD_TSV_PATH}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load Master Subject List (Copied) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    all_subjects_formatted = [f\"sub-{s}\" for s in all_subjects_raw]\n",
    "    all_subjects_formatted.sort()\n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load and Truncate FD Mean TSV (Copied) ---\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "filtered_fd_df = fd_new_df_raw[fd_new_df_raw['Sub'].isin(all_subjects_formatted)].copy()\n",
    "print(f\"Truncated FD DataFrame to {len(filtered_fd_df)} records, including only subjects from {SUBLIST_PATH}.\")\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean type for proper aggregation\n",
    "# Coercing errors to NaN and then filling with False before converting to bool.\n",
    "filtered_fd_df['outlier_acq_Custom1'] = pd.to_numeric(filtered_fd_df['outlier_acq_Custom1'], errors='coerce')\n",
    "filtered_fd_df['outlier_acq_Custom1'] = filtered_fd_df['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# --- Step 3: Consolidate Multi-Echo Data ---\n",
    "print(\"\\n--- Consolidating Multi-Echo Data ---\")\n",
    "\n",
    "# Create a new 'consolidated_acq' column\n",
    "# This regex will extract 'mbXmeY' from strings like 'mb1me4_echo-1_part-mag'\n",
    "# If the pattern doesn't match (e.g., 'mb3me1'), it will keep the original 'acq' value.\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1) # Return the 'mbXmeY' part\n",
    "    return acq_str # Fallback to original if no match (shouldn't happen with given data)\n",
    "\n",
    "filtered_fd_df['consolidated_acq'] = filtered_fd_df['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules:\n",
    "# - For 'tsnr' and 'fd_mean', take the mean.\n",
    "# - For 'outlier_acq_Custom1', take `any()`: True if any of the original echoes were an outlier.\n",
    "# - Other columns like 'task' will be grouped by and implicitly kept as they are constant per group.\n",
    "aggregation_rules = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any' # True if ANY echo was an outlier\n",
    "}\n",
    "\n",
    "# Perform the grouping and aggregation\n",
    "consolidated_df = filtered_fd_df.groupby(['Sub', 'task', 'consolidated_acq'], as_index=False).agg(aggregation_rules)\n",
    "\n",
    "# Rename 'consolidated_acq' back to 'acq' for consistency with original column name expectations\n",
    "consolidated_df = consolidated_df.rename(columns={'consolidated_acq': 'acq'})\n",
    "\n",
    "# Ensure correct column order, if desired\n",
    "consolidated_df = consolidated_df[['Sub', 'task', 'acq', 'tsnr', 'fd_mean', 'outlier_acq_Custom1']]\n",
    "\n",
    "print(f\"Original DataFrame rows: {len(filtered_fd_df)}\")\n",
    "print(f\"Consolidated DataFrame rows: {len(consolidated_df)}\")\n",
    "print(\"Consolidation complete.\")\n",
    "\n",
    "# --- Step 4: Display the consolidated DataFrame (head/tail) ---\n",
    "print(\"\\n--- Consolidated FD Mean DataFrame (Head) ---\")\n",
    "print(consolidated_df.head())\n",
    "\n",
    "print(\"\\n--- Consolidated FD Mean DataFrame (Tail) ---\")\n",
    "print(consolidated_df.tail())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA CONSOLIDATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229268a-e544-4041-adce-b768b748dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning from pandas if needed\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"DataFrame.applymap has been deprecated\"\n",
    ")\n",
    "# Suppress SyntaxWarning for '\\s' if using regular string, though raw string is better practice\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING SMOOTHNESS TABLE FOR MULTI-ECHO ACQUISITIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the CSV file\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/smoothness-all.csv')\n",
    "# Define the subject list path (using openneuro as requested)\n",
    "SUBLIST_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code/sublist-openneuro.txt\")\n",
    "\n",
    "# Define subjects with 64-channel headcoil (used for headcoil assignment)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define the specific multi-echo acquisitions to display\n",
    "MULTI_ECHO_ACQS = ['mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Output CSV file path for the smoothness table\n",
    "SMOOTHNESS_OUTPUT_CSV = os.path.join(os.path.dirname(SUBLIST_PATH), 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "print(f\"Input smoothness file: {csv_path}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"Displaying multi-echo acquisitions: {', '.join(MULTI_ECHO_ACQS)}\")\n",
    "print(f\"Output CSV will be saved to: {SMOOTHNESS_OUTPUT_CSV}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load and Process Smoothness Data ---\n",
    "print(\"\\n--- Processing Smoothness Data ---\")\n",
    "\n",
    "# Define a helper function to consolidate acquisition names\n",
    "# This is crucial for matching 'mbXmeY' from original data sources\n",
    "# which might have echo suffixes in other files but not in smoothness.csv paths.\n",
    "def get_consolidated_acq_name(acq_str):\n",
    "    # Regex to capture 'mbXmeY' from a string, handles cases like 'mb1me4' or 'mb1me4_echo-1_part-mag'\n",
    "    match = re.match(r'(mb\\dme\\d+)(?:_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1) # Returns 'mbXmeY' part\n",
    "    return acq_str # Returns original string if no match (e.g., 'mb3me1')\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, all_subjects_list):\n",
    "    processed_records = [] # To store valid, parsed records\n",
    "    try:\n",
    "        raw_data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Rename columns for clarity based on known structure\n",
    "        data_to_process = raw_data.rename(columns={\n",
    "            raw_data.columns[0]: 'path', # First column is the path string\n",
    "            'Unnamed: 3': 'smoothness'   # Column with the actual smoothness value\n",
    "        })\n",
    "\n",
    "        # Apply shift up procedure: current row's 'smoothness' corresponds to previous row's 'path'\n",
    "        data_to_process['file_path'] = data_to_process['path'].shift(1)\n",
    "        \n",
    "        # Convert 'smoothness' column to numeric, coercing errors to NaN\n",
    "        data_to_process['smoothness'] = pd.to_numeric(data_to_process['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter out rows that do not have a valid smoothness value or a file_path\n",
    "        # Also, explicitly convert 'file_path' to string before iterating to prevent float errors\n",
    "        data_filtered = data_to_process[\n",
    "            data_to_process['smoothness'].notna() & data_to_process['file_path'].notna()\n",
    "        ].copy()\n",
    "        data_filtered['file_path'] = data_filtered['file_path'].astype(str)\n",
    "\n",
    "        # Regex pattern to extract subject ID and the full acquisition string\n",
    "        # from the file_path, e.g., 'sub-XXXXX_..._acq-YYYYY_...'\n",
    "        path_parse_pattern = r'sub-(\\d+).*acq-([a-zA-Z0-9_.-]+)'\n",
    "        \n",
    "        initial_smoothness_rows = 0\n",
    "        for index, row in data_filtered.iterrows():\n",
    "            initial_smoothness_rows += 1 # Count rows that passed initial NaN filter\n",
    "            file_path_str = row['file_path']\n",
    "            smoothness_val = row['smoothness']\n",
    "\n",
    "            # Only attempt regex if the string is not 'nan' (from float NaNs) and is non-empty\n",
    "            if file_path_str != 'nan' and file_path_str.strip() != '':\n",
    "                match = re.search(path_parse_pattern, file_path_str)\n",
    "                if match:\n",
    "                    subject = match.group(1)\n",
    "                    acq_raw = match.group(2)\n",
    "                    \n",
    "                    # Consolidate acq name (e.g., 'mb1me4_echo-1' -> 'mb1me4')\n",
    "                    acq = get_consolidated_acq_name(acq_raw)\n",
    "                    \n",
    "                    # Only include if subject is in our list of non-'sp' subjects\n",
    "                    if subject in all_subjects_list:\n",
    "                        processed_records.append({\n",
    "                            'subject': subject,\n",
    "                            'acq': acq,\n",
    "                            'smoothness': smoothness_val\n",
    "                        })\n",
    "                # else: print(f\"Debug: No regex match for path: {file_path_str}\") # Uncomment for debugging\n",
    "            # else: print(f\"Debug: Skipping due to invalid file_path_str: '{file_path_str}'\") # Uncomment for debugging\n",
    "\n",
    "        if not processed_records:\n",
    "            print(\"  No valid smoothness records found after parsing and filtering.\")\n",
    "            return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "\n",
    "        data_parsed = pd.DataFrame(processed_records)\n",
    "        data_parsed['subject'] = data_parsed['subject'].astype(str)\n",
    "        data_parsed['acq'] = data_parsed['acq'].astype(str) # Ensure acq is string before next step\n",
    "\n",
    "        # Assign headcoil based on our defined HEADCOIL_64_SUBJECTS\n",
    "        data_parsed['headcoil'] = data_parsed['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20')\n",
    "        \n",
    "        # Select relevant columns before grouping\n",
    "        data_parsed = data_parsed[['subject', 'headcoil', 'acq', 'smoothness']].copy()\n",
    "        \n",
    "        # Group by subject, headcoil, and consolidated acquisition, then take the mean of smoothness.\n",
    "        # This handles potential multiple entries for the same (subject, acq) combination,\n",
    "        # ensuring a unique smoothness value per group.\n",
    "        initial_rows_before_grouping = len(data_parsed)\n",
    "        data_final = data_parsed.groupby(['subject', 'headcoil', 'acq'], as_index=False)['smoothness'].mean()\n",
    "        \n",
    "        print(f\"  Smoothness data records initially extracted: {initial_smoothness_rows}\")\n",
    "        print(f\"  Smoothness records after subject/acq/smoothness filtering: {len(data_parsed)}\")\n",
    "        print(f\"  Smoothness data records after de-duplication/averaging: {len(data_final)}\")\n",
    "        \n",
    "        print(f\"Loaded and filtered {len(data_final)} smoothness records.\")\n",
    "        return data_final\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # Re-raise the exception for full traceback during debugging if needed.\n",
    "        # raise\n",
    "        return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "\n",
    "# --- Load and Process Smoothness Data using the refined function ---\n",
    "smoothness_df = load_and_process_smoothness_data(csv_path, all_subjects)\n",
    "\n",
    "\n",
    "# --- Step 3: Create the final table for display and saving ---\n",
    "print(\"\\n--- Creating Final Smoothness Table ---\")\n",
    "\n",
    "if smoothness_df.empty:\n",
    "    print(\"Error: Smoothness data could not be loaded or processed. Cannot create table.\")\n",
    "else:\n",
    "    # Filter to include only the desired multi-echo acquisitions\n",
    "    smoothness_filtered_for_display = smoothness_df[\n",
    "        smoothness_df['acq'].isin(MULTI_ECHO_ACQS)\n",
    "    ].copy()\n",
    "\n",
    "    # Ensure all subjects from 'all_subjects' are represented, even if they have no smoothness data\n",
    "    # for the selected acquisitions.\n",
    "    # Create a base DataFrame with all relevant subject-acq combinations.\n",
    "    base_display_combinations = pd.MultiIndex.from_product(\n",
    "        [all_subjects, MULTI_ECHO_ACQS], \n",
    "        names=['subject', 'acq']\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Merge the filtered smoothness data onto this base, ensuring all subjects are present.\n",
    "    # This will introduce NaNs for subjects/acqs where data is missing.\n",
    "    final_display_df = pd.merge(\n",
    "        base_display_combinations,\n",
    "        smoothness_filtered_for_display[['subject', 'acq', 'smoothness']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Pivot the table to get acquisitions as columns and subjects as rows\n",
    "    smoothness_pivot = final_display_df.pivot_table(\n",
    "        values='smoothness',\n",
    "        index='subject',\n",
    "        columns='acq',\n",
    "        aggfunc='first' # 'first' is fine here as we already handled duplicates with .mean() earlier\n",
    "    )\n",
    "\n",
    "    # Reindex columns to ensure desired order of multi-echo acquisitions\n",
    "    smoothness_pivot = smoothness_pivot.reindex(columns=MULTI_ECHO_ACQS)\n",
    "    \n",
    "    # Get headcoil information for all_subjects and merge (should be consistent)\n",
    "    headcoil_info = pd.DataFrame({\n",
    "        'subject': all_subjects,\n",
    "        'headcoil': [('64' if s in HEADCOIL_64_SUBJECTS else '20') for s in all_subjects]\n",
    "    }).set_index('subject')\n",
    "\n",
    "    # Merge headcoil info onto the pivoted table\n",
    "    smoothness_pivot = smoothness_pivot.merge(headcoil_info, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Reorder columns: headcoil first, then the acq types\n",
    "    cols_order = ['headcoil'] + MULTI_ECHO_ACQS\n",
    "    smoothness_pivot = smoothness_pivot[cols_order].sort_index()\n",
    "\n",
    "    # Round smoothness values for display\n",
    "    smoothness_pivot[MULTI_ECHO_ACQS] = smoothness_pivot[MULTI_ECHO_ACQS].round(3)\n",
    "\n",
    "    print(f\"\\nFinal Smoothness Table (Multi-echo Acquisitions Only, All Subjects):\")\n",
    "    # Set display options to show entire DataFrame\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(smoothness_pivot.to_string())\n",
    "    # Reset options to default\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    # Save the generated table to CSV\n",
    "    try:\n",
    "        smoothness_pivot.to_csv(SMOOTHNESS_OUTPUT_CSV)\n",
    "        print(f\"\\nSmoothness table saved to '{SMOOTHNESS_OUTPUT_CSV}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving smoothness table to CSV: {e}\")\n",
    "\n",
    "    # Print a summary of NaNs for these specific acquisitions\n",
    "    print(f\"\\nSummary of missing Smoothness data for multi-echo acquisitions:\")\n",
    "    for acq in MULTI_ECHO_ACQS:\n",
    "        missing_count = smoothness_pivot[acq].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_subs = smoothness_pivot[smoothness_pivot[acq].isna()].index.tolist()\n",
    "            print(f\"  {acq}: {missing_count} subjects missing data: {missing_subs}\")\n",
    "        else:\n",
    "            print(f\"  {acq}: No missing data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SMOOTHNESS TABLE GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f99a1-8c8c-4b94-9548-028eeaca8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING GROUP-LEVEL DESIGN MATRICES FOR RANDOMISE\")\n",
    "print(\"  (Using Consolidated FD_MEAN and Pre-Processed Smoothness Table)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "\n",
    "# Subject list path\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-openneuro.txt\")\n",
    "\n",
    "# Corrected path for the FD TSV file (original raw source)\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "# Path to the regenerated missing FD mean values\n",
    "RECALCULATED_FD_PATH = os.path.join(BASE_CODE_DIR, 'missing_fd_mean_recalculated.csv')\n",
    "\n",
    "# UPDATED: Path to the new, pre-processed smoothness table CSV\n",
    "SMOOTHNESS_TABLE_PATH = os.path.join(BASE_CODE_DIR, 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "# Define headcoil assignments (from previous kernel)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define acquisition types for which to generate dataframes (these are the consolidated ones)\n",
    "ACQ_TYPES = [\"mb1me4\", \"mb3me4\", \"mb6me4\"] # These are the *target* acq types after consolidation\n",
    "\n",
    "# Output directory for design matrices\n",
    "DESIGN_MATRIX_OUTPUT_DIR = os.path.join(BASE_CODE_DIR, 'design_matrices')\n",
    "os.makedirs(DESIGN_MATRIX_OUTPUT_DIR, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"FD TSV data path (original source): {FD_TSV_PATH}\")\n",
    "print(f\"Recalculated FD data path: {RECALCULATED_FD_PATH}\")\n",
    "print(f\"Smoothness table path (NEW SOURCE): {SMOOTHNESS_TABLE_PATH}\")\n",
    "print(f\"Target acquisition types (for consolidation): {', '.join(ACQ_TYPES)}\")\n",
    "print(f\"Design matrices will be saved to: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load, Truncate, and Consolidate FD Mean Data ---\n",
    "print(\"\\n--- Processing FD Mean Data ---\")\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Filter by subjects in our master list (non-'sp' subjects) and strip 'sub-' for consistency\n",
    "fd_df_filtered_raw = fd_new_df_raw[fd_new_df_raw['Sub'].isin([f\"sub-{s}\" for s in all_subjects])].copy()\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['Sub'].str.replace('sub-', '')\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['subject'].astype(str)\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean for proper aggregation\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = pd.to_numeric(fd_df_filtered_raw['outlier_acq_Custom1'], errors='coerce')\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = fd_df_filtered_raw['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# Create a new 'consolidated_acq' column by stripping echo info\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return acq_str\n",
    "\n",
    "fd_df_filtered_raw['consolidated_acq'] = fd_df_filtered_raw['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules for consolidation\n",
    "aggregation_rules_fd = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any'\n",
    "}\n",
    "\n",
    "# --- Initialize consolidated_fd_df with all expected subject-acq combinations ---\n",
    "# This ensures that even if original data is missing for a combination, a row exists with NaN\n",
    "base_combinations_fd = pd.MultiIndex.from_product([all_subjects, ACQ_TYPES], names=['subject', 'acq']).to_frame(index=False)\n",
    "\n",
    "# Perform the grouping and aggregation on the raw data\n",
    "temp_consolidated_fd_df = fd_df_filtered_raw.groupby(['subject', 'consolidated_acq'], as_index=False).agg(aggregation_rules_fd)\n",
    "temp_consolidated_fd_df = temp_consolidated_fd_df.rename(columns={'consolidated_acq': 'acq'}) # Rename back\n",
    "\n",
    "# Merge with the base combinations to ensure all subject-acq pairs are present\n",
    "consolidated_fd_df = pd.merge(base_combinations_fd, temp_consolidated_fd_df, on=['subject', 'acq'], how='left')\n",
    "\n",
    "\n",
    "print(f\"Original FD data records (after initial subject filter): {len(fd_df_filtered_raw)}\")\n",
    "print(f\"Consolidated FD data records (after echo averaging): {len(temp_consolidated_fd_df)}\")\n",
    "print(f\"Final consolidated_fd_df records (including missing pairs): {len(consolidated_fd_df)}\")\n",
    "print(\"FD data consolidation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2a: Incorporate Recalculated Missing FD Mean Data ---\n",
    "print(\"\\n--- Incorporating Recalculated FD Mean Data ---\")\n",
    "try:\n",
    "    recalculated_fd_df = pd.read_csv(RECALCULATED_FD_PATH)\n",
    "    # Rename 'acquisition' column to 'acq' to match consolidated_fd_df\n",
    "    recalculated_fd_df = recalculated_fd_df.rename(columns={'acquisition': 'acq'})\n",
    "    \n",
    "    # Ensure correct data types for merging/updating\n",
    "    recalculated_fd_df['subject'] = recalculated_fd_df['subject'].astype(str)\n",
    "    recalculated_fd_df['acq'] = recalculated_fd_df['acq'].astype(str)\n",
    "    recalculated_fd_df['fd_mean_recalc'] = pd.to_numeric(recalculated_fd_df['fd_mean'], errors='coerce') \n",
    "\n",
    "    # Before update, count NaNs in fd_mean\n",
    "    nan_count_before_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df BEFORE update: {nan_count_before_update}\")\n",
    "\n",
    "    # Merge the recalculated data into the consolidated dataframe\n",
    "    # This adds a 'fd_mean_recalc' column.\n",
    "    consolidated_fd_df = pd.merge(\n",
    "        consolidated_fd_df,\n",
    "        recalculated_fd_df[['subject', 'acq', 'fd_mean_recalc']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs in the original 'fd_mean' column using values from 'fd_mean_recalc'\n",
    "    # This will only fill if 'fd_mean' is NaN AND 'fd_mean_recalc' is NOT NaN.\n",
    "    consolidated_fd_df['fd_mean'] = consolidated_fd_df['fd_mean'].fillna(\n",
    "        consolidated_fd_df['fd_mean_recalc']\n",
    "    )\n",
    "\n",
    "    # Drop the temporary 'fd_mean_recalc' column\n",
    "    consolidated_fd_df = consolidated_fd_df.drop(columns=['fd_mean_recalc'])\n",
    "\n",
    "    # After update, count NaNs in fd_mean\n",
    "    nan_count_after_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df AFTER update: {nan_count_after_update}\")\n",
    "\n",
    "    if nan_count_before_update > nan_count_after_update:\n",
    "        print(\"  Successfully filled some missing FD mean values from recalculated data.\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update == 0:\n",
    "        print(\"  No missing FD mean values were present to update (already complete).\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update > 0:\n",
    "        print(\"  Warning: Some missing FD mean values still persist. This means the recalculated data also contained NaNs or did not cover all missing entries.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Recalculated FD data file not found at {RECALCULATED_FD_PATH}. Skipping update.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error incorporating recalculated FD data: {e}. Skipping update.\")\n",
    "    # raise # Uncomment for detailed traceback during debugging if needed\n",
    "print(\"Recalculated FD mean data incorporation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Load Pre-Processed Smoothness Data (and Melt) ---\n",
    "print(\"\\n--- Processing Smoothness Data (from pre-processed table) ---\")\n",
    "def load_preprocessed_smoothness_data(csv_path, all_subjects_list, target_acqs_list):\n",
    "    try:\n",
    "        # Read the CSV. The 'subject' column is the index by default if not specified.\n",
    "        # Given the previous kernel output, 'subject' is likely the first column but not the index.\n",
    "        # Let's read it without index_col and explicitly set it later if needed.\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure 'subject' column is recognized correctly\n",
    "        if 'subject' not in data.columns and data.index.name == 'subject':\n",
    "            data = data.reset_index() # If 'subject' was the index, make it a column\n",
    "\n",
    "        # Ensure 'subject' and 'headcoil' are strings for consistency\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data['headcoil'] = data['headcoil'].astype(str)\n",
    "\n",
    "        # Melt the DataFrame from wide to long format\n",
    "        # The columns to melt are the target_acqs_list.\n",
    "        acq_columns_to_melt = [col for col in data.columns if col in target_acqs_list]\n",
    "        \n",
    "        if not acq_columns_to_melt:\n",
    "            print(\"  Warning: No target acquisition columns found in the smoothness table CSV for melting. Returning empty DataFrame for smoothness.\")\n",
    "            return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "            \n",
    "        data_long = pd.melt(\n",
    "            data,\n",
    "            id_vars=['subject', 'headcoil'],\n",
    "            value_vars=acq_columns_to_melt,\n",
    "            var_name='acq',\n",
    "            value_name='smoothness'\n",
    "        )\n",
    "\n",
    "        # Ensure 'smoothness' column is numeric, coercing errors to NaN\n",
    "        data_long['smoothness'] = pd.to_numeric(data_long['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter by our subjects (non-'sp')\n",
    "        data_long = data_long[data_long['subject'].isin(all_subjects_list)].copy()\n",
    "        \n",
    "        # Filter to include only the desired multi-echo acquisitions (redundant with melt, but safe check)\n",
    "        data_long = data_long[data_long['acq'].isin(target_acqs_list)].copy()\n",
    "        \n",
    "        print(f\"  Smoothness records loaded from '{os.path.basename(csv_path)}' and melted: {len(data_long)}\")\n",
    "        print(f\"Loaded and processed {len(data_long)} smoothness records from pre-processed table.\")\n",
    "        return data_long[['subject', 'acq', 'smoothness']] # Only return relevant columns for merge\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Pre-processed smoothness table not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading pre-processed smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # raise # Uncomment this to see the full traceback if the error persists.\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "\n",
    "# Call the updated function to load pre-processed smoothness data\n",
    "smoothness_df = load_preprocessed_smoothness_data(SMOOTHNESS_TABLE_PATH, all_subjects, ACQ_TYPES)\n",
    "\n",
    "\n",
    "# --- Step 4: Generate Headcoil Encoded Data ---\n",
    "print(\"\\n--- Generating Headcoil Data ---\")\n",
    "headcoil_map = {sub_id: 1 if sub_id in HEADCOIL_64_SUBJECTS else 0 for sub_id in all_subjects}\n",
    "headcoil_series = pd.Series(headcoil_map, name='headcoil_encoded').rename_axis('subject').reset_index()\n",
    "headcoil_series['subject'] = headcoil_series['subject'].astype(str)\n",
    "print(f\"Generated headcoil encoding for {len(headcoil_series)} subjects.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Create Design Matrices for Each Acquisition Type ---\n",
    "design_matrices = {}\n",
    "\n",
    "for acq_type in ACQ_TYPES:\n",
    "    print(f\"\\n--- Constructing Design Matrix for {acq_type} ---\")\n",
    "\n",
    "    # Initialize DataFrame with all subjects and the constant 'ones'\n",
    "    df = pd.DataFrame({'subject': all_subjects})\n",
    "    df['ones'] = 1\n",
    "\n",
    "    # Merge Consolidated FD Mean data for the current acq_type\n",
    "    fd_mean_filtered_for_acq = consolidated_fd_df[consolidated_fd_df['acq'] == acq_type][['subject', 'fd_mean']].copy()\n",
    "    df = pd.merge(df, fd_mean_filtered_for_acq, on='subject', how='left')\n",
    "\n",
    "    # Merge Headcoil data (constant for all acq types per subject)\n",
    "    df = pd.merge(df, headcoil_series, on='subject', how='left')\n",
    "\n",
    "    # Ensure correct column types, coercing errors to NaN\n",
    "    df['fd_mean'] = pd.to_numeric(df['fd_mean'], errors='coerce')\n",
    "    df['headcoil_encoded'] = pd.to_numeric(df['headcoil_encoded'], errors='coerce') # Should not have NaNs if all_subjects are covered\n",
    "\n",
    "    # --- De-meaning (FD & Headcoil) and Interaction Calculation ---\n",
    "    # Calculate means, ignoring NaNs. Pandas .mean() skips NaNs by default.\n",
    "    mean_fd_mean = df['fd_mean'].mean()\n",
    "    mean_headcoil_encoded = df['headcoil_encoded'].mean()\n",
    "\n",
    "    # De-mean individual columns. NaNs will remain NaN after subtraction.\n",
    "    df['fd_mean_demeaned'] = df['fd_mean'] - mean_fd_mean\n",
    "    df['headcoil_demeaned'] = df['headcoil_encoded'] - mean_headcoil_encoded\n",
    "\n",
    "    # Calculate raw interaction (will propagate NaNs from its components)\n",
    "    df['interaction_raw'] = df['fd_mean_demeaned'] * df['headcoil_demeaned']\n",
    "    \n",
    "    # De-mean the interaction term itself\n",
    "    mean_interaction_raw = df['interaction_raw'].mean()\n",
    "    df['fd_mean_x_headcoil_demeaned'] = df['interaction_raw'] - mean_interaction_raw\n",
    "\n",
    "\n",
    "    # --- NEW: Append Smoothness Data (after initial dataframe assembly) ---\n",
    "    if not smoothness_df.empty:\n",
    "        smoothness_filtered_for_acq = smoothness_df[smoothness_df['acq'] == acq_type][['subject', 'smoothness']].copy()\n",
    "        \n",
    "        # Merge this filtered smoothness data onto the main df\n",
    "        df = pd.merge(df, smoothness_filtered_for_acq, on='subject', how='left')\n",
    "        \n",
    "        # Ensure smoothness is numeric after merge\n",
    "        df['smoothness'] = pd.to_numeric(df['smoothness'], errors='coerce')\n",
    "\n",
    "        # De-mean smoothness\n",
    "        mean_smoothness = df['smoothness'].mean() # Calculate mean of merged smoothness\n",
    "        df['smoothness_demeaned'] = df['smoothness'] - mean_smoothness\n",
    "    else:\n",
    "        print(f\"  Warning: Pre-processed smoothness data is empty. 'smoothness' and 'smoothness_demeaned' columns will be all NaNs for {acq_type}.\")\n",
    "        df['smoothness'] = np.nan\n",
    "        df['smoothness_demeaned'] = np.nan # Ensure de-meaned column also exists as NaN\n",
    "\n",
    "\n",
    "    # Select and reorder final columns for the design matrix\n",
    "    final_cols = [\n",
    "        'subject',\n",
    "        'ones',\n",
    "        'fd_mean_demeaned',\n",
    "        'headcoil_demeaned',\n",
    "        'fd_mean_x_headcoil_demeaned',\n",
    "        'smoothness_demeaned'\n",
    "    ]\n",
    "    design_matrix_df = df[final_cols].copy()\n",
    "\n",
    "    # --- Drop rows with NaN in fd_mean_demeaned (as requested) ---\n",
    "    original_subject_count_before_fd_drop = len(design_matrix_df)\n",
    "    missing_fd_subjects_before_drop = design_matrix_df[design_matrix_df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    \n",
    "    if missing_fd_subjects_before_drop:\n",
    "        print(f\"  Subjects with missing FD data (before dropping): {len(missing_fd_subjects_before_drop)}\")\n",
    "        print(f\"  Listing missing FD subjects: {missing_fd_subjects_before_drop}\")\n",
    "        design_matrix_df = design_matrix_df.dropna(subset=['fd_mean_demeaned']).copy()\n",
    "        subjects_dropped_fd = original_subject_count_before_fd_drop - len(design_matrix_df)\n",
    "        print(f\"  Dropped {subjects_dropped_fd} subjects due to missing FD data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing FD data found in {acq_type} to drop.\")\n",
    "\n",
    "    # --- NEW: Drop rows with NaN in smoothness_demeaned ---\n",
    "    original_subject_count_after_fd_drop = len(design_matrix_df)\n",
    "    missing_smoothness_subjects_before_drop = design_matrix_df[design_matrix_df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_smoothness_subjects_before_drop:\n",
    "        print(f\"  Subjects with missing Smoothness data (before dropping): {len(missing_smoothness_subjects_before_drop)}\")\n",
    "        print(f\"  Listing missing Smoothness subjects: {missing_smoothness_subjects_before_drop}\")\n",
    "        design_matrix_df = design_matrix_df.dropna(subset=['smoothness_demeaned']).copy()\n",
    "        subjects_dropped_smoothness = original_subject_count_after_fd_drop - len(design_matrix_df)\n",
    "        print(f\"  Dropped {subjects_dropped_smoothness} subjects due to missing Smoothness data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing Smoothness data found in {acq_type} to drop.\")\n",
    "\n",
    "\n",
    "    # Store the DataFrame\n",
    "    design_matrices[acq_type] = design_matrix_df\n",
    "    print(f\"Design Matrix for {acq_type} created. Final number of subjects: {len(design_matrix_df)}\")\n",
    "    # Re-check NaNs after final drops (should be 0 for both)\n",
    "    print(f\"Subjects with missing FD data (after final drop): {design_matrices[acq_type]['fd_mean_demeaned'].isna().sum()}\")\n",
    "    print(f\"Subjects with missing Smoothness data (after final drop): {design_matrices[acq_type]['smoothness_demeaned'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# --- Display Sample Heads ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE HEADS OF GENERATED DESIGN MATRICES (AFTER ALL NaNs DROPPED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    print(f\"\\n--- Design Matrix for {acq_type} (First 10 rows) ---\")\n",
    "    # Display more rows for better inspection, adjust as needed\n",
    "    pd.set_option('display.max_rows', 10) # Show more rows for inspection\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df.head(10)) # Print only first 10 rows\n",
    "    pd.reset_option('display.max_rows') # Reset after printing\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Final Check on NaNs after Dropping ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CHECK: SUBJECTS WITH MISSING FD_MEAN AND SMOOTHNESS DATA (SHOULD BE ZERO AFTER DROP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    missing_fd_subjects_final = df[df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    missing_smoothness_subjects_final = df[df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_fd_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_fd_subjects_final)} subjects still have missing FD data:\")\n",
    "        print(f\"  {missing_fd_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing FD data (as expected).\")\n",
    "\n",
    "    if missing_smoothness_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_smoothness_subjects_final)} subjects still have missing Smoothness data:\")\n",
    "        print(f\"  {missing_smoothness_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing Smoothness data (as expected).\")\n",
    "\n",
    "# --- NEW: Save Design Matrices to CSV files ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"SAVING DESIGN MATRICES TO CSV FILES IN: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    output_filename = os.path.join(DESIGN_MATRIX_OUTPUT_DIR, f'design_matrix_{acq_type}.csv')\n",
    "    try:\n",
    "        # Save without the 'subject' column as index, just as data\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved '{output_filename}' with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "print(\"\\nAll design matrices generated, adjusted, and saved.\")\n",
    "print(\"================================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea02d4-cebb-41d0-b932-87f826e77157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING GROUP-LEVEL DESIGN MATRICES FOR RANDOMISE\")\n",
    "print(\"  (Using Consolidated FD_MEAN and Pre-Processed Smoothness Table)\")\n",
    "print(\"  *** Ensured proper de-meaning after all filtering ***\") # Highlight this change\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "\n",
    "# Subject list path\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-openneuro.txt\")\n",
    "\n",
    "# Corrected path for the FD TSV file (original raw source)\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "# Path to the regenerated missing FD mean values\n",
    "RECALCULATED_FD_PATH = os.path.join(BASE_CODE_DIR, 'missing_fd_mean_recalculated.csv')\n",
    "\n",
    "# UPDATED: Path to the new, pre-processed smoothness table CSV\n",
    "SMOOTHNESS_TABLE_PATH = os.path.join(BASE_CODE_DIR, 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "# Define headcoil assignments (from previous kernel)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define acquisition types for which to generate dataframes (these are the consolidated ones)\n",
    "ACQ_TYPES = [\"mb1me4\", \"mb3me4\", \"mb6me4\"] # These are the *target* acq types after consolidation\n",
    "\n",
    "# Output directory for design matrices\n",
    "DESIGN_MATRIX_OUTPUT_DIR = os.path.join(BASE_CODE_DIR, 'design_matrices')\n",
    "os.makedirs(DESIGN_MATRIX_OUTPUT_DIR, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"FD TSV data path (original source): {FD_TSV_PATH}\")\n",
    "print(f\"Recalculated FD data path: {RECALCULATED_FD_PATH}\")\n",
    "print(f\"Smoothness table path (NEW SOURCE): {SMOOTHNESS_TABLE_PATH}\")\n",
    "print(f\"Target acquisition types (for consolidation): {', '.join(ACQ_TYPES)}\")\n",
    "print(f\"Design matrices will be saved to: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load, Truncate, and Consolidate FD Mean Data ---\n",
    "print(\"\\n--- Processing FD Mean Data ---\")\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Filter by subjects in our master list (non-'sp' subjects) and strip 'sub-' for consistency\n",
    "fd_df_filtered_raw = fd_new_df_raw[fd_new_df_raw['Sub'].isin([f\"sub-{s}\" for s in all_subjects])].copy()\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['Sub'].str.replace('sub-', '')\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['subject'].astype(str)\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean for proper aggregation\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = pd.to_numeric(fd_df_filtered_raw['outlier_acq_Custom1'], errors='coerce')\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = fd_df_filtered_raw['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# Create a new 'consolidated_acq' column by stripping echo info\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return acq_str\n",
    "\n",
    "fd_df_filtered_raw['consolidated_acq'] = fd_df_filtered_raw['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules for consolidation\n",
    "aggregation_rules_fd = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any'\n",
    "}\n",
    "\n",
    "# --- Initialize consolidated_fd_df with all expected subject-acq combinations ---\n",
    "# This ensures that even if original data is missing for a combination, a row exists with NaN\n",
    "base_combinations_fd = pd.MultiIndex.from_product([all_subjects, ACQ_TYPES], names=['subject', 'acq']).to_frame(index=False)\n",
    "\n",
    "# Perform the grouping and aggregation on the raw data\n",
    "temp_consolidated_fd_df = fd_df_filtered_raw.groupby(['subject', 'consolidated_acq'], as_index=False).agg(aggregation_rules_fd)\n",
    "temp_consolidated_fd_df = temp_consolidated_fd_df.rename(columns={'consolidated_acq': 'acq'}) # Rename back\n",
    "\n",
    "# Merge with the base combinations to ensure all subject-acq pairs are present\n",
    "consolidated_fd_df = pd.merge(base_combinations_fd, temp_consolidated_fd_df, on=['subject', 'acq'], how='left')\n",
    "\n",
    "\n",
    "print(f\"Original FD data records (after initial subject filter): {len(fd_df_filtered_raw)}\")\n",
    "print(f\"Consolidated FD data records (after echo averaging): {len(temp_consolidated_fd_df)}\")\n",
    "print(f\"Final consolidated_fd_df records (including missing pairs): {len(consolidated_fd_df)}\")\n",
    "print(\"FD data consolidation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2a: Incorporate Recalculated Missing FD Mean Data ---\n",
    "print(\"\\n--- Incorporating Recalculated FD Mean Data ---\")\n",
    "try:\n",
    "    recalculated_fd_df = pd.read_csv(RECALCULATED_FD_PATH)\n",
    "    # Rename 'acquisition' column to 'acq' to match consolidated_fd_df\n",
    "    recalculated_fd_df = recalculated_fd_df.rename(columns={'acquisition': 'acq'})\n",
    "    \n",
    "    # Ensure correct data types for merging/updating\n",
    "    recalculated_fd_df['subject'] = recalculated_fd_df['subject'].astype(str)\n",
    "    recalculated_fd_df['acq'] = recalculated_fd_df['acq'].astype(str)\n",
    "    recalculated_fd_df['fd_mean_recalc'] = pd.to_numeric(recalculated_fd_df['fd_mean'], errors='coerce') \n",
    "\n",
    "    # Before update, count NaNs in fd_mean\n",
    "    nan_count_before_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df BEFORE update: {nan_count_before_update}\")\n",
    "\n",
    "    # Merge the recalculated data into the consolidated dataframe\n",
    "    # This adds a 'fd_mean_recalc' column.\n",
    "    consolidated_fd_df = pd.merge(\n",
    "        consolidated_fd_df,\n",
    "        recalculated_fd_df[['subject', 'acq', 'fd_mean_recalc']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs in the original 'fd_mean' column using values from 'fd_mean_recalc'\n",
    "    # This will only fill if 'fd_mean' is NaN AND 'fd_mean_recalc' is NOT NaN.\n",
    "    consolidated_fd_df['fd_mean'] = consolidated_fd_df['fd_mean'].fillna(\n",
    "        consolidated_fd_df['fd_mean_recalc']\n",
    "    )\n",
    "\n",
    "    # Drop the temporary 'fd_mean_recalc' column\n",
    "    consolidated_fd_df = consolidated_fd_df.drop(columns=['fd_mean_recalc'])\n",
    "\n",
    "    # After update, count NaNs in fd_mean\n",
    "    nan_count_after_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df AFTER update: {nan_count_after_update}\")\n",
    "\n",
    "    if nan_count_before_update > nan_count_after_update:\n",
    "        print(\"  Successfully filled some missing FD mean values from recalculated data.\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update == 0:\n",
    "        print(\"  No missing FD mean values were present to update (already complete).\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update > 0:\n",
    "        print(\"  Warning: Some missing FD mean values still persist. This means the recalculated data also contained NaNs or did not cover all missing entries.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Recalculated FD data file not found at {RECALCULATED_FD_PATH}. Skipping update.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error incorporating recalculated FD data: {e}. Skipping update.\")\n",
    "    # raise # Uncomment for detailed traceback during debugging if needed\n",
    "print(\"Recalculated FD mean data incorporation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Load Pre-Processed Smoothness Data (and Melt) ---\n",
    "print(\"\\n--- Processing Smoothness Data (from pre-processed table) ---\")\n",
    "def load_preprocessed_smoothness_data(csv_path, all_subjects_list, target_acqs_list):\n",
    "    try:\n",
    "        # Read the CSV. The 'subject' column is the index by default if not specified.\n",
    "        # Given the previous kernel output, 'subject' is likely the first column but not the index.\n",
    "        # Let's read it without index_col and explicitly set it later if needed.\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure 'subject' column is recognized correctly\n",
    "        if 'subject' not in data.columns and data.index.name == 'subject':\n",
    "            data = data.reset_index() # If 'subject' was the index, make it a column\n",
    "\n",
    "        # Ensure 'subject' and 'headcoil' are strings\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data['headcoil'] = data['headcoil'].astype(str) # Keep headcoil as string\n",
    "\n",
    "        # Melt the DataFrame from wide to long format\n",
    "        # The columns to melt are the target_acqs_list.\n",
    "        acq_columns_to_melt = [col for col in data.columns if col in target_acqs_list]\n",
    "        \n",
    "        if not acq_columns_to_melt:\n",
    "            print(\"  Warning: No target acquisition columns found in the smoothness table CSV for melting. Returning empty DataFrame for smoothness.\")\n",
    "            return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "            \n",
    "        data_long = pd.melt(\n",
    "            data,\n",
    "            id_vars=['subject', 'headcoil'],\n",
    "            value_vars=acq_columns_to_melt,\n",
    "            var_name='acq',\n",
    "            value_name='smoothness'\n",
    "        )\n",
    "\n",
    "        # Ensure 'smoothness' column is numeric, coercing errors to NaN\n",
    "        data_long['smoothness'] = pd.to_numeric(data_long['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter by our subjects (non-'sp')\n",
    "        data_long = data_long[data_long['subject'].isin(all_subjects_list)].copy()\n",
    "        \n",
    "        # Filter to include only the desired multi-echo acquisitions (redundant with melt, but safe check)\n",
    "        data_long = data_long[data_long['acq'].isin(target_acqs_list)].copy()\n",
    "        \n",
    "        print(f\"  Smoothness records loaded from '{os.path.basename(csv_path)}' and melted: {len(data_long)}\")\n",
    "        print(f\"Loaded and processed {len(data_long)} smoothness records from pre-processed table.\")\n",
    "        return data_long[['subject', 'acq', 'smoothness']] # Only return relevant columns for merge\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Pre-processed smoothness table not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading pre-processed smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # raise # Uncomment this to see the full traceback if the error persists.\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "\n",
    "# Call the updated function to load pre-processed smoothness data\n",
    "smoothness_df = load_preprocessed_smoothness_data(SMOOTHNESS_TABLE_PATH, all_subjects, ACQ_TYPES)\n",
    "\n",
    "\n",
    "# --- Step 4: Generate Headcoil Encoded Data ---\n",
    "print(\"\\n--- Generating Headcoil Data ---\")\n",
    "headcoil_map = {sub_id: 1 if sub_id in HEADCOIL_64_SUBJECTS else 0 for sub_id in all_subjects}\n",
    "headcoil_series = pd.Series(headcoil_map, name='headcoil_encoded').rename_axis('subject').reset_index()\n",
    "headcoil_series['subject'] = headcoil_series['subject'].astype(str)\n",
    "print(f\"Generated headcoil encoding for {len(headcoil_series)} subjects.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Create Design Matrices for Each Acquisition Type ---\n",
    "design_matrices = {}\n",
    "\n",
    "for acq_type in ACQ_TYPES:\n",
    "    print(f\"\\n--- Constructing Design Matrix for {acq_type} ---\")\n",
    "\n",
    "    # Initialize DataFrame with all subjects and the constant 'ones'\n",
    "    df = pd.DataFrame({'subject': all_subjects})\n",
    "    df['ones'] = 1\n",
    "\n",
    "    # Merge Consolidated FD Mean data for the current acq_type\n",
    "    fd_mean_filtered_for_acq = consolidated_fd_df[consolidated_fd_df['acq'] == acq_type][['subject', 'fd_mean']].copy()\n",
    "    df = pd.merge(df, fd_mean_filtered_for_acq, on='subject', how='left')\n",
    "\n",
    "    # Merge Headcoil data (constant for all acq types per subject)\n",
    "    df = pd.merge(df, headcoil_series, on='subject', how='left')\n",
    "\n",
    "    # Ensure raw columns are numeric, coercing errors to NaN\n",
    "    df['fd_mean'] = pd.to_numeric(df['fd_mean'], errors='coerce')\n",
    "    df['headcoil_encoded'] = pd.to_numeric(df['headcoil_encoded'], errors='coerce')\n",
    "\n",
    "    # --- Append Smoothness Data (after initial dataframe assembly) ---\n",
    "    if not smoothness_df.empty:\n",
    "        smoothness_filtered_for_acq = smoothness_df[smoothness_df['acq'] == acq_type][['subject', 'smoothness']].copy()\n",
    "        \n",
    "        # Merge this filtered smoothness data onto the main df\n",
    "        df = pd.merge(df, smoothness_filtered_for_acq, on='subject', how='left')\n",
    "        \n",
    "        # Ensure smoothness is numeric after merge\n",
    "        df['smoothness'] = pd.to_numeric(df['smoothness'], errors='coerce')\n",
    "\n",
    "    else:\n",
    "        print(f\"  Warning: Pre-processed smoothness data is empty. 'smoothness' column will be all NaNs for {acq_type}.\")\n",
    "        df['smoothness'] = np.nan # Ensure column exists as NaN\n",
    "\n",
    "\n",
    "    # --- First, drop rows with NaN in fd_mean_demeaned (using raw 'fd_mean' for dropping) ---\n",
    "    # We are dropping before de-meaning to ensure means are calculated on the final set of subjects\n",
    "    original_subject_count_before_drops = len(df)\n",
    "    \n",
    "    # Identify subjects with missing FD data for reporting\n",
    "    # Note: We are now dropping based on the raw `fd_mean` before de-meaning for a clean set\n",
    "    missing_fd_subjects_to_drop = df[df['fd_mean'].isna()]['subject'].tolist()\n",
    "    \n",
    "    if missing_fd_subjects_to_drop:\n",
    "        print(f\"  Subjects with missing FD data (will be dropped): {len(missing_fd_subjects_to_drop)}\")\n",
    "        print(f\"  Listing missing FD subjects: {missing_fd_subjects_to_drop}\")\n",
    "        df = df.dropna(subset=['fd_mean']).copy() # Drop NaNs in the raw fd_mean\n",
    "        subjects_dropped_fd = original_subject_count_before_drops - len(df)\n",
    "        print(f\"  Dropped {subjects_dropped_fd} subjects due to missing FD data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing FD data found in {acq_type} to drop.\")\n",
    "\n",
    "\n",
    "    # --- NEW: Second, drop rows with NaN in smoothness (using raw 'smoothness' for dropping) ---\n",
    "    original_subject_count_after_fd_drop = len(df)\n",
    "    missing_smoothness_subjects_to_drop = df[df['smoothness'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_smoothness_subjects_to_drop:\n",
    "        print(f\"  Subjects with missing Smoothness data (will be dropped): {len(missing_smoothness_subjects_to_drop)}\")\n",
    "        print(f\"  Listing missing Smoothness subjects: {missing_smoothness_subjects_to_drop}\")\n",
    "        df = df.dropna(subset=['smoothness']).copy() # Drop NaNs in the raw smoothness\n",
    "        subjects_dropped_smoothness = original_subject_count_after_fd_drop - len(df)\n",
    "        print(f\"  Dropped {subjects_dropped_smoothness} subjects due to missing Smoothness data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing Smoothness data found in {acq_type} to drop.\")\n",
    "\n",
    "    # Now that we have the final set of subjects without NaNs for main regressors, perform de-meaning.\n",
    "    # --- IMPORTANT: Re-calculate means on the *final* filtered DataFrame ---\n",
    "    mean_fd_mean = df['fd_mean'].mean()\n",
    "    mean_headcoil_encoded = df['headcoil_encoded'].mean()\n",
    "    mean_smoothness = df['smoothness'].mean()\n",
    "\n",
    "    # De-mean individual columns. Since NaNs were dropped, these should now be fully numerical.\n",
    "    df['fd_mean_demeaned'] = df['fd_mean'] - mean_fd_mean\n",
    "    df['headcoil_demeaned'] = df['headcoil_encoded'] - mean_headcoil_encoded\n",
    "    df['smoothness_demeaned'] = df['smoothness'] - mean_smoothness\n",
    "\n",
    "    # Calculate raw interaction (will propagate NaNs from its components)\n",
    "    df['interaction_raw'] = df['fd_mean_demeaned'] * df['headcoil_demeaned']\n",
    "    \n",
    "    # De-mean the interaction term itself\n",
    "    mean_interaction_raw = df['interaction_raw'].mean()\n",
    "    df['fd_mean_x_headcoil_demeaned'] = df['interaction_raw'] - mean_interaction_raw\n",
    "\n",
    "    # Select and reorder final columns for the design matrix\n",
    "    final_cols = [\n",
    "        'subject',\n",
    "        'ones',\n",
    "        'fd_mean_demeaned',\n",
    "        'headcoil_demeaned',\n",
    "        'fd_mean_x_headcoil_demeaned',\n",
    "        'smoothness_demeaned'\n",
    "    ]\n",
    "    design_matrix_df = df[final_cols].copy()\n",
    "\n",
    "    # Store the DataFrame\n",
    "    design_matrices[acq_type] = design_matrix_df\n",
    "    print(f\"Design Matrix for {acq_type} created. Final number of subjects: {len(design_matrix_df)}\")\n",
    "    # Re-check NaNs after final drops (should be 0 for both)\n",
    "    print(f\"Subjects with missing FD data (after final drop): {design_matrices[acq_type]['fd_mean_demeaned'].isna().sum()}\")\n",
    "    print(f\"Subjects with missing Smoothness data (after final drop): {design_matrices[acq_type]['smoothness_demeaned'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# --- Display Sample Heads ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE HEADS OF GENERATED DESIGN MATRICES (AFTER ALL NaNs DROPPED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    print(f\"\\n--- Design Matrix for {acq_type} (First 10 rows) ---\")\n",
    "    # Display more rows for better inspection, adjust as needed\n",
    "    pd.set_option('display.max_rows', 10) # Show more rows for inspection\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df.head(10)) # Print only first 10 rows\n",
    "    pd.reset_option('display.max_rows') # Reset after printing\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Final Check on NaNs after Dropping ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CHECK: SUBJECTS WITH MISSING FD_MEAN AND SMOOTHNESS DATA (SHOULD BE ZERO AFTER DROP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    missing_fd_subjects_final = df[df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    missing_smoothness_subjects_final = df[df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_fd_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_fd_subjects_final)} subjects still have missing FD data:\")\n",
    "        print(f\"  {missing_fd_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing FD data (as expected).\")\n",
    "\n",
    "    if missing_smoothness_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_smoothness_subjects_final)} subjects still have missing Smoothness data:\")\n",
    "        print(f\"  {missing_smoothness_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing Smoothness data (as expected).\")\n",
    "\n",
    "# --- Save Design Matrices to CSV files ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"SAVING DESIGN MATRICES TO CSV FILES IN: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    output_filename = os.path.join(DESIGN_MATRIX_OUTPUT_DIR, f'design_matrix_{acq_type}.csv')\n",
    "    try:\n",
    "        # Save without the 'subject' column as index, just as data\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved '{output_filename}' with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "print(\"\\nAll design matrices generated, adjusted, and saved.\")\n",
    "print(\"================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddc9b5-4454-4c52-ada4-e3cb7b20b504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
