{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306f7327-acc1-41a2-8758-a955930128c6",
   "metadata": {},
   "source": [
    "# Subject Demographics & Data Quality Analysis\n",
    "\n",
    "This kernel analyzes the final sample for the multiecho pilot study by:\n",
    "\n",
    "1. **Loading demographics data** and separating main study participants from special participants\n",
    "2. **Applying quality filters** to exclude subjects with incomplete data (SixRuns = 0)\n",
    "3. **Removing motion outliers** based on MRIQC analysis results (tsnr & fd_mean)\n",
    "4. **Reporting final sample characteristics** including:\n",
    "   - Total sample size (N)\n",
    "   - Age distribution (mean, SD, histogram)\n",
    "   - Breakdown by headcoil type (20 vs 64 channel)\n",
    "\n",
    "The analysis ensures we have a clean, well-characterized dataset for subsequent neuroimaging analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9faa7f82-2961-451d-a425-a55b0c290224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading demographics from: /Users/jameswyngaarden/Documents/GitHub/multiecho-pilot/code/multiecho-pilot_Demographics.xlsx\n",
      "\n",
      "Initial separation:\n",
      "- SP subjects (spSubs=1): 14 subjects\n",
      "- Main subjects (spSubs=0): 50 subjects\n",
      "\n",
      "=== MAIN POPULATION (spSubs=0) - ALL SUBJECTS ===\n",
      "Total N: 50\n",
      "Age: Mean = 28.8, SD = 13.4\n",
      "\n",
      "=== AFTER REMOVING SixRuns=0 ===\n",
      "Removed 9 subjects\n",
      "Total N: 41\n",
      "Age: Mean = 27.4, SD = 10.5\n",
      "\n",
      "=== MOTION OUTLIER ANALYSIS ===\n",
      "Loading outlier data from: /Users/jameswyngaarden/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\n",
      "Successfully loaded 718 rows\n",
      "\n",
      "Total outlier runs identified: 32\n",
      "Unique subjects with outlier runs: 7\n",
      "Outlier subjects (cleaned IDs): ['10085', '10094', '10438', '10659sp', '10716', '10738', '10741sp']\n",
      "Removed 3 subjects due to motion outliers\n",
      "\n",
      "=== FINAL POPULATION (After All Filters) ===\n",
      "Total N: 38\n",
      "Age: Mean = 25.4, SD = 7.8\n",
      "Gender breakdown:\n",
      "- Males: 18\n",
      "- Females: 20\n",
      "\n",
      "Breakdown by Headcoil:\n",
      "- Headcoil 20.0: N = 16\n",
      "- Headcoil 64.0: N = 22\n",
      "\n",
      "=== FILTERING SUMMARY ===\n",
      "Initial main subjects: 50\n",
      "Removed due to SixRuns=0: 9\n",
      "Removed due to motion outliers: 3\n",
      "Final sample size: 38\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATAFRAMES READY FOR NEXT STEPS ===\n",
      "main_df_final: 38 subjects (filtered main population)\n",
      "sp_df: 14 subjects (SP population for later use)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths relative to the multiecho-pilot directory\n",
    "# This will work across different operating systems and user setups\n",
    "project_root = Path.cwd()\n",
    "while project_root.name != 'multiecho-pilot' and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "    \n",
    "if project_root.name != 'multiecho-pilot':\n",
    "    raise ValueError(\"Could not find 'multiecho-pilot' directory. Please run this script from within the project directory.\")\n",
    "\n",
    "# Define file paths relative to project root\n",
    "demographics_path = project_root / 'code' / 'multiecho-pilot_Demographics.xlsx'\n",
    "outlier_path = project_root / 'derivatives' / 'Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv'\n",
    "\n",
    "# Read the demographics Excel file\n",
    "print(f\"Loading demographics from: {demographics_path}\")\n",
    "df = pd.read_excel(demographics_path)\n",
    "\n",
    "# Separate data based on spSubs column\n",
    "sp_df = df[df['spSubs'] == 1].copy()\n",
    "main_df = df[df['spSubs'] == 0].copy()\n",
    "\n",
    "print(f\"\\nInitial separation:\")\n",
    "print(f\"- SP subjects (spSubs=1): {len(sp_df)} subjects\")\n",
    "print(f\"- Main subjects (spSubs=0): {len(main_df)} subjects\")\n",
    "\n",
    "# Print initial statistics for main population\n",
    "print(\"\\n=== MAIN POPULATION (spSubs=0) - ALL SUBJECTS ===\")\n",
    "print(f\"Total N: {len(main_df)}\")  # Since OpenNeuro is binary and all are 1\n",
    "print(f\"Age: Mean = {main_df['Age'].mean():.1f}, SD = {main_df['Age'].std():.1f}\")\n",
    "\n",
    "# Filter 1: Remove subjects with SixRuns = 0\n",
    "main_df_filtered = main_df[main_df['SixRuns'] != 0].copy()\n",
    "subjects_removed_sixruns = len(main_df) - len(main_df_filtered)\n",
    "\n",
    "print(\"\\n=== AFTER REMOVING SixRuns=0 ===\")\n",
    "print(f\"Removed {subjects_removed_sixruns} subjects\")\n",
    "print(f\"Total N: {len(main_df_filtered)}\")\n",
    "print(f\"Age: Mean = {main_df_filtered['Age'].mean():.1f}, SD = {main_df_filtered['Age'].std():.1f}\")\n",
    "\n",
    "# Filter 2: Remove subjects with motion outliers\n",
    "print(\"\\n=== MOTION OUTLIER ANALYSIS ===\")\n",
    "print(f\"Loading outlier data from: {outlier_path}\")\n",
    "\n",
    "# Read the outlier TSV file\n",
    "outlier_df = pd.read_csv(outlier_path, sep='\\t')\n",
    "print(f\"Successfully loaded {len(outlier_df)} rows\")\n",
    "\n",
    "# Find outlier runs (where outlier_acq_Custom1 is True)\n",
    "outlier_runs = outlier_df[outlier_df['outlier_acq_Custom1'] == True]\n",
    "print(f\"\\nTotal outlier runs identified: {len(outlier_runs)}\")\n",
    "\n",
    "if len(outlier_runs) > 0:\n",
    "    # Extract unique subject IDs from outlier runs\n",
    "    outlier_subjects_raw = outlier_runs['Sub'].unique()\n",
    "    print(f\"Unique subjects with outlier runs: {len(outlier_subjects_raw)}\")\n",
    "    \n",
    "    # Convert outlier subject IDs to match demographics format\n",
    "    # Remove 'sub-' prefix but keep the full ID (including 'sp' suffix if present)\n",
    "    outlier_subjects_clean = []\n",
    "    for sub in outlier_subjects_raw:\n",
    "        if sub.startswith('sub-'):\n",
    "            clean_id = sub.replace('sub-', '')\n",
    "            outlier_subjects_clean.append(clean_id)\n",
    "    \n",
    "    print(f\"Outlier subjects (cleaned IDs): {sorted(outlier_subjects_clean)}\")\n",
    "    \n",
    "    # Since demographics Subject column can be either numeric or string with 'sp' suffix,\n",
    "    # we need to convert to string for comparison\n",
    "    main_df_filtered['Subject_str'] = main_df_filtered['Subject'].astype(str)\n",
    "    \n",
    "    # Remove subjects with outlier runs from main_df_filtered\n",
    "    main_df_final = main_df_filtered[~main_df_filtered['Subject_str'].isin(outlier_subjects_clean)].copy()\n",
    "    \n",
    "    # Drop the temporary string column\n",
    "    main_df_final = main_df_final.drop(columns=['Subject_str'])\n",
    "    subjects_removed_outliers = len(main_df_filtered) - len(main_df_final)\n",
    "    print(f\"Removed {subjects_removed_outliers} subjects due to motion outliers\")\n",
    "else:\n",
    "    print(\"No outlier runs found.\")\n",
    "    main_df_final = main_df_filtered.copy()\n",
    "    subjects_removed_outliers = 0\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\n=== FINAL POPULATION (After All Filters) ===\")\n",
    "print(f\"Total N: {len(main_df_final)}\")\n",
    "print(f\"Age: Mean = {main_df_final['Age'].mean():.1f}, SD = {main_df_final['Age'].std():.1f}\")\n",
    "if 'Gender' in main_df_final.columns:\n",
    "    gender_counts = main_df_final['Gender'].value_counts()\n",
    "    print(f\"Gender breakdown:\")\n",
    "    if 'Male' in gender_counts:\n",
    "        print(f\"- Males: {gender_counts['Male']}\")\n",
    "    if 'Female' in gender_counts:\n",
    "        print(f\"- Females: {gender_counts['Female']}\")\n",
    "    if 'Non-binary' in gender_counts:\n",
    "        print(f\"- Non-binary: {gender_counts['Non-binary']}\")\n",
    "    \n",
    "    # Handle any other unexpected gender categories\n",
    "    for gender_type, count in gender_counts.items():\n",
    "        if gender_type not in ['Male', 'Female', 'Non-binary']:\n",
    "            print(f\"- {gender_type}: {count} (Other/Unspecified)\")\n",
    "else:\n",
    "    print(\"Warning: 'Gender' column not found in the final dataset. Cannot provide gender breakdown.\")\n",
    "\n",
    "# Print breakdown by Headcoil\n",
    "print(f\"\\nBreakdown by Headcoil:\")\n",
    "headcoil_counts = main_df_final['Headcoil'].value_counts().sort_index()\n",
    "for headcoil, count in headcoil_counts.items():\n",
    "    print(f\"- Headcoil {headcoil}: N = {count}\")\n",
    "\n",
    "# Summary of filtering\n",
    "print(\"\\n=== FILTERING SUMMARY ===\")\n",
    "print(f\"Initial main subjects: {len(main_df)}\")\n",
    "print(f\"Removed due to SixRuns=0: {subjects_removed_sixruns}\")\n",
    "print(f\"Removed due to motion outliers: {subjects_removed_outliers}\")\n",
    "print(f\"Final sample size: {len(main_df_final)}\")\n",
    "\n",
    "# Create histogram of age for final filtered population\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create histogram\n",
    "n, bins, patches = plt.hist(main_df_final['Age'], bins=10, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "\n",
    "# Fit and plot a normal distribution trendline\n",
    "mu, sigma = stats.norm.fit(main_df_final['Age'])\n",
    "x = np.linspace(main_df_final['Age'].min(), main_df_final['Age'].max(), 100)\n",
    "y = stats.norm.pdf(x, mu, sigma) * len(main_df_final['Age']) * (bins[1] - bins[0])\n",
    "plt.plot(x, y, 'r-', linewidth=3, label=f'Normal fit (={mu:.1f}, ={sigma:.1f})')\n",
    "\n",
    "plt.xlabel('Age', fontsize=38)\n",
    "plt.ylabel('Frequency', fontsize=38)\n",
    "plt.title('Age Distribution\\nFinal Population (After All Filters)', fontsize=38)\n",
    "plt.tick_params(axis='both', which='major', labelsize=29)\n",
    "#plt.legend(fontsize=24, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store the key dataframes for use in subsequent kernels\n",
    "print(\"\\n=== DATAFRAMES READY FOR NEXT STEPS ===\")\n",
    "print(f\"main_df_final: {len(main_df_final)} subjects (filtered main population)\")\n",
    "print(f\"sp_df: {len(sp_df)} subjects (SP population for later use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed57ac-2d11-495b-8359-0cc9292dda02",
   "metadata": {},
   "source": [
    "# Spatial Smoothness Analysis (Fig. 3)\n",
    "This kernel analyzes spatial smoothness in fMRI data before and after preprocessing by:\n",
    "1. **Loading smoothness measurements** from two CSV files (pre- and post-smoothing)\n",
    "2. **Filtering subjects** to include only those from the main study population with complete data\n",
    "3. **Running Linear Mixed Effects models** to test effects of:\n",
    "   - Headcoil type (20 vs 64 channel)\n",
    "   - Multiband acceleration factor (mb1, mb3, mb6)\n",
    "   - Multi-echo acquisition (me1, me4)\n",
    "   - All interactions between these factors\n",
    "4. **Generating visualizations** in a 22 subplot layout showing:\n",
    "   - Top row: Pre-smoothing spatial smoothness\n",
    "   - Bottom row: Post-smoothing spatial smoothness\n",
    "   - Columns separated by headcoil type\n",
    "5. **Saving outputs** to `derivatives/plots/` including:\n",
    "   - APA-formatted ANOVA tables\n",
    "   - Complete subject lists\n",
    "   - Combined bar plot figure\n",
    "\n",
    "The analysis quantifies how acquisition parameters affect spatial smoothness and the impact of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d86f18-b342-485c-a207-772ce8b9752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SMOOTHNESS ANALYSIS PIPELINE - PRE AND POST SMOOTHING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths to both CSV files\n",
    "demographics_path = project_root / 'code' / 'multiecho-pilot_Demographics.xlsx'\n",
    "outlier_path = project_root / 'derivatives' / 'Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv'\n",
    "\n",
    "csv_path_pre = project_root / 'code' / 'smoothness-all-zero.csv'\n",
    "csv_path_post = project_root / 'code' / 'smoothness-all.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get valid subjects and headcoil information from main_df_final\n",
    "valid_subjects = main_df_final['Subject'].astype(str).tolist()\n",
    "headcoil_mapping = dict(zip(main_df_final['Subject'].astype(str), \n",
    "                           main_df_final['Headcoil'].astype(str)))\n",
    "\n",
    "print(f\"Filtering to {len(valid_subjects)} subjects from main population\")\n",
    "\n",
    "# Output file names\n",
    "complete_subjects_file_pre = output_dir / 'complete_subjects_smoothness_pre_with_headcoil.csv'\n",
    "complete_subjects_file_post = output_dir / 'complete_subjects_smoothness_post_with_headcoil.csv'\n",
    "anova_table_file_pre = output_dir / 'smoothness_pre_lme_anova_complete_subjects.csv'\n",
    "anova_table_file_post = output_dir / 'smoothness_post_lme_anova_complete_subjects.csv'\n",
    "combined_plot_file = output_dir / 'smoothness_combined_bar_plot.png'\n",
    "\n",
    "print(f\"Pre-smoothing file: {csv_path_pre}\")\n",
    "print(f\"Post-smoothing file: {csv_path_post}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SHARED FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_process_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process the CSV file with shift-up correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift up procedure to align smoothness values with correct acquisition\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        \n",
    "        # Filter rows with non-null smoothness and file_path\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, and me from file_path\n",
    "        def parse_path(path):\n",
    "            try:\n",
    "                if not isinstance(path, str):\n",
    "                    return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "                sub_match = re.search(r'sub-(\\d+)', path)\n",
    "                acq_match = re.search(r'acq-(mb\\dme\\d)', path)\n",
    "                subject = sub_match.group(1) if sub_match else None\n",
    "                acq = acq_match.group(1) if acq_match else None\n",
    "                if acq:\n",
    "                    mb = acq[:3]  # e.g., mb1\n",
    "                    me = acq[3:]  # e.g., me1\n",
    "                else:\n",
    "                    mb = None\n",
    "                    me = None\n",
    "                return pd.Series({'subject': subject, 'mb': mb, 'me': me})\n",
    "            except Exception as e:\n",
    "                return pd.Series({'subject': None, 'mb': None, 'me': None})\n",
    "        \n",
    "        parsed_data = data['file_path'].apply(parse_path)\n",
    "        data = pd.concat([data, parsed_data], axis=1)\n",
    "        \n",
    "        # Filter to only include valid subjects from main_df_final\n",
    "        data = data[data['subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Assign headcoil based on mapping from main_df_final\n",
    "        data['headcoil'] = data['subject'].map(headcoil_mapping).str.replace('.0', '', regex=False)\n",
    "        \n",
    "        # Select relevant columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        \n",
    "        # Filter out invalid rows\n",
    "        data = data.dropna(subset=['subject', 'mb', 'me', 'smoothness'])\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    # Create a combined mb_me column for pivoting\n",
    "    data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "    \n",
    "    # Pivot the data to create a table with subjects as rows and mb_me combinations as columns\n",
    "    pivot_table = data.pivot_table(\n",
    "        values='smoothness',\n",
    "        index='subject',\n",
    "        columns='mb_me',\n",
    "        aggfunc='mean'  # In case of duplicates, take the mean\n",
    "    )\n",
    "    \n",
    "    # Ensure all expected mb_me combinations are present as columns\n",
    "    expected_columns = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    pivot_table = pivot_table.reindex(columns=expected_columns)\n",
    "    \n",
    "    # Identify subjects with no NaN values across all mb_me columns\n",
    "    complete_subjects = pivot_table.dropna()\n",
    "    \n",
    "    # Sort the index (subjects) for consistency\n",
    "    complete_subjects = complete_subjects.sort_index()\n",
    "    \n",
    "    # Round smoothness values to 3 decimal places for readability\n",
    "    complete_subjects = complete_subjects.round(3)\n",
    "    \n",
    "    # Get headcoil information for complete subjects\n",
    "    headcoil_data = data[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "    complete_headcoil = headcoil_data.loc[complete_subjects.index].astype(str)\n",
    "    \n",
    "    # Add headcoil as a column to the complete subjects table\n",
    "    complete_subjects = complete_subjects.reset_index().merge(\n",
    "        complete_headcoil[['headcoil']].reset_index(),\n",
    "        on='subject',\n",
    "        how='left'\n",
    "    ).set_index('subject')\n",
    "    \n",
    "    # Reorder columns to have headcoil first\n",
    "    cols = ['headcoil'] + expected_columns\n",
    "    complete_subjects = complete_subjects[cols]\n",
    "    \n",
    "    return complete_subjects\n",
    "\n",
    "def run_lme_analysis(data_complete, label):\n",
    "    \"\"\"Run Linear Mixed Effects analysis and return results\"\"\"\n",
    "    print(f\"\\nRunning LME analysis for {label} data\")\n",
    "    print(f\"Observations: {len(data_complete)}, Subjects: {data_complete['subject'].nunique()}\")\n",
    "    \n",
    "    # Prepare data for LME model with sum-to-zero contrasts\n",
    "    data_model = data_complete.copy()\n",
    "    data_model['headcoil'] = data_model['headcoil'].cat.codes - 0.5  # 20=-0.5, 64=0.5\n",
    "    \n",
    "    # Fit the LME model\n",
    "    model = Lmer('smoothness ~ headcoil * mb * me + (1 | subject)', data=data_model)\n",
    "    model.fit()\n",
    "    \n",
    "    # Get ANOVA table\n",
    "    anova_table = model.anova()\n",
    "    \n",
    "    # Define effect names and numerator df for APA table\n",
    "    effect_map = {\n",
    "        'headcoil': 'Head Coil',\n",
    "        'mb': 'Multiband',\n",
    "        'me': 'Multi-echo',\n",
    "        'headcoil:mb': 'Head Coil  Multiband',\n",
    "        'headcoil:me': 'Head Coil  Multi-echo',\n",
    "        'mb:me': 'Multiband  Multi-echo',\n",
    "        'headcoil:mb:me': 'Head Coil  Multiband  Multi-echo'\n",
    "    }\n",
    "    \n",
    "    df_dict = {\n",
    "        'Head Coil': 1,\n",
    "        'Multiband': 2,\n",
    "        'Multi-echo': 1,\n",
    "        'Head Coil  Multiband': 2,\n",
    "        'Head Coil  Multi-echo': 1,\n",
    "        'Multiband  Multi-echo': 2,\n",
    "        'Head Coil  Multiband  Multi-echo': 2\n",
    "    }\n",
    "    \n",
    "    # Build APA table\n",
    "    apa_data = []\n",
    "    for effect in anova_table.index:\n",
    "        if effect in ['(Intercept)', 'Residuals']:\n",
    "            continue\n",
    "        effect_name = effect_map.get(effect, effect)\n",
    "        apa_data.append({\n",
    "            'Effect': effect_name,\n",
    "            'Sum Sq': anova_table.loc[effect, 'SS'] if 'SS' in anova_table.columns else np.nan,\n",
    "            'Mean Sq': anova_table.loc[effect, 'MS'] if 'MS' in anova_table.columns else np.nan,\n",
    "            'Num df': df_dict.get(effect_name, np.nan),\n",
    "            'Den df': anova_table.loc[effect, 'DenomDF'] if 'DenomDF' in anova_table.columns else np.nan,\n",
    "            'F': anova_table.loc[effect, 'F-stat'] if 'F-stat' in anova_table.columns else np.nan,\n",
    "            'p': anova_table.loc[effect, 'P-val'] if 'P-val' in anova_table.columns else np.nan,\n",
    "            'Partial ': np.nan  # Computed below\n",
    "        })\n",
    "    \n",
    "    # Compute partial eta-squared\n",
    "    try:\n",
    "        residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "    except (KeyError, IndexError):\n",
    "        residual_var = model.ranef_var.iloc[1]['Var']\n",
    "    \n",
    "    n_obs = len(data_model)\n",
    "    n_fixed = sum(df_dict.values())\n",
    "    n_subj = data_model['subject'].nunique()\n",
    "    ss_residual = residual_var * (n_obs - n_fixed - n_subj)\n",
    "    \n",
    "    for i, row in enumerate(apa_data):\n",
    "        ss_effect = row['Sum Sq']\n",
    "        if pd.notna(ss_effect):\n",
    "            apa_data[i]['Partial '] = ss_effect / (ss_effect + ss_residual)\n",
    "    \n",
    "    # Create APA table\n",
    "    apa_table = pd.DataFrame(apa_data)\n",
    "    apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "    apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "    apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "    apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "    apa_table['F'] = apa_table['F'].round(2)\n",
    "    apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "    apa_table['Partial '] = apa_table['Partial '].round(3)\n",
    "    \n",
    "    return apa_table\n",
    "\n",
    "def process_data_for_plotting(data, value_column='smoothness'):\n",
    "    \"\"\"Calculate mean and standard error by multiband, multi-echo, and headcoil\"\"\"\n",
    "    if data is None or data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = data.dropna(subset=['mb', 'me', 'headcoil', value_column])\n",
    "    \n",
    "    if data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    agg_data = data.groupby(['mb', 'me', 'headcoil'], observed=True).agg({\n",
    "        value_column: 'mean',\n",
    "        'subject': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    std_error = data.groupby(['mb', 'me', 'headcoil'], observed=True)[value_column].apply(\n",
    "        lambda x: x.std() / np.sqrt(len(x))\n",
    "    ).reset_index()\n",
    "    std_error.columns = ['mb', 'me', 'headcoil', 'se']\n",
    "    \n",
    "    result = pd.merge(agg_data, std_error, on=['mb', 'me', 'headcoil'])\n",
    "    result.columns = ['mb', 'me', 'headcoil', value_column, 'n_subjects', 'se']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: PROCESS PRE-SMOOTHING DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: PRE-SMOOTHING DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load and process pre-smoothing data\n",
    "data_pre = load_and_process_data(csv_path_pre, valid_subjects, headcoil_mapping)\n",
    "\n",
    "# debugging\n",
    "print(\"\\nDEBUGGING INFO:\")\n",
    "print(f\"Sample subject IDs from CSV: {data_pre['subject'].unique()[:5]}\")\n",
    "print(f\"Sample subject IDs from main_df_final: {main_df_final['Subject'].astype(str).unique()[:5]}\")\n",
    "print(f\"Headcoil mapping preview: {list(headcoil_mapping.items())[:5]}\")\n",
    "print(f\"Headcoil assignment results: {data_pre['headcoil'].value_counts(dropna=False)}\")\n",
    "\n",
    "if data_pre is None:\n",
    "    raise Exception(\"Failed to load pre-smoothing data. Stopping execution.\")\n",
    "\n",
    "# Identify complete subjects\n",
    "complete_subjects_table_pre = identify_complete_subjects(data_pre)\n",
    "total_n_pre = len(complete_subjects_table_pre)\n",
    "headcoil_counts_pre = complete_subjects_table_pre['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nPre-smoothing complete subjects: {total_n_pre}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_pre['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_pre['64']}\")\n",
    "\n",
    "# Save complete subjects table\n",
    "complete_subjects_table_pre.to_csv(complete_subjects_file_pre)\n",
    "print(f\"Complete subjects table saved to '{complete_subjects_file_pre}'\")\n",
    "\n",
    "# Filter data to complete subjects only\n",
    "complete_subject_list_pre = [str(subj) for subj in complete_subjects_table_pre.index.tolist()]\n",
    "data_complete_pre = data_pre[data_pre['subject'].isin(complete_subject_list_pre)]\n",
    "\n",
    "# Run LME analysis\n",
    "apa_table_pre = run_lme_analysis(data_complete_pre, \"pre-smoothing\")\n",
    "print(\"\\nAPA-Style ANOVA Table for Pre-smoothing Data:\")\n",
    "print(apa_table_pre.to_string(index=False))\n",
    "apa_table_pre.to_csv(anova_table_file_pre, index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: PROCESS POST-SMOOTHING DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: POST-SMOOTHING DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load and process post-smoothing data\n",
    "data_post = load_and_process_data(csv_path_post, valid_subjects, headcoil_mapping)\n",
    "\n",
    "if data_post is None:\n",
    "    raise Exception(\"Failed to load post-smoothing data. Stopping execution.\")\n",
    "\n",
    "# Identify complete subjects\n",
    "complete_subjects_table_post = identify_complete_subjects(data_post)\n",
    "total_n_post = len(complete_subjects_table_post)\n",
    "headcoil_counts_post = complete_subjects_table_post['headcoil'].value_counts().reindex(['20', '64'], fill_value=0)\n",
    "\n",
    "print(f\"\\nPost-smoothing complete subjects: {total_n_post}\")\n",
    "print(f\"  - 20-channel headcoil: {headcoil_counts_post['20']}\")\n",
    "print(f\"  - 64-channel headcoil: {headcoil_counts_post['64']}\")\n",
    "\n",
    "# Save complete subjects table\n",
    "complete_subjects_table_post.to_csv(complete_subjects_file_post)\n",
    "print(f\"Complete subjects table saved to '{complete_subjects_file_post}'\")\n",
    "\n",
    "# Filter data to complete subjects only\n",
    "complete_subject_list_post = [str(subj) for subj in complete_subjects_table_post.index.tolist()]\n",
    "data_complete_post = data_post[data_post['subject'].isin(complete_subject_list_post)]\n",
    "\n",
    "# Run LME analysis\n",
    "apa_table_post = run_lme_analysis(data_complete_post, \"post-smoothing\")\n",
    "print(\"\\nAPA-Style ANOVA Table for Post-smoothing Data:\")\n",
    "print(apa_table_post.to_string(index=False))\n",
    "apa_table_post.to_csv(anova_table_file_post, index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: COMBINED VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING COMBINED BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process data for plotting\n",
    "smoothness_processed_pre = process_data_for_plotting(data_complete_pre, 'smoothness')\n",
    "smoothness_processed_post = process_data_for_plotting(data_complete_post, 'smoothness')\n",
    "\n",
    "# Get n_subjects per coil for both datasets\n",
    "n_subjects_per_coil_pre = data_complete_pre.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "n_subjects_per_coil_post = data_complete_post.groupby('headcoil', observed=True)['subject'].nunique().to_dict()\n",
    "\n",
    "# Create combined 2x2 plot\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "width = 0.4\n",
    "x1 = [0, 1.2, 2.4]\n",
    "x2 = [x + width for x in x1]\n",
    "me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "mb_levels = ['mb1', 'mb3', 'mb6']\n",
    "\n",
    "# Set consistent y-axis limits for all subplots\n",
    "y_limits = (3.5, 5.5)\n",
    "\n",
    "# Function to plot data in a subplot\n",
    "def plot_smoothness_data(ax, data_processed, n_subjects_per_coil, headcoil):\n",
    "    \"\"\"Plot smoothness data for a specific headcoil\"\"\"\n",
    "    \n",
    "    coil_data = data_processed[data_processed['headcoil'] == headcoil]\n",
    "    \n",
    "    if coil_data.empty:\n",
    "        ax.set_title(f\"{headcoil}-Channel (n=0)\", fontsize=38)\n",
    "        ax.set_ylim(y_limits)\n",
    "        return\n",
    "    \n",
    "    me1_data = coil_data[coil_data['me'] == 'me1']\n",
    "    me4_data = coil_data[coil_data['me'] == 'me4']\n",
    "    \n",
    "    me1_means = me1_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "    me1_errors = me1_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "    me4_means = me4_data.set_index('mb')['smoothness'].reindex(mb_levels).fillna(0)\n",
    "    me4_errors = me4_data.set_index('mb')['se'].reindex(mb_levels).fillna(0)\n",
    "    \n",
    "    ax.bar(x1, me1_means, width, color=me_colors['me1'], label='me1', \n",
    "           yerr=me1_errors, capsize=5)\n",
    "    ax.bar(x2, me4_means, width, color=me_colors['me4'], label='me4', \n",
    "           yerr=me4_errors, capsize=5)\n",
    "    \n",
    "    ax.set_xticks([width/2, 1.2+width/2, 2.4+width/2])\n",
    "    ax.set_xticklabels(mb_levels)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "    \n",
    "    n_subjects = n_subjects_per_coil.get(headcoil, 0)\n",
    "    ax.set_title(f\"{headcoil}-Channel (n={n_subjects})\", fontsize=38)\n",
    "    ax.set_ylim(y_limits)\n",
    "\n",
    "# Plot data in 2x2 layout\n",
    "# Top row: Pre-smoothing\n",
    "plot_smoothness_data(axes[0, 0], smoothness_processed_pre, n_subjects_per_coil_pre, '20')\n",
    "plot_smoothness_data(axes[0, 1], smoothness_processed_pre, n_subjects_per_coil_pre, '64')\n",
    "\n",
    "# Bottom row: Post-smoothing\n",
    "plot_smoothness_data(axes[1, 0], smoothness_processed_post, n_subjects_per_coil_post, '20')\n",
    "plot_smoothness_data(axes[1, 1], smoothness_processed_post, n_subjects_per_coil_post, '64')\n",
    "\n",
    "# Add labels\n",
    "axes[0, 0].set_ylabel('Smoothness', fontsize=38)\n",
    "axes[1, 0].set_ylabel('Smoothness', fontsize=38)\n",
    "\n",
    "# Add legend to the upper right subplot\n",
    "axes[0, 1].legend(title='Multi-echo', fontsize=29, title_fontsize=29, \n",
    "                  loc='upper right')\n",
    "\n",
    "# Add shared x-label for entire figure\n",
    "fig.supxlabel('Multiband Factor', fontsize=38, y=0.06)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.88, hspace=0.35)\n",
    "\n",
    "# Add centered labels above each row (after subplots_adjust)\n",
    "row_labels = ['Pre-smoothing', 'Post-smoothing']\n",
    "for row_idx in range(2):\n",
    "    # Get the position of the top of the current row's subplots\n",
    "    bbox = axes[row_idx, 0].get_position()\n",
    "    # Place label at a fixed offset above the top of the subplot\n",
    "    y_pos = bbox.y1 + 0.045\n",
    "    fig.text(0.5, y_pos, row_labels[row_idx], \n",
    "            ha='center', fontsize=42, fontweight='bold',\n",
    "            transform=fig.transFigure)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(combined_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Combined plot saved to '{combined_plot_file}'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPre-smoothing data:\")\n",
    "print(f\"  Complete subjects: {total_n_pre}\")\n",
    "print(f\"  - 20-channel: {headcoil_counts_pre['20']}\")\n",
    "print(f\"  - 64-channel: {headcoil_counts_pre['64']}\")\n",
    "print(f\"\\nPost-smoothing data:\")\n",
    "print(f\"  Complete subjects: {total_n_post}\")\n",
    "print(f\"  - 20-channel: {headcoil_counts_post['20']}\")\n",
    "print(f\"  - 64-channel: {headcoil_counts_post['64']}\")\n",
    "print(f\"\\nFiles generated in {output_dir}:\")\n",
    "print(f\"  - {complete_subjects_file_pre.name}\")\n",
    "print(f\"  - {complete_subjects_file_post.name}\")\n",
    "print(f\"  - {anova_table_file_pre.name}\")\n",
    "print(f\"  - {anova_table_file_post.name}\")\n",
    "print(f\"  - {combined_plot_file.name}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b04d7-2082-4588-8086-516611e36064",
   "metadata": {},
   "source": [
    "# Temporal Signal-to-Noise Ratio (TSNR) Analysis (Fig. 4)\n",
    "This kernel analyzes TSNR in fMRI data with spatial smoothness as a covariate by:\n",
    "1. **Loading and merging datasets** from TSNR measurements and smoothness data\n",
    "2. **Filtering subjects** to include only those from the main study population with complete data in both metrics\n",
    "3. **Running Linear Mixed Effects models** testing the effects of:\n",
    "   - Headcoil type (20 vs 64 channel)\n",
    "   - Multiband acceleration factor (mb1, mb3, mb6)\n",
    "   - Multi-echo acquisition (me1, me4)\n",
    "   - Spatial smoothness as a continuous covariate\n",
    "   - All interactions between acquisition parameters\n",
    "4. **Generating visualizations** showing TSNR median values:\n",
    "   - Bar plots separated by headcoil type\n",
    "   - Error bars representing standard error\n",
    "   - Legend indicating multi-echo conditions\n",
    "5. **Saving outputs** to `derivatives/plots/` including:\n",
    "   - Complete APA-formatted ANOVA table with effect sizes\n",
    "   - List of subjects with complete data\n",
    "   - Bar plot figure comparing acquisition parameters\n",
    "\n",
    "The analysis quantifies how acquisition parameters and spatial smoothness jointly influence temporal signal quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc22677-996b-4576-998b-f8251bc81924",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TSNR ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths relative to project root\n",
    "tsnr_path = project_root / 'code' / 'outputs' / 'tables' / 'combined_tsnr_coil_output.csv'\n",
    "smoothness_path = project_root / 'code' / 'smoothness-all.csv'\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get valid subjects and headcoil information from main_df_final\n",
    "valid_subjects = main_df_final['Subject'].astype(str).tolist()\n",
    "headcoil_mapping = dict(zip(main_df_final['Subject'].astype(str), \n",
    "                           main_df_final['Headcoil'].astype(int).astype(str)))\n",
    "\n",
    "print(f\"Filtering to {len(valid_subjects)} subjects from main population\")\n",
    "\n",
    "# Output file names\n",
    "complete_subjects_file = output_dir / 'complete_subjects_tsnr_common_with_headcoil.csv'\n",
    "anova_table_file = output_dir / 'tsnr_lme_anova_with_smoothness.csv'\n",
    "tsnr_plot_file = output_dir / 'tsnr_bar_plot_median.png'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_process_tsnr_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process TSNR data\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure string types\n",
    "        data['Subject'] = data['Subject'].astype(str)\n",
    "        data['AcquisitionType'] = data['AcquisitionType'].astype(str)\n",
    "        \n",
    "        # Filter to valid subjects only\n",
    "        data = data[data['Subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Extract mb and me from AcquisitionType\n",
    "        data['mb'] = data['AcquisitionType'].str.extract(r'(mb\\d)')\n",
    "        data['me'] = data['AcquisitionType'].str.extract(r'(me\\d)')\n",
    "        \n",
    "        # Add headcoil from mapping\n",
    "        data['headcoil'] = data['Subject'].map(headcoil_mapping).str.replace('.0', '', regex=False)\n",
    "        \n",
    "        # Rename for consistency\n",
    "        data = data.rename(columns={'Subject': 'subject'})\n",
    "        \n",
    "        # Create combined mb_me label\n",
    "        data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Select relevant columns\n",
    "        required_cols = ['subject', 'headcoil', 'mb', 'me', 'mb_me', 'tsnrMean', 'tsnrMedian']\n",
    "        if not all(col in data.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in data.columns]\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "        data = data[required_cols]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TSNR data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process smoothness data with shift correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift correction\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        \n",
    "        # Filter valid rows\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, me\n",
    "        data['subject'] = data['file_path'].str.extract(r'sub-(\\d+)')\n",
    "        data['acq'] = data['file_path'].str.extract(r'acq-(mb\\dme\\d)')\n",
    "        data['mb'] = data['acq'].str[:3]\n",
    "        data['me'] = data['acq'].str[3:]\n",
    "        \n",
    "        # Filter to valid subjects\n",
    "        data = data[data['subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Add headcoil from mapping\n",
    "        data['headcoil'] = data['subject'].map(headcoil_mapping)\n",
    "        \n",
    "        # Create combined mb_me label\n",
    "        data['mb_me'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Select relevant columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'mb_me', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data, value_col):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    pivot = data.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='mb_me',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    expected_cols = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    pivot = pivot.reindex(columns=expected_cols)\n",
    "    \n",
    "    complete_subjects = pivot.dropna().index.tolist()\n",
    "    return complete_subjects\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD DATA AND IDENTIFY COMMON COMPLETE SUBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: IDENTIFYING COMMON COMPLETE SUBJECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load TSNR data\n",
    "tsnr_data = load_and_process_tsnr_data(tsnr_path, valid_subjects, headcoil_mapping)\n",
    "if tsnr_data is None:\n",
    "    raise Exception(\"Failed to load TSNR data\")\n",
    "\n",
    "# Load smoothness data\n",
    "smoothness_data = load_and_process_smoothness_data(smoothness_path, valid_subjects, headcoil_mapping)\n",
    "if smoothness_data is None:\n",
    "    raise Exception(\"Failed to load smoothness data\")\n",
    "\n",
    "# Identify complete subjects in each dataset\n",
    "complete_tsnr = identify_complete_subjects(tsnr_data, 'tsnrMedian')\n",
    "complete_smoothness = identify_complete_subjects(smoothness_data, 'smoothness')\n",
    "\n",
    "print(f\"Complete TSNR subjects: {len(complete_tsnr)}\")\n",
    "print(f\"Complete smoothness subjects: {len(complete_smoothness)}\")\n",
    "\n",
    "# Find common complete subjects\n",
    "common_complete = sorted(list(set(complete_tsnr) & set(complete_smoothness)))\n",
    "print(f\"\\nCommon complete subjects: {len(common_complete)}\")\n",
    "\n",
    "if not common_complete:\n",
    "    raise Exception(\"No common complete subjects found\")\n",
    "\n",
    "# Filter data to common complete subjects\n",
    "tsnr_filtered = tsnr_data[tsnr_data['subject'].isin(common_complete)].copy()\n",
    "smoothness_filtered = smoothness_data[smoothness_data['subject'].isin(common_complete)].copy()\n",
    "\n",
    "# Merge datasets for analysis\n",
    "merged_data = pd.merge(\n",
    "    tsnr_filtered[['subject', 'headcoil', 'mb', 'me', 'tsnrMedian']],\n",
    "    smoothness_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "    on=['subject', 'mb', 'me'],\n",
    "    how='inner'\n",
    ").dropna()\n",
    "\n",
    "# Summary statistics\n",
    "total_n = merged_data['subject'].nunique()\n",
    "headcoil_counts = merged_data[['subject', 'headcoil']].drop_duplicates()['headcoil'].value_counts()\n",
    "\n",
    "print(f\"\\nFinal dataset: {total_n} subjects, {len(merged_data)} observations\")\n",
    "print(f\"  20-channel: {headcoil_counts.get('20', 0)}\")\n",
    "print(f\"  64-channel: {headcoil_counts.get('64', 0)}\")\n",
    "\n",
    "# Save complete subjects table\n",
    "tsnr_complete_table = tsnr_filtered.pivot_table(\n",
    "    values='tsnrMean',\n",
    "    index='subject',\n",
    "    columns='mb_me',\n",
    "    aggfunc='mean'\n",
    ").round(3)\n",
    "\n",
    "# Add headcoil info\n",
    "headcoil_info = tsnr_filtered[['subject', 'headcoil']].drop_duplicates().set_index('subject')\n",
    "tsnr_complete_table = pd.concat([headcoil_info, tsnr_complete_table], axis=1)\n",
    "tsnr_complete_table.to_csv(complete_subjects_file)\n",
    "print(f\"\\nComplete subjects table saved to '{complete_subjects_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LINEAR MIXED EFFECTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data for LME\n",
    "model_data = merged_data.copy()\n",
    "model_data['headcoil_encoded'] = model_data['headcoil'].cat.codes - 0.5\n",
    "\n",
    "# Fit model with smoothness as covariate\n",
    "model = Lmer('tsnrMedian ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', \n",
    "             data=model_data)\n",
    "model.fit()\n",
    "\n",
    "# Get ANOVA results\n",
    "anova_results = model.anova()\n",
    "\n",
    "# Create APA table\n",
    "effect_names = {\n",
    "    'headcoil_encoded': 'Head Coil',\n",
    "    'mb': 'Multiband',\n",
    "    'me': 'Multi-echo',\n",
    "    'smoothness': 'Smoothness',\n",
    "    'headcoil_encoded:mb': 'Head Coil  Multiband',\n",
    "    'headcoil_encoded:me': 'Head Coil  Multi-echo',\n",
    "    'mb:me': 'Multiband  Multi-echo',\n",
    "    'headcoil_encoded:mb:me': 'Head Coil  Multiband  Multi-echo'\n",
    "}\n",
    "\n",
    "# Define degrees of freedom for each effect\n",
    "df_dict = {\n",
    "    'Head Coil': 1,\n",
    "    'Multiband': 2,\n",
    "    'Multi-echo': 1,\n",
    "    'Smoothness': 1,\n",
    "    'Head Coil  Multiband': 2,\n",
    "    'Head Coil  Multi-echo': 1,\n",
    "    'Multiband  Multi-echo': 2,\n",
    "    'Head Coil  Multiband  Multi-echo': 2\n",
    "}\n",
    "\n",
    "# Build APA data\n",
    "apa_data = []\n",
    "for effect in anova_results.index:\n",
    "    if effect not in ['(Intercept)', 'Residuals']:\n",
    "        effect_name = effect_names.get(effect, effect)\n",
    "        apa_data.append({\n",
    "            'Effect': effect_name,\n",
    "            'Sum Sq': anova_results.loc[effect, 'SS'] if 'SS' in anova_results.columns else np.nan,\n",
    "            'Mean Sq': anova_results.loc[effect, 'MS'] if 'MS' in anova_results.columns else np.nan,\n",
    "            'Num df': df_dict.get(effect_name, np.nan),\n",
    "            'Den df': anova_results.loc[effect, 'DenomDF'] if 'DenomDF' in anova_results.columns else np.nan,\n",
    "            'F': anova_results.loc[effect, 'F-stat'] if 'F-stat' in anova_results.columns else np.nan,\n",
    "            'p': anova_results.loc[effect, 'P-val'] if 'P-val' in anova_results.columns else np.nan,\n",
    "            'Partial ': np.nan  # Computed below\n",
    "        })\n",
    "\n",
    "# Compute partial eta-squared\n",
    "try:\n",
    "    residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "except (KeyError, IndexError):\n",
    "    residual_var = model.ranef_var.iloc[1]['Var']\n",
    "\n",
    "n_obs = len(model_data)\n",
    "n_fixed = sum(df_dict.values())\n",
    "n_subj = model_data['subject'].nunique()\n",
    "ss_residual = residual_var * (n_obs - n_fixed - n_subj)\n",
    "\n",
    "for i, row in enumerate(apa_data):\n",
    "    ss_effect = row['Sum Sq']\n",
    "    if pd.notna(ss_effect):\n",
    "        apa_data[i]['Partial '] = ss_effect / (ss_effect + ss_residual)\n",
    "\n",
    "# Create APA table\n",
    "apa_table = pd.DataFrame(apa_data)\n",
    "apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "apa_table['F'] = apa_table['F'].round(2)\n",
    "apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "apa_table['Partial '] = apa_table['Partial '].round(3)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "print(f\"Model: tsnrMedian ~ headcoil * mb * me + smoothness + (1 | subject)\")\n",
    "print(f\"Data: Common complete subjects (N = {total_n})\\n\")\n",
    "print(apa_table.to_string(index=False))\n",
    "\n",
    "# Save APA table\n",
    "apa_table.to_csv(anova_table_file, index=False)\n",
    "print(f\"\\nAPA table saved to '{anova_table_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate means and SEs for plotting\n",
    "plot_data = tsnr_filtered.groupby(['mb', 'me', 'headcoil']).agg({\n",
    "    'tsnrMedian': ['mean', 'sem']\n",
    "}).reset_index()\n",
    "plot_data.columns = ['mb', 'me', 'headcoil', 'mean', 'sem']\n",
    "\n",
    "# Create bar plot\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "width = 0.4\n",
    "x_pos = np.array([0, 1.2, 2.4])\n",
    "colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "\n",
    "# Calculate y-axis limits based on data\n",
    "all_values = plot_data['mean'].values\n",
    "all_errors = plot_data['sem'].values\n",
    "y_max = max(all_values + all_errors) * 1.1\n",
    "y_min = min(all_values - all_errors) * 0.9\n",
    "y_limits = (max(0, y_min), y_max)\n",
    "\n",
    "for idx, (headcoil, ax) in enumerate(zip(['20', '64'], axes)):\n",
    "    hc_data = plot_data[plot_data['headcoil'] == headcoil]\n",
    "    \n",
    "    for me_level in ['me1', 'me4']:\n",
    "        me_data = hc_data[hc_data['me'] == me_level]\n",
    "        \n",
    "        means = []\n",
    "        sems = []\n",
    "        for mb in ['mb1', 'mb3', 'mb6']:\n",
    "            mb_data = me_data[me_data['mb'] == mb]\n",
    "            if not mb_data.empty:\n",
    "                means.append(mb_data['mean'].iloc[0])\n",
    "                sems.append(mb_data['sem'].iloc[0])\n",
    "            else:\n",
    "                means.append(0)\n",
    "                sems.append(0)\n",
    "        \n",
    "        offset = -width/2 if me_level == 'me1' else width/2\n",
    "        ax.bar(x_pos + offset, means, width, \n",
    "               label=me_level, color=colors[me_level],\n",
    "               yerr=sems, capsize=5)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(['mb1', 'mb3', 'mb6'])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "    ax.set_ylim(y_limits)\n",
    "    \n",
    "    n_subjects = len(tsnr_filtered[tsnr_filtered['headcoil'] == headcoil]['subject'].unique())\n",
    "    ax.set_title(f'{headcoil}-Channel (n={n_subjects})', fontsize=38)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('TSNR Median', fontsize=38)\n",
    "\n",
    "# Add legend to lower right\n",
    "axes[1].legend(title='Multi-echo', fontsize=29, title_fontsize=29,\n",
    "               loc='lower right')\n",
    "\n",
    "# Add shared x-label\n",
    "fig.supxlabel('Multiband Factor', fontsize=38, y=0.12)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(tsnr_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot saved to '{tsnr_plot_file}'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Common complete subjects: {total_n}\")\n",
    "print(f\"  - 20-channel: {headcoil_counts.get('20', 0)}\")\n",
    "print(f\"  - 64-channel: {headcoil_counts.get('64', 0)}\")\n",
    "print(f\"\\nFiles generated in {output_dir}:\")\n",
    "print(f\"  - {complete_subjects_file.name}\")\n",
    "print(f\"  - {anova_table_file.name}\")\n",
    "print(f\"  - {tsnr_plot_file.name}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43a733-f223-4636-b2e2-3a40e70dbec9",
   "metadata": {},
   "source": [
    "### ROI-Based Temporal Signal-to-Noise Ratio (TSNR) Analysis (Fig. 5)\n",
    "This kernel analyzes TSNR in fMRI data from 3 ROIs with spatial smoothness as a covariate by:\n",
    "1. **Loading and merging datasets** from TSNR measurements and smoothness data\n",
    "2. **Filtering subjects** to include only those from the main study population with complete data in both metrics\n",
    "3. **Running Linear Mixed Effects models** testing the effects of:\n",
    "   - Headcoil type (20 vs 64 channel)\n",
    "   - Multiband acceleration factor (mb1, mb3, mb6)\n",
    "   - Multi-echo acquisition (me1, me4)\n",
    "   - Spatial smoothness as a continuous covariate\n",
    "   - All interactions between acquisition parameters\n",
    "4. **Generating visualizations** showing TSNR median values:\n",
    "   - Bar plots separated by headcoil type\n",
    "   - Error bars representing standard error\n",
    "   - Legend indicating multi-echo conditions\n",
    "5. **Saving outputs** to `derivatives/plots/` including:\n",
    "   - Complete APA-formatted ANOVA table with effect sizes\n",
    "   - List of subjects with complete data\n",
    "   - Bar plot figure comparing acquisition parameters\n",
    "\n",
    "The analysis quantifies how acquisition parameters and spatial smoothness jointly influence temporal signal quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172cd8-0f5a-4d0c-ba1b-fca08590a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-ROI TSNR ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths relative to project root\n",
    "extractions_dir = project_root / 'derivatives' / 'extractions'\n",
    "smoothness_path = project_root / 'code' / 'smoothness-all.csv'\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define parameters\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"tsnr\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define ROIs to analyze\n",
    "ROIS = [\"VSconstrained\", \"rFFA\", \"bilateralMotor\"]\n",
    "ROI_LABELS = {\n",
    "    \"VSconstrained\": \"Ventral Striatum\",\n",
    "    \"rFFA\": \"Right FFA\", \n",
    "    \"bilateralMotor\": \"Motor Cortex\"\n",
    "}\n",
    "\n",
    "# Get valid subjects and headcoil information from main_df_final\n",
    "valid_subjects = main_df_final['Subject'].astype(str).tolist()\n",
    "headcoil_mapping = dict(zip(main_df_final['Subject'].astype(str), \n",
    "                           main_df_final['Headcoil'].astype(int).astype(str)))\n",
    "\n",
    "print(f\"Filtering to {len(valid_subjects)} subjects from main population\")\n",
    "print(f\"Processing TSNR for ROIs: {', '.join(ROIS)}\")\n",
    "\n",
    "# Output file names\n",
    "complete_subjects_file = output_dir / 'complete_subjects_multi_roi_tsnr_with_smoothness.csv'\n",
    "combined_plot_file = output_dir / 'multi_roi_tsnr_bar_plot.png'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_roi_tsnr_data(base_dir, type_value, img_value, mask_value, denoise_value, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Extract ROI TSNR data from .txt files\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip if not matching criteria\n",
    "            if (sub_id not in valid_subjects or \n",
    "                file_type != type_value or \n",
    "                img != img_value or \n",
    "                mask != mask_value or \n",
    "                denoise != denoise_value or\n",
    "                acq not in acq_params_list):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = headcoil_mapping.get(sub_id, None)\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_smoothness_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process smoothness data with shift correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift correction\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, me\n",
    "        data['subject'] = data['file_path'].str.extract(r'sub-(\\d+)')\n",
    "        data['acq'] = data['file_path'].str.extract(r'acq-(mb\\dme\\d)')\n",
    "        data['mb'] = data['acq'].str[:3]\n",
    "        data['me'] = data['acq'].str[3:]\n",
    "        \n",
    "        # Filter to valid subjects\n",
    "        data = data[data['subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Add headcoil from mapping\n",
    "        data['headcoil'] = data['subject'].map(headcoil_mapping)\n",
    "        \n",
    "        # Create combined label\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Select columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data_df, value_col):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    expected_acq = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    \n",
    "    pivot = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    ).reindex(columns=expected_acq)\n",
    "    \n",
    "    complete = pivot.dropna().index.tolist()\n",
    "    return complete\n",
    "\n",
    "def run_lme_analysis(merged_data, roi_name):\n",
    "    \"\"\"Run LME analysis for a specific ROI\"\"\"\n",
    "    # Prepare data\n",
    "    model_data = merged_data.copy()\n",
    "    model_data['headcoil_encoded'] = model_data['headcoil'].cat.codes - 0.5\n",
    "    \n",
    "    # Fit model\n",
    "    model = Lmer(f'{IMG_VALUE} ~ headcoil_encoded * mb * me + smoothness + (1 | subject)', \n",
    "                 data=model_data)\n",
    "    model.fit()\n",
    "    \n",
    "    # Get ANOVA results\n",
    "    anova_results = model.anova()\n",
    "    \n",
    "    # Define effect names\n",
    "    effect_names = {\n",
    "        'headcoil_encoded': 'Head Coil',\n",
    "        'mb': 'Multiband',\n",
    "        'me': 'Multi-echo',\n",
    "        'smoothness': 'Smoothness',\n",
    "        'headcoil_encoded:mb': 'Head Coil  Multiband',\n",
    "        'headcoil_encoded:me': 'Head Coil  Multi-echo',\n",
    "        'mb:me': 'Multiband  Multi-echo',\n",
    "        'headcoil_encoded:mb:me': 'Head Coil  Multiband  Multi-echo'\n",
    "    }\n",
    "    \n",
    "    # Define degrees of freedom\n",
    "    df_dict = {\n",
    "        'Head Coil': 1,\n",
    "        'Multiband': 2,\n",
    "        'Multi-echo': 1,\n",
    "        'Smoothness': 1,\n",
    "        'Head Coil  Multiband': 2,\n",
    "        'Head Coil  Multi-echo': 1,\n",
    "        'Multiband  Multi-echo': 2,\n",
    "        'Head Coil  Multiband  Multi-echo': 2\n",
    "    }\n",
    "    \n",
    "    # Build APA table\n",
    "    apa_data = []\n",
    "    for effect in anova_results.index:\n",
    "        if effect not in ['(Intercept)', 'Residuals']:\n",
    "            effect_name = effect_names.get(effect, effect)\n",
    "            apa_data.append({\n",
    "                'Effect': effect_name,\n",
    "                'Sum Sq': anova_results.loc[effect, 'SS'] if 'SS' in anova_results.columns else np.nan,\n",
    "                'Mean Sq': anova_results.loc[effect, 'MS'] if 'MS' in anova_results.columns else np.nan,\n",
    "                'Num df': df_dict.get(effect_name, np.nan),\n",
    "                'Den df': anova_results.loc[effect, 'DenomDF'] if 'DenomDF' in anova_results.columns else np.nan,\n",
    "                'F': anova_results.loc[effect, 'F-stat'] if 'F-stat' in anova_results.columns else np.nan,\n",
    "                'p': anova_results.loc[effect, 'P-val'] if 'P-val' in anova_results.columns else np.nan,\n",
    "                'Partial ': np.nan\n",
    "            })\n",
    "    \n",
    "    # Compute partial eta-squared\n",
    "    try:\n",
    "        residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "    except (KeyError, IndexError):\n",
    "        residual_var = model.ranef_var.iloc[1]['Var']\n",
    "    \n",
    "    n_obs = len(model_data)\n",
    "    n_fixed = sum(df_dict.values())\n",
    "    n_subj = model_data['subject'].nunique()\n",
    "    ss_residual = residual_var * (n_obs - n_fixed - n_subj)\n",
    "    \n",
    "    for i, row in enumerate(apa_data):\n",
    "        ss_effect = row['Sum Sq']\n",
    "        if pd.notna(ss_effect):\n",
    "            apa_data[i]['Partial '] = ss_effect / (ss_effect + ss_residual)\n",
    "    \n",
    "    # Create APA table\n",
    "    apa_table = pd.DataFrame(apa_data)\n",
    "    apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "    apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "    apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "    apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "    apa_table['F'] = apa_table['F'].round(2)\n",
    "    apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "    apa_table['Partial '] = apa_table['Partial '].round(3)\n",
    "    \n",
    "    return apa_table\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD DATA AND IDENTIFY COMPLETE SUBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: LOADING DATA AND IDENTIFYING COMPLETE SUBJECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load smoothness data\n",
    "smoothness_data = load_smoothness_data(smoothness_path, valid_subjects, headcoil_mapping)\n",
    "if smoothness_data is None:\n",
    "    raise Exception(\"Failed to load smoothness data\")\n",
    "\n",
    "complete_smoothness = identify_complete_subjects(smoothness_data, 'smoothness')\n",
    "print(f\"Complete smoothness subjects: {len(complete_smoothness)}\")\n",
    "\n",
    "# Process each ROI\n",
    "roi_results = {}\n",
    "all_roi_data = {}\n",
    "\n",
    "for roi in ROIS:\n",
    "    print(f\"\\nProcessing {roi}...\")\n",
    "    \n",
    "    # Load ROI data\n",
    "    roi_data = extract_roi_tsnr_data(extractions_dir, TYPE_VALUE, IMG_VALUE, \n",
    "                                     roi, DENOISE_VALUE, valid_subjects, headcoil_mapping)\n",
    "    \n",
    "    if roi_data is None:\n",
    "        print(f\"Warning: No data found for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Find complete subjects\n",
    "    complete_roi = identify_complete_subjects(roi_data, IMG_VALUE)\n",
    "    print(f\"Complete {roi} subjects: {len(complete_roi)}\")\n",
    "    \n",
    "    # Find common complete subjects\n",
    "    common_complete = sorted(list(set(complete_roi) & set(complete_smoothness)))\n",
    "    print(f\"Common complete subjects: {len(common_complete)}\")\n",
    "    \n",
    "    if not common_complete:\n",
    "        print(f\"No common complete subjects for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    roi_filtered = roi_data[roi_data['subject'].isin(common_complete)].copy()\n",
    "    smoothness_filtered = smoothness_data[smoothness_data['subject'].isin(common_complete)].copy()\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged = pd.merge(\n",
    "        roi_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "        smoothness_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "        on=['subject', 'mb', 'me'],\n",
    "        how='inner'\n",
    "    ).dropna()\n",
    "    \n",
    "    # Store results\n",
    "    roi_results[roi] = {\n",
    "        'data': merged,\n",
    "        'n_subjects': merged['subject'].nunique(),\n",
    "        'n_20ch': len(merged[merged['headcoil'] == '20']['subject'].unique()),\n",
    "        'n_64ch': len(merged[merged['headcoil'] == '64']['subject'].unique())\n",
    "    }\n",
    "    \n",
    "    all_roi_data[roi] = roi_filtered\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: RUN LME ANALYSES FOR ALL ROIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for roi in ROIS:\n",
    "    if roi not in roi_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n=== {ROI_LABELS[roi]} ({roi}) ===\")\n",
    "    roi_data = roi_results[roi]\n",
    "    print(f\"N = {roi_data['n_subjects']} subjects ({roi_data['n_20ch']} 20-ch, {roi_data['n_64ch']} 64-ch)\")\n",
    "    \n",
    "    # Run LME\n",
    "    apa_table = run_lme_analysis(roi_data['data'], roi)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "    print(f\"Model: {IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)\")\n",
    "    print(f\"Data: {ROI_LABELS[roi]} ROI\\n\")\n",
    "    print(apa_table.to_string(index=False))\n",
    "    \n",
    "    # Save APA table\n",
    "    anova_file = output_dir / f'{roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "    apa_table.to_csv(anova_file, index=False)\n",
    "    print(f\"\\nAPA table saved to '{anova_file.name}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: CREATE COMBINED VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING COMBINED BAR PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate plot data for each ROI\n",
    "plot_data_by_roi = {}\n",
    "for roi in ROIS:\n",
    "    if roi not in all_roi_data:\n",
    "        continue\n",
    "        \n",
    "    roi_df = all_roi_data[roi]\n",
    "    plot_data = roi_df.groupby(['mb', 'me', 'headcoil']).agg({\n",
    "        IMG_VALUE: ['mean', 'sem']\n",
    "    }).reset_index()\n",
    "    plot_data.columns = ['mb', 'me', 'headcoil', 'mean', 'sem']\n",
    "    plot_data_by_roi[roi] = plot_data\n",
    "\n",
    "# Create 3x2 subplot layout\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "fig, axes = plt.subplots(len(ROIS), 2, figsize=(16, 8 * len(ROIS)))\n",
    "\n",
    "width = 0.4\n",
    "x_pos = np.array([0, 1.2, 2.4])\n",
    "colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "mb_levels = ['mb1', 'mb3', 'mb6']\n",
    "\n",
    "# Function to plot data in a subplot\n",
    "def plot_roi_data(ax, data, headcoil, roi_label, n_subjects):\n",
    "    \"\"\"Plot TSNR data for a specific ROI and headcoil\"\"\"\n",
    "    \n",
    "    hc_data = data[data['headcoil'] == headcoil]\n",
    "    \n",
    "    if hc_data.empty:\n",
    "        ax.set_title(f\"{headcoil}-Channel (n=0)\", fontsize=38)\n",
    "        return\n",
    "    \n",
    "    for me_level in ['me1', 'me4']:\n",
    "        me_data = hc_data[hc_data['me'] == me_level]\n",
    "        \n",
    "        means = []\n",
    "        sems = []\n",
    "        for mb in mb_levels:\n",
    "            mb_data = me_data[me_data['mb'] == mb]\n",
    "            if not mb_data.empty:\n",
    "                means.append(mb_data['mean'].iloc[0])\n",
    "                sems.append(mb_data['sem'].iloc[0])\n",
    "            else:\n",
    "                means.append(0)\n",
    "                sems.append(0)\n",
    "        \n",
    "        offset = -width/2 if me_level == 'me1' else width/2\n",
    "        ax.bar(x_pos + offset, means, width, \n",
    "               label=me_level, color=colors[me_level],\n",
    "               yerr=sems, capsize=5)\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(mb_levels)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "    \n",
    "    ax.set_title(f\"{headcoil}-Channel (n={n_subjects})\", fontsize=38)\n",
    "\n",
    "# Plot each ROI\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi not in plot_data_by_roi:\n",
    "        continue\n",
    "    \n",
    "    plot_data = plot_data_by_roi[roi]\n",
    "    roi_label = ROI_LABELS[roi]\n",
    "    \n",
    "    # Get subject counts\n",
    "    if roi in all_roi_data:\n",
    "        n_20ch = len(all_roi_data[roi][all_roi_data[roi]['headcoil'] == '20']['subject'].unique())\n",
    "        n_64ch = len(all_roi_data[roi][all_roi_data[roi]['headcoil'] == '64']['subject'].unique())\n",
    "    else:\n",
    "        n_20ch = n_64ch = 0\n",
    "    \n",
    "    # Plot 20-channel (left column)\n",
    "    plot_roi_data(axes[roi_idx, 0], plot_data, '20', roi_label, n_20ch)\n",
    "    \n",
    "    # Plot 64-channel (right column)\n",
    "    plot_roi_data(axes[roi_idx, 1], plot_data, '64', roi_label, n_64ch)\n",
    "    \n",
    "    # Add specific y-axis labels to each leftmost plot\n",
    "    roi_ylabel_map = {\n",
    "        0: \"Reward>Punishment\",      # VSconstrained\n",
    "        1: \"Face>Non-Face\",           # rFFA\n",
    "        2: \"Button Press>Non-Press\"   # bilateralMotor\n",
    "    }\n",
    "    axes[roi_idx, 0].set_ylabel(roi_ylabel_map[roi_idx], fontsize=29)\n",
    "\n",
    "# Add legend to bottom right subplot\n",
    "axes[0, 1].legend(title='Multi-echo', fontsize=29, title_fontsize=29, \n",
    "                  loc='lower right')\n",
    "\n",
    "# Add overall y-axis label for entire figure\n",
    "fig.text(0.001, 0.5, 'Temporal Signal-to-Noise Ratio (TSNR)', \n",
    "         va='center', rotation='vertical', fontsize=38)\n",
    "\n",
    "# Add shared x-label\n",
    "fig.supxlabel('Multiband Factor', fontsize=38, y=0.04)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.88, left=0.12, hspace=0.35)  # Add hspace for vertical spacing between rows\n",
    "\n",
    "# Add centered ROI labels above each row (after subplots_adjust)\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi in plot_data_by_roi:\n",
    "        # Get the position of the top of the current row's subplots\n",
    "        bbox = axes[roi_idx, 0].get_position()\n",
    "        # Place label at a fixed offset above the top of the subplot\n",
    "        y_pos = bbox.y1 + 0.03\n",
    "        fig.text(0.5, y_pos, ROI_LABELS[roi], \n",
    "                ha='center', fontsize=42, fontweight='bold',\n",
    "                transform=fig.transFigure)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(combined_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Combined plot saved to '{combined_plot_file}'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROIs analyzed: {', '.join(ROIS)}\")\n",
    "print(f\"\\nSubject counts by ROI:\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        r = roi_results[roi]\n",
    "        print(f\"  {ROI_LABELS[roi]}: {r['n_subjects']} total ({r['n_20ch']} 20-ch, {r['n_64ch']} 64-ch)\")\n",
    "print(f\"\\nFiles generated in {output_dir}:\")\n",
    "print(f\"  - {combined_plot_file.name}\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        print(f\"  - {roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04961b-8922-4ce8-afe9-fee8a41248b3",
   "metadata": {},
   "source": [
    "# ROI Beta Activation Analysis with Estimated Marginal Means (Fig. 6)\n",
    "This kernel analyzes beta activation estimates across three brain regions of interest by:\n",
    "1. **Loading beta values** from text files for three ROIs:\n",
    "   - Ventral Striatum (VSconstrained)\n",
    "   - Right Fusiform Face Area (rFFA)\n",
    "   - Motor Cortex (bilateralMotor)\n",
    "2. **Filtering subjects** to include only those from the main study population with complete data\n",
    "3. **Running Linear Mixed Effects models** for each ROI testing effects of:\n",
    "   - Headcoil type (20 vs 64 channel)\n",
    "   - Multiband acceleration factor (mb1, mb3, mb6)\n",
    "   - Multi-echo acquisition (me1, me4)\n",
    "   - Spatial smoothness as a covariate\n",
    "   - All interactions between acquisition parameters\n",
    "4. **Computing Estimated Marginal Means** using R's emmeans package to account for the covariate\n",
    "5. **Generating visualizations** in a 32 subplot layout:\n",
    "   - 3 rows (one per ROI)\n",
    "   - 2 columns (20-channel left, 64-channel right)\n",
    "   - Line plots with jittered points and error bars\n",
    "6. **Saving outputs** to `derivatives/plots/` including:\n",
    "   - Separate APA-formatted ANOVA tables for each ROI\n",
    "   - Combined EMM line plot figure\n",
    "\n",
    "The analysis examines how acquisition parameters affect task-related brain activation while controlling for spatial smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd8d92-af1a-4c3e-8314-0fc203bab287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from pathlib import Path\n",
    "\n",
    "# R integration for EMM calculations\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Activate conversions\n",
    "numpy2ri.activate()\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import R packages\n",
    "lme4 = importr('lme4')\n",
    "emmeans = importr('emmeans')\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-ROI BETA ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND EMMs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths relative to project root\n",
    "extractions_dir = project_root / 'derivatives' / 'extractions'\n",
    "smoothness_path = project_root / 'code' / 'smoothness-all.csv'\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define parameters\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"beta\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define ROIs to analyze\n",
    "ROIS = [\"VSconstrained\", \"rFFA\", \"bilateralMotor\"]\n",
    "ROI_LABELS = {\n",
    "    \"VSconstrained\": \"Ventral Striatum\",\n",
    "    \"rFFA\": \"Right FFA\", \n",
    "    \"bilateralMotor\": \"Motor Cortex\"\n",
    "}\n",
    "\n",
    "# Get valid subjects and headcoil information from main_df_final\n",
    "valid_subjects = main_df_final['Subject'].astype(str).tolist()\n",
    "headcoil_mapping = dict(zip(main_df_final['Subject'].astype(str), \n",
    "                           main_df_final['Headcoil'].astype(int).astype(str)))\n",
    "\n",
    "print(f\"Filtering to {len(valid_subjects)} subjects from main population\")\n",
    "print(f\"Processing beta values for ROIs: {', '.join(ROIS)}\")\n",
    "\n",
    "# Output file\n",
    "combined_emm_plot_file = output_dir / 'multi_roi_beta_emm_plot.png'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_roi_beta_data(base_dir, type_value, img_value, mask_value, denoise_value, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Extract ROI beta data from .txt files\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip if not matching criteria\n",
    "            if (sub_id not in valid_subjects or \n",
    "                file_type != type_value or \n",
    "                img != img_value or \n",
    "                mask != mask_value or \n",
    "                denoise != denoise_value or\n",
    "                acq not in acq_params_list):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = headcoil_mapping.get(sub_id, None)\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_smoothness_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process smoothness data with shift correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift correction\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, me\n",
    "        data['subject'] = data['file_path'].str.extract(r'sub-(\\d+)')\n",
    "        data['acq'] = data['file_path'].str.extract(r'acq-(mb\\dme\\d)')\n",
    "        data['mb'] = data['acq'].str[:3]\n",
    "        data['me'] = data['acq'].str[3:]\n",
    "        \n",
    "        # Filter to valid subjects\n",
    "        data = data[data['subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Add headcoil from mapping\n",
    "        data['headcoil'] = data['subject'].map(headcoil_mapping)\n",
    "        \n",
    "        # Create combined label\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Select columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data_df, value_col):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    expected_acq = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    \n",
    "    pivot = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    ).reindex(columns=expected_acq)\n",
    "    \n",
    "    complete = pivot.dropna().index.tolist()\n",
    "    return complete\n",
    "\n",
    "def run_lme_analysis(merged_data, roi_name):\n",
    "    \"\"\"Run LME analysis for a specific ROI\"\"\"\n",
    "    # Prepare data\n",
    "    model_data = merged_data.copy()\n",
    "    \n",
    "    # Fit model using pymer4\n",
    "    model = Lmer(f'{IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)', \n",
    "                 data=model_data)\n",
    "    model.fit()\n",
    "    \n",
    "    # Get ANOVA results\n",
    "    anova_results = model.anova()\n",
    "    \n",
    "    # Define effect names\n",
    "    effect_names = {\n",
    "        'headcoil': 'Head Coil',\n",
    "        'mb': 'Multiband',\n",
    "        'me': 'Multi-echo',\n",
    "        'smoothness': 'Smoothness',\n",
    "        'headcoil:mb': 'Head Coil  Multiband',\n",
    "        'headcoil:me': 'Head Coil  Multi-echo',\n",
    "        'mb:me': 'Multiband  Multi-echo',\n",
    "        'headcoil:mb:me': 'Head Coil  Multiband  Multi-echo'\n",
    "    }\n",
    "    \n",
    "    # Define degrees of freedom\n",
    "    df_dict = {\n",
    "        'Head Coil': 1,\n",
    "        'Multiband': 2,\n",
    "        'Multi-echo': 1,\n",
    "        'Smoothness': 1,\n",
    "        'Head Coil  Multiband': 2,\n",
    "        'Head Coil  Multi-echo': 1,\n",
    "        'Multiband  Multi-echo': 2,\n",
    "        'Head Coil  Multiband  Multi-echo': 2\n",
    "    }\n",
    "    \n",
    "    # Build APA table\n",
    "    apa_data = []\n",
    "    for effect in anova_results.index:\n",
    "        if effect not in ['(Intercept)', 'Residuals']:\n",
    "            effect_name = effect_names.get(effect, effect)\n",
    "            apa_data.append({\n",
    "                'Effect': effect_name,\n",
    "                'Sum Sq': anova_results.loc[effect, 'SS'] if 'SS' in anova_results.columns else np.nan,\n",
    "                'Mean Sq': anova_results.loc[effect, 'MS'] if 'MS' in anova_results.columns else np.nan,\n",
    "                'Num df': df_dict.get(effect_name, np.nan),\n",
    "                'Den df': anova_results.loc[effect, 'DenomDF'] if 'DenomDF' in anova_results.columns else np.nan,\n",
    "                'F': anova_results.loc[effect, 'F-stat'] if 'F-stat' in anova_results.columns else np.nan,\n",
    "                'p': anova_results.loc[effect, 'P-val'] if 'P-val' in anova_results.columns else np.nan,\n",
    "                'Partial ': np.nan\n",
    "            })\n",
    "    \n",
    "    # Compute partial eta-squared\n",
    "    try:\n",
    "        residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "    except (KeyError, IndexError):\n",
    "        residual_var = model.ranef_var.iloc[1]['Var']\n",
    "    \n",
    "    for i, row in enumerate(apa_data):\n",
    "        ss_effect = row['Sum Sq']\n",
    "        denom_df = row['Den df']\n",
    "        if pd.notna(ss_effect) and pd.notna(residual_var) and pd.notna(denom_df):\n",
    "            apa_data[i]['Partial '] = ss_effect / (ss_effect + (residual_var * denom_df))\n",
    "    \n",
    "    # Create APA table\n",
    "    apa_table = pd.DataFrame(apa_data)\n",
    "    apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "    apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "    apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "    apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "    apa_table['F'] = apa_table['F'].round(2)\n",
    "    apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "    apa_table['Partial '] = apa_table['Partial '].round(3)\n",
    "    \n",
    "    return apa_table, model_data\n",
    "\n",
    "def compute_emms_for_roi(data, roi_name):\n",
    "    \"\"\"Compute estimated marginal means using R for a specific ROI\"\"\"\n",
    "    # Prepare data for R\n",
    "    data_r = data.rename(columns={\n",
    "        'subject': 'Subj',\n",
    "        'headcoil': 'HC',\n",
    "        'mb': 'MB',\n",
    "        'me': 'ME',\n",
    "        IMG_VALUE: 'BetaValue'\n",
    "    })\n",
    "    \n",
    "    # Drop NaN values\n",
    "    data_r = data_r.dropna(subset=['BetaValue', 'MB', 'ME', 'HC', 'Subj', 'smoothness'])\n",
    "    \n",
    "    # Ensure string types\n",
    "    data_r['Subj'] = data_r['Subj'].astype(str)\n",
    "    data_r['MB'] = data_r['MB'].astype(str)\n",
    "    data_r['ME'] = data_r['ME'].astype(str)\n",
    "    data_r['HC'] = data_r['HC'].astype(str)\n",
    "    \n",
    "    # Convert to R DataFrame\n",
    "    r_df_temp = pandas2ri.py2rpy(data_r)\n",
    "    r_df_name = f\"temp_emm_data_{roi_name}\"\n",
    "    ro.globalenv[r_df_name] = r_df_temp\n",
    "    \n",
    "    # Define factor levels in R\n",
    "    ro.r('''\n",
    "    levels_MB <- c(\"mb1\", \"mb3\", \"mb6\")\n",
    "    levels_ME <- c(\"me1\", \"me4\")\n",
    "    levels_HC <- c(\"20\", \"64\")\n",
    "    ''')\n",
    "    \n",
    "    # Apply factor conversions\n",
    "    r_df = ro.r(f'''\n",
    "    transform({r_df_name}, MB = factor(MB, levels = levels_MB), ME = factor(ME, levels = levels_ME), HC = factor(HC, levels = levels_HC))\n",
    "    ''')\n",
    "    \n",
    "    # Clean up\n",
    "    del ro.globalenv[r_df_name]\n",
    "    \n",
    "    # Fit model in R\n",
    "    formula = Formula('BetaValue ~ HC * MB * ME + smoothness + (1 | Subj)')\n",
    "    model = lme4.lmer(formula, data=r_df)\n",
    "    \n",
    "    # Compute EMMs\n",
    "    emm = emmeans.emmeans(model, 'MB', by=ro.StrVector(['HC', 'ME']))\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    emm_r_df = ro.r('as.data.frame')(emm)\n",
    "    emm_df = pandas2ri.rpy2py(emm_r_df)\n",
    "    \n",
    "    # Map numeric factors back to strings\n",
    "    mb_map = {1: 'mb1', 2: 'mb3', 3: 'mb6'}\n",
    "    me_map = {1: 'me1', 2: 'me4'}\n",
    "    hc_map = {1: '20', 2: '64'}\n",
    "    \n",
    "    emm_df['MB'] = emm_df['MB'].map(mb_map)\n",
    "    emm_df['ME'] = emm_df['ME'].map(me_map)\n",
    "    emm_df['HC'] = emm_df['HC'].map(hc_map)\n",
    "    \n",
    "    return emm_df\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD DATA AND IDENTIFY COMPLETE SUBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: LOADING DATA AND IDENTIFYING COMPLETE SUBJECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load smoothness data\n",
    "smoothness_data = load_smoothness_data(smoothness_path, valid_subjects, headcoil_mapping)\n",
    "if smoothness_data is None:\n",
    "    raise Exception(\"Failed to load smoothness data\")\n",
    "\n",
    "complete_smoothness = identify_complete_subjects(smoothness_data, 'smoothness')\n",
    "print(f\"Complete smoothness subjects: {len(complete_smoothness)}\")\n",
    "\n",
    "# Process each ROI\n",
    "roi_results = {}\n",
    "emm_results = {}\n",
    "\n",
    "for roi in ROIS:\n",
    "    print(f\"\\nProcessing {roi}...\")\n",
    "    \n",
    "    # Load ROI data\n",
    "    roi_data = extract_roi_beta_data(extractions_dir, TYPE_VALUE, IMG_VALUE, \n",
    "                                     roi, DENOISE_VALUE, valid_subjects, headcoil_mapping)\n",
    "    \n",
    "    if roi_data is None:\n",
    "        print(f\"Warning: No data found for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Find complete subjects\n",
    "    complete_roi = identify_complete_subjects(roi_data, IMG_VALUE)\n",
    "    print(f\"Complete {roi} subjects: {len(complete_roi)}\")\n",
    "    \n",
    "    # Find common complete subjects\n",
    "    common_complete = sorted(list(set(complete_roi) & set(complete_smoothness)))\n",
    "    print(f\"Common complete subjects: {len(common_complete)}\")\n",
    "    \n",
    "    # Identify missing subjects if there are fewer than expected\n",
    "    if roi == \"bilateralMotor\" and len(common_complete) < len(complete_smoothness):\n",
    "        missing_in_motor = sorted(list(set(complete_smoothness) - set(complete_roi)))\n",
    "        print(f\"Subjects missing from Motor Cortex: {missing_in_motor}\")\n",
    "    \n",
    "    if not common_complete:\n",
    "        print(f\"No common complete subjects for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    roi_filtered = roi_data[roi_data['subject'].isin(common_complete)].copy()\n",
    "    smoothness_filtered = smoothness_data[smoothness_data['subject'].isin(common_complete)].copy()\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged = pd.merge(\n",
    "        roi_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "        smoothness_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "        on=['subject', 'mb', 'me'],\n",
    "        how='inner'\n",
    "    ).dropna()\n",
    "    \n",
    "    # Store results\n",
    "    roi_results[roi] = {\n",
    "        'data': merged,\n",
    "        'n_subjects': merged['subject'].nunique(),\n",
    "        'n_20ch': len(merged[merged['headcoil'] == '20']['subject'].unique()),\n",
    "        'n_64ch': len(merged[merged['headcoil'] == '64']['subject'].unique()),\n",
    "        'filtered_data': roi_filtered\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: RUN LME ANALYSES FOR ALL ROIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for roi in ROIS:\n",
    "    if roi not in roi_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n=== {ROI_LABELS[roi]} ({roi}) ===\")\n",
    "    roi_data = roi_results[roi]\n",
    "    print(f\"N = {roi_data['n_subjects']} subjects ({roi_data['n_20ch']} 20-ch, {roi_data['n_64ch']} 64-ch)\")\n",
    "    \n",
    "    # Run LME\n",
    "    apa_table, model_data = run_lme_analysis(roi_data['data'], roi)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "    print(f\"Model: {IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)\")\n",
    "    print(f\"Data: {ROI_LABELS[roi]} ROI\\n\")\n",
    "    print(apa_table.to_string(index=False))\n",
    "    \n",
    "    # Save APA table\n",
    "    anova_file = output_dir / f'{roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "    apa_table.to_csv(anova_file, index=False)\n",
    "    print(f\"\\nAPA table saved to '{anova_file.name}'\")\n",
    "    \n",
    "    # Compute EMMs\n",
    "    emm_df = compute_emms_for_roi(roi_data['data'], roi)\n",
    "    emm_results[roi] = emm_df\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: CREATE COMBINED EMM VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING COMBINED EMM PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create 3x2 subplot layout\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "fig, axes = plt.subplots(len(ROIS), 2, figsize=(16, 8 * len(ROIS)))\n",
    "\n",
    "me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "\n",
    "# Function to plot EMM data in a subplot\n",
    "def plot_emm_data(ax, emm_df, headcoil, roi_label, n_subjects):\n",
    "    \"\"\"Plot EMM data for a specific ROI and headcoil\"\"\"\n",
    "    \n",
    "    coil_data = emm_df[emm_df['HC'] == headcoil]\n",
    "    \n",
    "    if coil_data.empty:\n",
    "        ax.set_title(f\"{headcoil}-Channel (n=0)\", fontsize=38)\n",
    "        return\n",
    "    \n",
    "    # Define x positions and jitter\n",
    "    x_positions = {'mb1': 0, 'mb3': 1, 'mb6': 2}\n",
    "    jitter_amount = 0.05  # Adjust this value to control separation\n",
    "    me_jitter = {'me1': -jitter_amount, 'me4': jitter_amount}\n",
    "    \n",
    "    for me in ['me1', 'me4']:\n",
    "        me_data = coil_data[coil_data['ME'] == me]\n",
    "        me_data = me_data.sort_values('MB', key=lambda x: x.str.extract(r'mb(\\d+)')[0].astype(int))\n",
    "        \n",
    "        # Create jittered x positions\n",
    "        x_vals = [x_positions[mb] + me_jitter[me] for mb in me_data['MB']]\n",
    "        \n",
    "        ax.plot(x_vals, me_data['emmean'], \n",
    "                marker='o', color=me_colors[me], label=me, \n",
    "                linewidth=5, markersize=15)\n",
    "        ax.errorbar(x_vals, me_data['emmean'], yerr=me_data['SE'], \n",
    "                    fmt='none', color=me_colors[me], \n",
    "                    capsize=8, capthick=4, elinewidth=3)\n",
    "    \n",
    "    # Set x-axis properties\n",
    "    ax.set_xticks([0, 1, 2])\n",
    "    ax.set_xticklabels(['mb1', 'mb3', 'mb6'])\n",
    "    ax.set_title(f\"{headcoil}-Channel (n={n_subjects})\", fontsize=38)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "\n",
    "# Calculate global y-limits across all ROIs\n",
    "all_y_values = []\n",
    "all_y_errors = []\n",
    "for roi in ROIS:\n",
    "    if roi in emm_results:\n",
    "        emm_df = emm_results[roi]\n",
    "        all_y_values.extend(emm_df['emmean'].values)\n",
    "        all_y_errors.extend(emm_df['SE'].values)\n",
    "\n",
    "if all_y_values:\n",
    "    y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "    y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "    margin = (y_max - y_min) * 0.1\n",
    "    y_limits = (y_min - margin, y_max + margin)\n",
    "else:\n",
    "    y_limits = (-1, 1)\n",
    "\n",
    "# Plot each ROI\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi not in emm_results:\n",
    "        continue\n",
    "    \n",
    "    emm_df = emm_results[roi]\n",
    "    roi_label = ROI_LABELS[roi]\n",
    "    roi_data = roi_results[roi]\n",
    "    \n",
    "    # Plot 20-channel (left column)\n",
    "    plot_emm_data(axes[roi_idx, 0], emm_df, '20', roi_label, roi_data['n_20ch'])\n",
    "    axes[roi_idx, 0].set_ylim(y_limits)\n",
    "    \n",
    "    # Plot 64-channel (right column)\n",
    "    plot_emm_data(axes[roi_idx, 1], emm_df, '64', roi_label, roi_data['n_64ch'])\n",
    "    axes[roi_idx, 1].set_ylim(y_limits)\n",
    "    \n",
    "    # Add specific y-axis labels to each leftmost plot\n",
    "    roi_ylabel_map = {\n",
    "        0: \"Reward>Punishment\",      # VSconstrained\n",
    "        1: \"Face>Non-Face\",           # rFFA\n",
    "        2: \"Button Press>Non-Press\"   # bilateralMotor\n",
    "    }\n",
    "    axes[roi_idx, 0].set_ylabel(roi_ylabel_map[roi_idx], fontsize=29)\n",
    "\n",
    "# Add legend to lower right subplot\n",
    "axes[0, 1].legend(title='Multi-echo', fontsize=29, title_fontsize=29, \n",
    "                  loc='lower right')\n",
    "\n",
    "# Add overall y-axis label for entire figure\n",
    "fig.text(0.001, 0.5, 'Task-Relevant Activation (Estimated Marginal Means, Betas)', \n",
    "         va='center', rotation='vertical', fontsize=38)\n",
    "\n",
    "# Add shared x-label\n",
    "fig.supxlabel('Multiband Factor', fontsize=38, y=0.04)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.88, left=0.12, hspace=0.35)  # Add hspace for vertical spacing between rows\n",
    "\n",
    "# Add centered ROI labels above each row (after subplots_adjust)\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi in emm_results:\n",
    "        # Get the position of the top of the current row's subplots\n",
    "        bbox = axes[roi_idx, 0].get_position()\n",
    "        # Place label at a fixed offset above the top of the subplot\n",
    "        y_pos = bbox.y1 + 0.03\n",
    "        fig.text(0.5, y_pos, ROI_LABELS[roi], \n",
    "                ha='center', fontsize=42, fontweight='bold',\n",
    "                transform=fig.transFigure)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(combined_emm_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Combined EMM plot saved to '{combined_emm_plot_file}'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROIs analyzed: {', '.join(ROIS)}\")\n",
    "print(f\"\\nSubject counts by ROI:\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        r = roi_results[roi]\n",
    "        print(f\"  {ROI_LABELS[roi]}: {r['n_subjects']} total ({r['n_20ch']} 20-ch, {r['n_64ch']} 64-ch)\")\n",
    "print(f\"\\nFiles generated in {output_dir}:\")\n",
    "print(f\"  - {combined_emm_plot_file.name}\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        print(f\"  - {roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa28ce-4567-42a1-b0bc-6d778742e7d0",
   "metadata": {},
   "source": [
    "# ROI Z-statistic Analysis with Estimated Marginal Means (Supplement)\n",
    "This kernel analyzes z-statistics across three brain regions using the same pipeline as the beta analysis:\n",
    "1. **Loading z-stat values** from text files for three ROIs (Ventral Striatum, Right FFA, Motor Cortex)\n",
    "2. **Running Linear Mixed Effects models** with acquisition parameters and smoothness covariate\n",
    "3. **Computing Estimated Marginal Means** using R's emmeans package\n",
    "4. **Generating 32 EMM line plots** with jittered points and error bars\n",
    "5. **Saving outputs** including separate ANOVA tables and combined figure to `derivatives/plots/`\n",
    "\n",
    "The analysis examines how acquisition parameters affect statistical activation strength (z-scores) while controlling for spatial smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e95db-fa28-4ff8-9b9c-22dcd5eeea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pymer4.models import Lmer\n",
    "from pathlib import Path\n",
    "\n",
    "# R integration for EMM calculations\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Activate conversions\n",
    "numpy2ri.activate()\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Import R packages\n",
    "lme4 = importr('lme4')\n",
    "emmeans = importr('emmeans')\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-ROI Z-STAT ANALYSIS PIPELINE WITH SMOOTHNESS COVARIATE AND EMMs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths relative to project root\n",
    "extractions_dir = project_root / 'derivatives' / 'extractions'\n",
    "smoothness_path = project_root / 'code' / 'smoothness-all.csv'\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define parameters\n",
    "TYPE_VALUE = \"act\"\n",
    "IMG_VALUE = \"zstat\"  # Changed from \"beta\" to \"zstat\"\n",
    "DENOISE_VALUE = \"base\"\n",
    "\n",
    "# Define ROIs to analyze\n",
    "ROIS = [\"VSconstrained\", \"rFFA\", \"bilateralMotor\"]\n",
    "ROI_LABELS = {\n",
    "    \"VSconstrained\": \"Ventral Striatum\",\n",
    "    \"rFFA\": \"Right FFA\", \n",
    "    \"bilateralMotor\": \"Motor Cortex\"\n",
    "}\n",
    "\n",
    "# Get valid subjects and headcoil information from main_df_final\n",
    "valid_subjects = main_df_final['Subject'].astype(str).tolist()\n",
    "headcoil_mapping = dict(zip(main_df_final['Subject'].astype(str), \n",
    "                           main_df_final['Headcoil'].astype(int).astype(str)))\n",
    "\n",
    "print(f\"Filtering to {len(valid_subjects)} subjects from main population\")\n",
    "print(f\"Processing z-stat values for ROIs: {', '.join(ROIS)}\")\n",
    "\n",
    "# Output file\n",
    "combined_emm_plot_file = output_dir / 'multi_roi_zstat_emm_plot.png'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_roi_zstat_data(base_dir, type_value, img_value, mask_value, denoise_value, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Extract ROI z-stat data from .txt files\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"ts_sub-(\\d+)_acq_([^_]+)_type-((?:act|ppi_seed-VS_thr5))_img-([^_]+)_mask-([^_]+)_denoise_([^\\.]+)\\.txt\"\n",
    "    )\n",
    "    data_records = []\n",
    "    acq_params_list = [\"mb1me1\", \"mb3me1\", \"mb6me1\", \"mb1me4\", \"mb3me4\", \"mb6me4\"]\n",
    "    file_paths = glob.glob(os.path.join(base_dir, \"*.txt\"))\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            sub_id, acq, file_type, img, mask, denoise = match.groups()\n",
    "            \n",
    "            # Skip if not matching criteria\n",
    "            if (sub_id not in valid_subjects or \n",
    "                file_type != type_value or \n",
    "                img != img_value or \n",
    "                mask != mask_value or \n",
    "                denoise != denoise_value or\n",
    "                acq not in acq_params_list):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                \n",
    "                # Extract mb and me\n",
    "                mb_match = re.search(r'(mb\\d)', acq)\n",
    "                me_match = re.search(r'(me\\d)', acq)\n",
    "                mb = mb_match.group(1) if mb_match else None\n",
    "                me = me_match.group(1) if me_match else None\n",
    "\n",
    "                headcoil = headcoil_mapping.get(sub_id, None)\n",
    "                \n",
    "                data_records.append({\n",
    "                    'subject': sub_id,\n",
    "                    'headcoil': headcoil,\n",
    "                    'mb': mb,\n",
    "                    'me': me,\n",
    "                    'acq_combined': acq,\n",
    "                    img_value: value\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    if not data_records:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df['headcoil'] = pd.Categorical(df['headcoil'], categories=['20', '64'])\n",
    "    df['mb'] = pd.Categorical(df['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "    df['me'] = pd.Categorical(df['me'], categories=['me1', 'me4'])\n",
    "    df['subject'] = df['subject'].astype(str)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_smoothness_data(csv_path, valid_subjects, headcoil_mapping):\n",
    "    \"\"\"Load and process smoothness data with shift correction\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename columns\n",
    "        data = data.rename(columns={\n",
    "            data.columns[0]: 'path',\n",
    "            'Unnamed: 3': 'smoothness'\n",
    "        })\n",
    "        \n",
    "        # Apply shift correction\n",
    "        data['file_path'] = data['path'].shift(1)\n",
    "        data = data[data['smoothness'].notnull() & data['file_path'].notnull()]\n",
    "        \n",
    "        # Extract subject, mb, me\n",
    "        data['subject'] = data['file_path'].str.extract(r'sub-(\\d+)')\n",
    "        data['acq'] = data['file_path'].str.extract(r'acq-(mb\\dme\\d)')\n",
    "        data['mb'] = data['acq'].str[:3]\n",
    "        data['me'] = data['acq'].str[3:]\n",
    "        \n",
    "        # Filter to valid subjects\n",
    "        data = data[data['subject'].isin(valid_subjects)]\n",
    "        \n",
    "        # Add headcoil from mapping\n",
    "        data['headcoil'] = data['subject'].map(headcoil_mapping)\n",
    "        \n",
    "        # Create combined label\n",
    "        data['acq_combined'] = data['mb'].astype(str) + data['me'].astype(str)\n",
    "        \n",
    "        # Select columns\n",
    "        data = data[['subject', 'headcoil', 'mb', 'me', 'acq_combined', 'smoothness']]\n",
    "        \n",
    "        # Convert to categorical\n",
    "        data['headcoil'] = pd.Categorical(data['headcoil'], categories=['20', '64'])\n",
    "        data['mb'] = pd.Categorical(data['mb'], categories=['mb1', 'mb3', 'mb6'])\n",
    "        data['me'] = pd.Categorical(data['me'], categories=['me1', 'me4'])\n",
    "        \n",
    "        # Drop NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing smoothness data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def identify_complete_subjects(data_df, value_col):\n",
    "    \"\"\"Identify subjects with complete data across all 6 acquisitions\"\"\"\n",
    "    expected_acq = ['mb1me1', 'mb3me1', 'mb6me1', 'mb1me4', 'mb3me4', 'mb6me4']\n",
    "    \n",
    "    pivot = data_df.pivot_table(\n",
    "        values=value_col,\n",
    "        index='subject',\n",
    "        columns='acq_combined',\n",
    "        aggfunc='first'\n",
    "    ).reindex(columns=expected_acq)\n",
    "    \n",
    "    complete = pivot.dropna().index.tolist()\n",
    "    return complete\n",
    "\n",
    "def run_lme_analysis(merged_data, roi_name):\n",
    "    \"\"\"Run LME analysis for a specific ROI\"\"\"\n",
    "    # Prepare data\n",
    "    model_data = merged_data.copy()\n",
    "    \n",
    "    # Fit model using pymer4\n",
    "    model = Lmer(f'{IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)', \n",
    "                 data=model_data)\n",
    "    model.fit()\n",
    "    \n",
    "    # Get ANOVA results\n",
    "    anova_results = model.anova()\n",
    "    \n",
    "    # Define effect names\n",
    "    effect_names = {\n",
    "        'headcoil': 'Head Coil',\n",
    "        'mb': 'Multiband',\n",
    "        'me': 'Multi-echo',\n",
    "        'smoothness': 'Smoothness',\n",
    "        'headcoil:mb': 'Head Coil  Multiband',\n",
    "        'headcoil:me': 'Head Coil  Multi-echo',\n",
    "        'mb:me': 'Multiband  Multi-echo',\n",
    "        'headcoil:mb:me': 'Head Coil  Multiband  Multi-echo'\n",
    "    }\n",
    "    \n",
    "    # Define degrees of freedom\n",
    "    df_dict = {\n",
    "        'Head Coil': 1,\n",
    "        'Multiband': 2,\n",
    "        'Multi-echo': 1,\n",
    "        'Smoothness': 1,\n",
    "        'Head Coil  Multiband': 2,\n",
    "        'Head Coil  Multi-echo': 1,\n",
    "        'Multiband  Multi-echo': 2,\n",
    "        'Head Coil  Multiband  Multi-echo': 2\n",
    "    }\n",
    "    \n",
    "    # Build APA table\n",
    "    apa_data = []\n",
    "    for effect in anova_results.index:\n",
    "        if effect not in ['(Intercept)', 'Residuals']:\n",
    "            effect_name = effect_names.get(effect, effect)\n",
    "            apa_data.append({\n",
    "                'Effect': effect_name,\n",
    "                'Sum Sq': anova_results.loc[effect, 'SS'] if 'SS' in anova_results.columns else np.nan,\n",
    "                'Mean Sq': anova_results.loc[effect, 'MS'] if 'MS' in anova_results.columns else np.nan,\n",
    "                'Num df': df_dict.get(effect_name, np.nan),\n",
    "                'Den df': anova_results.loc[effect, 'DenomDF'] if 'DenomDF' in anova_results.columns else np.nan,\n",
    "                'F': anova_results.loc[effect, 'F-stat'] if 'F-stat' in anova_results.columns else np.nan,\n",
    "                'p': anova_results.loc[effect, 'P-val'] if 'P-val' in anova_results.columns else np.nan,\n",
    "                'Partial ': np.nan\n",
    "            })\n",
    "    \n",
    "    # Compute partial eta-squared\n",
    "    try:\n",
    "        residual_var = model.ranef_var.loc[model.ranef_var['Name'] == '', 'Var'].iloc[0]\n",
    "    except (KeyError, IndexError):\n",
    "        residual_var = model.ranef_var.iloc[1]['Var']\n",
    "    \n",
    "    for i, row in enumerate(apa_data):\n",
    "        ss_effect = row['Sum Sq']\n",
    "        denom_df = row['Den df']\n",
    "        if pd.notna(ss_effect) and pd.notna(residual_var) and pd.notna(denom_df):\n",
    "            apa_data[i]['Partial '] = ss_effect / (ss_effect + (residual_var * denom_df))\n",
    "    \n",
    "    # Create APA table\n",
    "    apa_table = pd.DataFrame(apa_data)\n",
    "    apa_table['Sum Sq'] = apa_table['Sum Sq'].round(2)\n",
    "    apa_table['Mean Sq'] = apa_table['Mean Sq'].round(2)\n",
    "    apa_table['Num df'] = apa_table['Num df'].astype('Int64').fillna(pd.NA)\n",
    "    apa_table['Den df'] = apa_table['Den df'].round(2)\n",
    "    apa_table['F'] = apa_table['F'].round(2)\n",
    "    apa_table['p'] = apa_table['p'].apply(lambda x: '< .001' if pd.notna(x) and x < 0.001 else f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "    apa_table['Partial '] = apa_table['Partial '].round(3)\n",
    "    \n",
    "    return apa_table, model_data\n",
    "\n",
    "def compute_emms_for_roi(data, roi_name):\n",
    "    \"\"\"Compute estimated marginal means using R for a specific ROI\"\"\"\n",
    "    # Prepare data for R\n",
    "    data_r = data.rename(columns={\n",
    "        'subject': 'Subj',\n",
    "        'headcoil': 'HC',\n",
    "        'mb': 'MB',\n",
    "        'me': 'ME',\n",
    "        IMG_VALUE: 'ZstatValue'  # Changed from 'BetaValue' to 'ZstatValue'\n",
    "    })\n",
    "    \n",
    "    # Drop NaN values\n",
    "    data_r = data_r.dropna(subset=['ZstatValue', 'MB', 'ME', 'HC', 'Subj', 'smoothness'])\n",
    "    \n",
    "    # Ensure string types\n",
    "    data_r['Subj'] = data_r['Subj'].astype(str)\n",
    "    data_r['MB'] = data_r['MB'].astype(str)\n",
    "    data_r['ME'] = data_r['ME'].astype(str)\n",
    "    data_r['HC'] = data_r['HC'].astype(str)\n",
    "    \n",
    "    # Convert to R DataFrame\n",
    "    r_df_temp = pandas2ri.py2rpy(data_r)\n",
    "    r_df_name = f\"temp_emm_data_{roi_name}\"\n",
    "    ro.globalenv[r_df_name] = r_df_temp\n",
    "    \n",
    "    # Define factor levels in R\n",
    "    ro.r('''\n",
    "    levels_MB <- c(\"mb1\", \"mb3\", \"mb6\")\n",
    "    levels_ME <- c(\"me1\", \"me4\")\n",
    "    levels_HC <- c(\"20\", \"64\")\n",
    "    ''')\n",
    "    \n",
    "    # Apply factor conversions\n",
    "    r_df = ro.r(f'''\n",
    "    transform({r_df_name}, MB = factor(MB, levels = levels_MB), ME = factor(ME, levels = levels_ME), HC = factor(HC, levels = levels_HC))\n",
    "    ''')\n",
    "    \n",
    "    # Clean up\n",
    "    del ro.globalenv[r_df_name]\n",
    "    \n",
    "    # Fit model in R\n",
    "    formula = Formula('ZstatValue ~ HC * MB * ME + smoothness + (1 | Subj)')\n",
    "    model = lme4.lmer(formula, data=r_df)\n",
    "    \n",
    "    # Compute EMMs\n",
    "    emm = emmeans.emmeans(model, 'MB', by=ro.StrVector(['HC', 'ME']))\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    emm_r_df = ro.r('as.data.frame')(emm)\n",
    "    emm_df = pandas2ri.rpy2py(emm_r_df)\n",
    "    \n",
    "    # Map numeric factors back to strings\n",
    "    mb_map = {1: 'mb1', 2: 'mb3', 3: 'mb6'}\n",
    "    me_map = {1: 'me1', 2: 'me4'}\n",
    "    hc_map = {1: '20', 2: '64'}\n",
    "    \n",
    "    emm_df['MB'] = emm_df['MB'].map(mb_map)\n",
    "    emm_df['ME'] = emm_df['ME'].map(me_map)\n",
    "    emm_df['HC'] = emm_df['HC'].map(hc_map)\n",
    "    \n",
    "    return emm_df\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD DATA AND IDENTIFY COMPLETE SUBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: LOADING DATA AND IDENTIFYING COMPLETE SUBJECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load smoothness data\n",
    "smoothness_data = load_smoothness_data(smoothness_path, valid_subjects, headcoil_mapping)\n",
    "if smoothness_data is None:\n",
    "    raise Exception(\"Failed to load smoothness data\")\n",
    "\n",
    "complete_smoothness = identify_complete_subjects(smoothness_data, 'smoothness')\n",
    "print(f\"Complete smoothness subjects: {len(complete_smoothness)}\")\n",
    "\n",
    "# Process each ROI\n",
    "roi_results = {}\n",
    "emm_results = {}\n",
    "\n",
    "for roi in ROIS:\n",
    "    print(f\"\\nProcessing {roi}...\")\n",
    "    \n",
    "    # Load ROI data\n",
    "    roi_data = extract_roi_zstat_data(extractions_dir, TYPE_VALUE, IMG_VALUE, \n",
    "                                      roi, DENOISE_VALUE, valid_subjects, headcoil_mapping)\n",
    "    \n",
    "    if roi_data is None:\n",
    "        print(f\"Warning: No data found for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Find complete subjects\n",
    "    complete_roi = identify_complete_subjects(roi_data, IMG_VALUE)\n",
    "    print(f\"Complete {roi} subjects: {len(complete_roi)}\")\n",
    "    \n",
    "    # Find common complete subjects\n",
    "    common_complete = sorted(list(set(complete_roi) & set(complete_smoothness)))\n",
    "    print(f\"Common complete subjects: {len(common_complete)}\")\n",
    "    \n",
    "    # Identify missing subjects if there are fewer than expected\n",
    "    if roi == \"bilateralMotor\" and len(common_complete) < len(complete_smoothness):\n",
    "        missing_in_motor = sorted(list(set(complete_smoothness) - set(complete_roi)))\n",
    "        print(f\"Subjects missing from Motor Cortex: {missing_in_motor}\")\n",
    "    \n",
    "    if not common_complete:\n",
    "        print(f\"No common complete subjects for {roi}\")\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    roi_filtered = roi_data[roi_data['subject'].isin(common_complete)].copy()\n",
    "    smoothness_filtered = smoothness_data[smoothness_data['subject'].isin(common_complete)].copy()\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged = pd.merge(\n",
    "        roi_filtered[['subject', 'headcoil', 'mb', 'me', IMG_VALUE]],\n",
    "        smoothness_filtered[['subject', 'mb', 'me', 'smoothness']],\n",
    "        on=['subject', 'mb', 'me'],\n",
    "        how='inner'\n",
    "    ).dropna()\n",
    "    \n",
    "    # Store results\n",
    "    roi_results[roi] = {\n",
    "        'data': merged,\n",
    "        'n_subjects': merged['subject'].nunique(),\n",
    "        'n_20ch': len(merged[merged['headcoil'] == '20']['subject'].unique()),\n",
    "        'n_64ch': len(merged[merged['headcoil'] == '64']['subject'].unique()),\n",
    "        'filtered_data': roi_filtered\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: RUN LME ANALYSES FOR ALL ROIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: LINEAR MIXED EFFECTS ANALYSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for roi in ROIS:\n",
    "    if roi not in roi_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n=== {ROI_LABELS[roi]} ({roi}) ===\")\n",
    "    roi_data = roi_results[roi]\n",
    "    print(f\"N = {roi_data['n_subjects']} subjects ({roi_data['n_20ch']} 20-ch, {roi_data['n_64ch']} 64-ch)\")\n",
    "    \n",
    "    # Run LME\n",
    "    apa_table, model_data = run_lme_analysis(roi_data['data'], roi)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nAPA-Style ANOVA Table for Linear Mixed Effects Model:\")\n",
    "    print(f\"Model: {IMG_VALUE} ~ headcoil * mb * me + smoothness + (1 | subject)\")\n",
    "    print(f\"Data: {ROI_LABELS[roi]} ROI\\n\")\n",
    "    print(apa_table.to_string(index=False))\n",
    "    \n",
    "    # Save APA table\n",
    "    anova_file = output_dir / f'{roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv'\n",
    "    apa_table.to_csv(anova_file, index=False)\n",
    "    print(f\"\\nAPA table saved to '{anova_file.name}'\")\n",
    "    \n",
    "    # Compute EMMs\n",
    "    emm_df = compute_emms_for_roi(roi_data['data'], roi)\n",
    "    emm_results[roi] = emm_df\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: CREATE COMBINED EMM VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: GENERATING COMBINED EMM PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create 3x2 subplot layout\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "fig, axes = plt.subplots(len(ROIS), 2, figsize=(16, 8 * len(ROIS)))\n",
    "\n",
    "me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "\n",
    "# Function to plot EMM data in a subplot\n",
    "def plot_emm_data(ax, emm_df, headcoil, roi_label, n_subjects):\n",
    "    \"\"\"Plot EMM data for a specific ROI and headcoil\"\"\"\n",
    "    \n",
    "    coil_data = emm_df[emm_df['HC'] == headcoil]\n",
    "    \n",
    "    if coil_data.empty:\n",
    "        ax.set_title(f\"{headcoil}-Channel (n=0)\", fontsize=38)\n",
    "        return\n",
    "    \n",
    "    # Define x positions and jitter\n",
    "    x_positions = {'mb1': 0, 'mb3': 1, 'mb6': 2}\n",
    "    jitter_amount = 0.05  # Adjust this value to control separation\n",
    "    me_jitter = {'me1': -jitter_amount, 'me4': jitter_amount}\n",
    "    \n",
    "    for me in ['me1', 'me4']:\n",
    "        me_data = coil_data[coil_data['ME'] == me]\n",
    "        me_data = me_data.sort_values('MB', key=lambda x: x.str.extract(r'mb(\\d+)')[0].astype(int))\n",
    "        \n",
    "        # Create jittered x positions\n",
    "        x_vals = [x_positions[mb] + me_jitter[me] for mb in me_data['MB']]\n",
    "        \n",
    "        ax.plot(x_vals, me_data['emmean'], \n",
    "                marker='o', color=me_colors[me], label=me, \n",
    "                linewidth=5, markersize=15)\n",
    "        ax.errorbar(x_vals, me_data['emmean'], yerr=me_data['SE'], \n",
    "                    fmt='none', color=me_colors[me], \n",
    "                    capsize=8, capthick=4, elinewidth=3)\n",
    "    \n",
    "    # Set x-axis properties\n",
    "    ax.set_xticks([0, 1, 2])\n",
    "    ax.set_xticklabels(['mb1', 'mb3', 'mb6'])\n",
    "    ax.set_title(f\"{headcoil}-Channel (n={n_subjects})\", fontsize=38)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "\n",
    "# Calculate global y-limits across all ROIs\n",
    "all_y_values = []\n",
    "all_y_errors = []\n",
    "for roi in ROIS:\n",
    "    if roi in emm_results:\n",
    "        emm_df = emm_results[roi]\n",
    "        all_y_values.extend(emm_df['emmean'].values)\n",
    "        all_y_errors.extend(emm_df['SE'].values)\n",
    "\n",
    "if all_y_values:\n",
    "    y_max = max([v + e for v, e in zip(all_y_values, all_y_errors)])\n",
    "    y_min = min([v - e for v, e in zip(all_y_values, all_y_errors)])\n",
    "    margin = (y_max - y_min) * 0.1\n",
    "    y_limits = (y_min - margin, y_max + margin)\n",
    "else:\n",
    "    y_limits = (-1, 1)\n",
    "\n",
    "# Plot each ROI\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi not in emm_results:\n",
    "        continue\n",
    "    \n",
    "    emm_df = emm_results[roi]\n",
    "    roi_label = ROI_LABELS[roi]\n",
    "    roi_data = roi_results[roi]\n",
    "    \n",
    "    # Plot 20-channel (left column)\n",
    "    plot_emm_data(axes[roi_idx, 0], emm_df, '20', roi_label, roi_data['n_20ch'])\n",
    "    axes[roi_idx, 0].set_ylim(y_limits)\n",
    "    \n",
    "    # Plot 64-channel (right column)\n",
    "    plot_emm_data(axes[roi_idx, 1], emm_df, '64', roi_label, roi_data['n_64ch'])\n",
    "    axes[roi_idx, 1].set_ylim(y_limits)\n",
    "    \n",
    "    # Add specific y-axis labels to each leftmost plot\n",
    "    roi_ylabel_map = {\n",
    "        0: \"Reward>Punishment\",      # VSconstrained\n",
    "        1: \"Face>Non-Face\",           # rFFA\n",
    "        2: \"Button Press>Non-Press\"   # bilateralMotor\n",
    "    }\n",
    "    axes[roi_idx, 0].set_ylabel(roi_ylabel_map[roi_idx], fontsize=29)\n",
    "\n",
    "# Add legend to upper right subplot\n",
    "axes[0, 1].legend(title='Multi-echo', fontsize=29, title_fontsize=29, \n",
    "                  loc='upper right')\n",
    "\n",
    "# Add overall y-axis label for entire figure\n",
    "fig.text(0.001, 0.5, 'Task-Relevant Activation (Estimated Marginal Means, ZStats)', \n",
    "         va='center', rotation='vertical', fontsize=38)\n",
    "\n",
    "# Add shared x-label\n",
    "fig.supxlabel('Multiband Factor', fontsize=38, y=0.04)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.88, left=0.12, hspace=0.35)\n",
    "\n",
    "# Add centered ROI labels above each row (after subplots_adjust)\n",
    "for roi_idx, roi in enumerate(ROIS):\n",
    "    if roi in emm_results:\n",
    "        # Get the position of the top of the current row's subplots\n",
    "        bbox = axes[roi_idx, 0].get_position()\n",
    "        # Place label at a fixed offset above the top of the subplot\n",
    "        y_pos = bbox.y1 + 0.03\n",
    "        fig.text(0.5, y_pos, ROI_LABELS[roi], \n",
    "                ha='center', fontsize=42, fontweight='bold',\n",
    "                transform=fig.transFigure)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(combined_emm_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"Combined EMM plot saved to '{combined_emm_plot_file}'\")\n",
    "plt.show()\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROIs analyzed: {', '.join(ROIS)}\")\n",
    "print(f\"\\nSubject counts by ROI:\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        r = roi_results[roi]\n",
    "        print(f\"  {ROI_LABELS[roi]}: {r['n_subjects']} total ({r['n_20ch']} 20-ch, {r['n_64ch']} 64-ch)\")\n",
    "print(f\"\\nFiles generated in {output_dir}:\")\n",
    "print(f\"  - {combined_plot_file.name}\")\n",
    "for roi in ROIS:\n",
    "    if roi in roi_results:\n",
    "        print(f\"  - {roi}_{IMG_VALUE}_lme_anova_with_smoothness.csv\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e599012-afc0-43d4-a604-be5eab3c6cd5",
   "metadata": {},
   "source": [
    "# Fig. 7 PPI-based analyses (Betas)\n",
    "This kernel analyzes beta ppi connectivity estimates with VS across 9 brain regions of interest. Not sure if I actually even have the betas though, need to confirm that I can even extract them first and then proceed from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514b0d3-f41b-422e-820b-7a31b0c9a89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2cab4e6-b9de-4797-b5dd-894001364ee5",
   "metadata": {},
   "source": [
    "# Fig. 7 Supplemental Info: PPI-based analyses (ZStats)\n",
    "This kernel analyzes beta ppi connectivity estimates with VS across 9 brain regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973823c-40bf-46cf-932f-89bea2207841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Masks Z-Stats Analysis (9 ROIs)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "project_root = Path.cwd()\n",
    "while project_root.name != 'multiecho-pilot' and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if project_root.name != 'multiecho-pilot':\n",
    "    raise ValueError(\"Could not find 'multiecho-pilot' directory.\")\n",
    "\n",
    "# Define paths\n",
    "timeseries_dir = project_root / 'derivatives' / 'extractions'\n",
    "output_dir = project_root / 'derivatives' / 'plots'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the 9 statistical masks\n",
    "stat_masks = [\n",
    "    \"LME_output_FWER_3way_FINAL\",\n",
    "    \"LME_output_FWER_HCxME_bonf\", \n",
    "    \"LME_output_FWER_MBxME\",\n",
    "    \"LME_output_tsnr_MB3_vs_MB1_FDR\",\n",
    "    \"LME_output_FWER_ME_bonf\",\n",
    "    \"MB_main_ppi_zstat17_thresh\",\n",
    "    \"MBxME_ppi_zstat17_thresh\", \n",
    "    \"ME_main_ppi_zstat17_thresh\",\n",
    "    \"overlap_ppi_zstat17_thr\"\n",
    "]\n",
    "\n",
    "# Use the filtered subject list from demographics analysis\n",
    "try:\n",
    "    subjects_to_include = main_df_final['Subject'].astype(str).tolist()\n",
    "    print(f\"Using {len(subjects_to_include)} subjects from filtered demographics\")\n",
    "except:\n",
    "    print(\"Warning: main_df_final not found, using all available subjects\")\n",
    "    subjects_to_include = None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICAL MASKS Z-STATS ANALYSIS - 9 ROIs\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Timeseries directory: {timeseries_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Processing {len(stat_masks)} statistical masks\")\n",
    "\n",
    "# Process each mask individually\n",
    "for mask_idx, mask in enumerate(stat_masks, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING MASK {mask_idx}/9: {mask}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find all zstat files for this mask\n",
    "    pattern = f\"ts_sub-*_acq_*_type-ppi_seed-VS_thr5_img-zstat_mask-{mask}_denoise_base.txt\"\n",
    "    zstat_files = list(timeseries_dir.glob(pattern))\n",
    "\n",
    "    # DEBUG: Check if files are found\n",
    "    print(f\"DEBUG: Searching in: {timeseries_dir}\")\n",
    "    print(f\"DEBUG: Pattern: {pattern}\")\n",
    "    print(f\"DEBUG: Found {len(zstat_files)} files\")\n",
    "    if len(zstat_files) > 0:\n",
    "        print(f\"DEBUG: First few files: {[f.name for f in zstat_files[:3]]}\")\n",
    "    \n",
    "    if not zstat_files:\n",
    "        print(f\"No zstat files found for mask: {mask}\")\n",
    "        print(f\"Search pattern: {pattern}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(zstat_files)} zstat files\")\n",
    "    \n",
    "    # Parse filenames and extract data\n",
    "    data_rows = []\n",
    "    \n",
    "    for file_path in zstat_files:\n",
    "        filename = file_path.name\n",
    "        # Parse: ts_sub-12042_acq_mb6me4_type-ppi_seed-VS_thr5_img-zstat_mask-overlap_ppi_zstat17_thr_denoise_base.txt\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        try:\n",
    "            # Extract subject ID\n",
    "            subject = parts[1].replace('sub-', '')\n",
    "            \n",
    "            # Extract acquisition parameters\n",
    "            acq_part = parts[3].replace('acq-', '')\n",
    "            \n",
    "            # Parse multiband factor\n",
    "            if 'mb1' in acq_part:\n",
    "                mb = 'mb1'\n",
    "            elif 'mb3' in acq_part:\n",
    "                mb = 'mb3'\n",
    "            elif 'mb6' in acq_part:\n",
    "                mb = 'mb6'\n",
    "            else:\n",
    "                print(f\"Could not parse multiband from: {acq_part}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse multi-echo\n",
    "            if 'me1' in acq_part:\n",
    "                me = 'me1'\n",
    "            elif 'me4' in acq_part:\n",
    "                me = 'me4'\n",
    "            else:\n",
    "                print(f\"Could not parse multi-echo from: {acq_part}\")\n",
    "                continue\n",
    "            \n",
    "            # Filter subjects if demographics filtering is available\n",
    "            if subjects_to_include and subject not in subjects_to_include:\n",
    "                continue\n",
    "            \n",
    "            # Read the zstat value\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    zstat_value = float(f.read().strip())\n",
    "            except:\n",
    "                print(f\"Could not read zstat value from: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Get headcoil info from demographics if available\n",
    "            headcoil = None\n",
    "            if 'main_df_final' in globals():\n",
    "                subj_demo = main_df_final[main_df_final['Subject'].astype(str) == subject]\n",
    "                if not subj_demo.empty:\n",
    "                    headcoil = int(subj_demo['Headcoil'].iloc[0])\n",
    "            \n",
    "            if headcoil is None:\n",
    "                print(f\"No headcoil info found for subject: {subject}\")\n",
    "                continue\n",
    "            \n",
    "            data_rows.append({\n",
    "                'subject': subject,\n",
    "                'mb': mb,\n",
    "                'me': me,\n",
    "                'headcoil': headcoil,\n",
    "                'zstat': zstat_value,\n",
    "                'mask': mask\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"DEBUG: Parsed {len(data_rows)} total rows for this mask\")\n",
    "    if len(data_rows) == 0:\n",
    "        print(\"DEBUG: No rows parsed - check subject filtering or parsing logic\")\n",
    "    \n",
    "    if not data_rows:\n",
    "        print(f\"No valid data found for mask: {mask}\")\n",
    "        continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_mask = pd.DataFrame(data_rows)\n",
    "    print(f\"Successfully parsed {len(df_mask)} observations\")\n",
    "    print(f\"Subjects: {df_mask['subject'].nunique()}\")\n",
    "    print(f\"Headcoil breakdown: {df_mask['headcoil'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Run Linear Mixed Effects Analysis\n",
    "    print(f\"\\nRunning LME analysis for {mask}...\")\n",
    "    \n",
    "    try:\n",
    "        import pymer4\n",
    "        from pymer4.models import Lmer\n",
    "        \n",
    "        # Prepare data for LME\n",
    "        df_lme = df_mask.copy()\n",
    "        df_lme['headcoil'] = df_lme['headcoil'].astype('category')\n",
    "        df_lme['mb'] = df_lme['mb'].astype('category') \n",
    "        df_lme['me'] = df_lme['me'].astype('category')\n",
    "        df_lme['subject'] = df_lme['subject'].astype('category')\n",
    "        \n",
    "        # Fit LME model\n",
    "        model = Lmer('zstat ~ headcoil * mb * me + (1|subject)', data=df_lme)\n",
    "        model.fit(summarize=False)\n",
    "        \n",
    "        # Get ANOVA table\n",
    "        anova_results = model.anova()\n",
    "        \n",
    "        print(\"ANOVA Results:\")\n",
    "        print(anova_results)\n",
    "        \n",
    "        # Save ANOVA results\n",
    "        anova_file = output_dir / f\"anova_zstats_{mask}.csv\"\n",
    "        anova_results.to_csv(anova_file)\n",
    "        print(f\"ANOVA table saved: {anova_file}\")\n",
    "        \n",
    "        # Save detailed model summary\n",
    "        summary_file = output_dir / f\"lme_summary_zstats_{mask}.txt\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"LME Analysis Results for {mask}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(\"Model: zstat ~ headcoil * mb * me + (1|subject)\\n\\n\")\n",
    "            f.write(\"ANOVA Results:\\n\")\n",
    "            f.write(str(anova_results))\n",
    "            f.write(\"\\n\\nModel Summary:\\n\")\n",
    "            f.write(str(model))\n",
    "        \n",
    "        print(f\"Detailed summary saved: {summary_file}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"pymer4 not available - skipping LME analysis\")\n",
    "        print(\"Install with: pip install pymer4\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LME analysis: {e}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    print(f\"\\nCreating visualization for {mask}...\")\n",
    "    \n",
    "    # Calculate summary statistics for plotting\n",
    "    summary_stats = df_mask.groupby(['headcoil', 'mb', 'me'])['zstat'].agg(['mean', 'sem']).reset_index()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.rcParams.update({'font.size': 38})\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Define colors and positions\n",
    "    me_colors = {'me1': 'royalblue', 'me4': 'darkorange'}\n",
    "    headcoils = sorted(df_mask['headcoil'].unique())\n",
    "    \n",
    "    for i, hc in enumerate(headcoils):\n",
    "        ax = axes[i]\n",
    "        hc_data = summary_stats[summary_stats['headcoil'] == hc]\n",
    "        \n",
    "        # Define x positions and jitter\n",
    "        x_positions = {'mb1': 0, 'mb3': 1, 'mb6': 2}\n",
    "        jitter_amount = 0.05\n",
    "        me_jitter = {'me1': -jitter_amount, 'me4': jitter_amount}\n",
    "        \n",
    "        # Plot lines for me1 and me4\n",
    "        for me_val in ['me1', 'me4']:\n",
    "            me_data = hc_data[hc_data['me'] == me_val]\n",
    "            me_data = me_data.sort_values('mb')\n",
    "            \n",
    "            # Create jittered x positions\n",
    "            x_vals = [x_positions[mb] + me_jitter[me_val] for mb in me_data['mb']]\n",
    "            \n",
    "            ax.plot(x_vals, me_data['mean'], \n",
    "                    marker='o', color=me_colors[me_val], label=me_val, \n",
    "                    linewidth=5, markersize=15)\n",
    "            ax.errorbar(x_vals, me_data['mean'], yerr=me_data['sem'], \n",
    "                        fmt='none', color=me_colors[me_val], \n",
    "                        capsize=8, capthick=4, elinewidth=3)\n",
    "        \n",
    "        # Customize subplot\n",
    "        n_subjects = df_mask[df_mask['headcoil'] == hc]['subject'].nunique()\n",
    "        ax.set_title(f'{hc}-Channel (n={n_subjects})', fontsize=38)\n",
    "        ax.set_xlabel('Multiband Factor', fontsize=38)\n",
    "        ax.set_xticks([0, 1, 2])\n",
    "        ax.set_xticklabels(['mb1', 'mb3', 'mb6'])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=29)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Z-statistic', fontsize=38)\n",
    "    \n",
    "    # Add legend to right subplot\n",
    "    axes[1].legend(title='Multi-echo', fontsize=29, title_fontsize=29,\n",
    "                   loc='lower right')\n",
    "    \n",
    "    # Add title above the whole figure\n",
    "    fig.suptitle(mask.replace(\"_\", \" \"), fontsize=42, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_file = output_dir / f\"zstats_{mask}.png\"\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved: {plot_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save detailed data\n",
    "    data_file = output_dir / f\"data_zstats_{mask}.csv\"\n",
    "    df_mask.to_csv(data_file, index=False)\n",
    "    print(f\"Data saved: {data_file}\")\n",
    "    \n",
    "    print(f\" Completed analysis for {mask}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL STATISTICAL MASKS Z-STATS ANALYSIS COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(\"Files generated per mask:\")\n",
    "print(\"  - anova_zstats_[mask].csv (ANOVA results)\")\n",
    "print(\"  - lme_summary_zstats_[mask].txt (detailed summary)\")  \n",
    "print(\"  - zstats_[mask].png (visualization)\")\n",
    "print(\"  - data_zstats_[mask].csv (raw data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104ec25-b869-4621-b50f-4f8a26ff59bf",
   "metadata": {},
   "source": [
    "# Base vs. Tedana Confounds: Generating Group-Level Design Matrices for FSL `randomise` (Figure 8)\n",
    "\n",
    "## Make sure this pulls from the full OpenNeuro dataset; largest N possible for each acq\n",
    "\n",
    "This kernel prepares the behavioral and nuisance covariate data for a group-level (L3) analysis using FSL's randomise. It constructs three separate Pandas DataFrames, one for each multi-echo multiband (MB/ME) acquisition type (`mb1me4`, `mb3me4`, `mb6me4`). These DataFrames serve as the \"right side\" of the `randomise` equation.\n",
    "\n",
    "For each acquisition type, the corresponding DataFrame will contain the following columns:\n",
    "- `ones`: A constant regressor (a column of 1s).\n",
    "- `fd_mean_demeaned`: Mean framewise displacement, de-meaned across subjects for that specific acquisition type.\n",
    "- `headcoil_demeaned`: Headcoil type (encoded as 0 for 20-channel and 1 for 64-channel), de-meaned across subjects for that acquisition type.\n",
    "- `fd_mean_x_headcoil_demeaned`: The interaction term between the de-meaned fd_mean and de-meaned headcoil regressors, with this interaction term also de-meaned.\n",
    "- `smoothness_demeaned`: Voxel-wise smoothness, de-meaned across subjects for that specific acquisition type.\n",
    "\n",
    "The script ensures that all subjects from `sublist-complete.txt` are represented in each DataFrame. If a subject is missing `fd_mean` or `smoothness` data for a particular acquisition type, `NaN` values will be inserted into the corresponding DataFrame columns instead of omitting the subject. This allows `randomise` (or other statistical software) to handle missing data appropriately during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cd23d-f3f2-4e35-bdd7-62d005aed423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress SyntaxWarning for '\\s' as r'\\s+' is used below\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING AND TRUNCATING NEW FD_MEAN DATA SOURCE WITH OUTLIER SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update this path if your GitHub repo is not in Documents/GitHub\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-complete.txt\")\n",
    "\n",
    "# >>>>> CORRECTED PATH FOR FD TSV FILE <<<<<\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "\n",
    "print(f\"New FD TSV data path: {FD_TSV_PATH}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load Master Subject List ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    # Format subjects from sublist-complete.txt to match the 'sub-XXXXX' format in the TSV\n",
    "    all_subjects_formatted = [f\"sub-{s}\" for s in all_subjects_raw]\n",
    "    all_subjects_formatted.sort() # Keep it sorted for consistent output\n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH} (formatted to 'sub-XXXXX').\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Please check the path. Exiting.\")\n",
    "    exit() # Exit if the subject list isn't found\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit() # Exit for other errors\n",
    "\n",
    "# --- Step 2: Load the new FD Mean TSV data ---\n",
    "fd_new_df_raw = None # Initialize to None so it exists even if read_csv fails\n",
    "try:\n",
    "    # Using a raw string r'\\s+' for the separator to handle variable whitespace\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Please check the path to your TSV file. Exiting.\")\n",
    "    exit() # Exit if the FD data file isn't found\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit() # Exit for other errors\n",
    "\n",
    "# --- Step 3: Truncate the DataFrame to only include subjects from the sublist ---\n",
    "# This step will only execute if fd_new_df_raw was successfully loaded due to the exit() calls above.\n",
    "# The 'Sub' column in the TSV should match the 'sub-XXXXX' format.\n",
    "# We will filter based on the formatted subject list.\n",
    "filtered_fd_df = fd_new_df_raw[fd_new_df_raw['Sub'].isin(all_subjects_formatted)].copy()\n",
    "\n",
    "print(f\"\\nTruncated FD DataFrame to {len(filtered_fd_df)} records, including only subjects from {SUBLIST_PATH}.\")\n",
    "\n",
    "# --- Step 4: Summarize and list Outlier Runs ---\n",
    "# Ensure the 'outlier_acq_Custom1' column is boolean type for accurate filtering\n",
    "# Coercing errors to NaN and then filling with False to handle any non-boolean values gracefully\n",
    "filtered_fd_df['outlier_acq_Custom1'] = pd.to_numeric(filtered_fd_df['outlier_acq_Custom1'], errors='coerce')\n",
    "filtered_fd_df['outlier_acq_Custom1'] = filtered_fd_df['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "\n",
    "outlier_runs = filtered_fd_df[filtered_fd_df['outlier_acq_Custom1'] == True].copy()\n",
    "num_outlier_runs = len(outlier_runs)\n",
    "\n",
    "print(f\"\\n--- Outlier Run Summary ---\")\n",
    "print(f\"Total number of runs classified as Outlier: {num_outlier_runs}\")\n",
    "\n",
    "if num_outlier_runs > 0:\n",
    "    print(\"\\nDetails of Outlier Runs:\")\n",
    "    # Display all columns for outlier runs, reset display options if needed\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(outlier_runs)\n",
    "    # Reset to default if desired after printing\n",
    "    # pd.reset_option('display.max_columns')\n",
    "    # pd.reset_option('display.width')\n",
    "else:\n",
    "    print(\"No runs classified as Outlier (i.e., 'True' for outlier_acq_Custom1).\")\n",
    "\n",
    "\n",
    "# --- Step 5: Print the truncated DataFrame (head/tail) as before ---\n",
    "print(\"\\n--- Truncated FD Mean DataFrame (Head) ---\")\n",
    "print(filtered_fd_df.head())\n",
    "\n",
    "print(\"\\n--- Truncated FD Mean DataFrame (Tail) ---\")\n",
    "print(filtered_fd_df.tail())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FD DATA TRUNCATION AND OUTLIER SUMMARY COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966f987-b956-41fe-b94a-ea770d4f433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress SyntaxWarning for '\\s' as r'\\s+' is used below\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONSOLIDATING MULTI-ECHO DATA BY AVERAGING ECHOES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration (Copied from previous kernel for self-containment) ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-complete.txt\")\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "print(f\"FD TSV data path: {FD_TSV_PATH}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load Master Subject List (Copied) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    all_subjects_formatted = [f\"sub-{s}\" for s in all_subjects_raw]\n",
    "    all_subjects_formatted.sort()\n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load and Truncate FD Mean TSV (Copied) ---\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "filtered_fd_df = fd_new_df_raw[fd_new_df_raw['Sub'].isin(all_subjects_formatted)].copy()\n",
    "print(f\"Truncated FD DataFrame to {len(filtered_fd_df)} records, including only subjects from {SUBLIST_PATH}.\")\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean type for proper aggregation\n",
    "# Coercing errors to NaN and then filling with False before converting to bool.\n",
    "filtered_fd_df['outlier_acq_Custom1'] = pd.to_numeric(filtered_fd_df['outlier_acq_Custom1'], errors='coerce')\n",
    "filtered_fd_df['outlier_acq_Custom1'] = filtered_fd_df['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# --- Step 3: Consolidate Multi-Echo Data ---\n",
    "print(\"\\n--- Consolidating Multi-Echo Data ---\")\n",
    "\n",
    "# Create a new 'consolidated_acq' column\n",
    "# This regex will extract 'mbXmeY' from strings like 'mb1me4_echo-1_part-mag'\n",
    "# If the pattern doesn't match (e.g., 'mb3me1'), it will keep the original 'acq' value.\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1) # Return the 'mbXmeY' part\n",
    "    return acq_str # Fallback to original if no match (shouldn't happen with given data)\n",
    "\n",
    "filtered_fd_df['consolidated_acq'] = filtered_fd_df['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules:\n",
    "# - For 'tsnr' and 'fd_mean', take the mean.\n",
    "# - For 'outlier_acq_Custom1', take `any()`: True if any of the original echoes were an outlier.\n",
    "# - Other columns like 'task' will be grouped by and implicitly kept as they are constant per group.\n",
    "aggregation_rules = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any' # True if ANY echo was an outlier\n",
    "}\n",
    "\n",
    "# Perform the grouping and aggregation\n",
    "consolidated_df = filtered_fd_df.groupby(['Sub', 'task', 'consolidated_acq'], as_index=False).agg(aggregation_rules)\n",
    "\n",
    "# Rename 'consolidated_acq' back to 'acq' for consistency with original column name expectations\n",
    "consolidated_df = consolidated_df.rename(columns={'consolidated_acq': 'acq'})\n",
    "\n",
    "# Ensure correct column order, if desired\n",
    "consolidated_df = consolidated_df[['Sub', 'task', 'acq', 'tsnr', 'fd_mean', 'outlier_acq_Custom1']]\n",
    "\n",
    "print(f\"Original DataFrame rows: {len(filtered_fd_df)}\")\n",
    "print(f\"Consolidated DataFrame rows: {len(consolidated_df)}\")\n",
    "print(\"Consolidation complete.\")\n",
    "\n",
    "# --- Step 4: Display the consolidated DataFrame (head/tail) ---\n",
    "print(\"\\n--- Consolidated FD Mean DataFrame (Head) ---\")\n",
    "print(consolidated_df.head())\n",
    "\n",
    "print(\"\\n--- Consolidated FD Mean DataFrame (Tail) ---\")\n",
    "print(consolidated_df.tail())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA CONSOLIDATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229268a-e544-4041-adce-b768b748dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning from pandas if needed\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"DataFrame.applymap has been deprecated\"\n",
    ")\n",
    "# Suppress SyntaxWarning for '\\s' if using regular string, though raw string is better practice\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING SMOOTHNESS TABLE FOR MULTI-ECHO ACQUISITIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the CSV file\n",
    "csv_path = os.path.expanduser('~/Documents/GitHub/multiecho-pilot/code/smoothness-all.csv')\n",
    "# Define the subject list path (using openneuro as requested)\n",
    "SUBLIST_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code/sublist-openneuro.txt\")\n",
    "\n",
    "# Define subjects with 64-channel headcoil (used for headcoil assignment)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define the specific multi-echo acquisitions to display\n",
    "MULTI_ECHO_ACQS = ['mb1me4', 'mb3me4', 'mb6me4']\n",
    "\n",
    "# Output CSV file path for the smoothness table\n",
    "SMOOTHNESS_OUTPUT_CSV = os.path.join(os.path.dirname(SUBLIST_PATH), 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "print(f\"Input smoothness file: {csv_path}\")\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"Displaying multi-echo acquisitions: {', '.join(MULTI_ECHO_ACQS)}\")\n",
    "print(f\"Output CSV will be saved to: {SMOOTHNESS_OUTPUT_CSV}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load and Process Smoothness Data ---\n",
    "print(\"\\n--- Processing Smoothness Data ---\")\n",
    "\n",
    "# Define a helper function to consolidate acquisition names\n",
    "# This is crucial for matching 'mbXmeY' from original data sources\n",
    "# which might have echo suffixes in other files but not in smoothness.csv paths.\n",
    "def get_consolidated_acq_name(acq_str):\n",
    "    # Regex to capture 'mbXmeY' from a string, handles cases like 'mb1me4' or 'mb1me4_echo-1_part-mag'\n",
    "    match = re.match(r'(mb\\dme\\d+)(?:_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1) # Returns 'mbXmeY' part\n",
    "    return acq_str # Returns original string if no match (e.g., 'mb3me1')\n",
    "\n",
    "def load_and_process_smoothness_data(csv_path, all_subjects_list):\n",
    "    processed_records = [] # To store valid, parsed records\n",
    "    try:\n",
    "        raw_data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Rename columns for clarity based on known structure\n",
    "        data_to_process = raw_data.rename(columns={\n",
    "            raw_data.columns[0]: 'path', # First column is the path string\n",
    "            'Unnamed: 3': 'smoothness'   # Column with the actual smoothness value\n",
    "        })\n",
    "\n",
    "        # Apply shift up procedure: current row's 'smoothness' corresponds to previous row's 'path'\n",
    "        data_to_process['file_path'] = data_to_process['path'].shift(1)\n",
    "        \n",
    "        # Convert 'smoothness' column to numeric, coercing errors to NaN\n",
    "        data_to_process['smoothness'] = pd.to_numeric(data_to_process['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter out rows that do not have a valid smoothness value or a file_path\n",
    "        # Also, explicitly convert 'file_path' to string before iterating to prevent float errors\n",
    "        data_filtered = data_to_process[\n",
    "            data_to_process['smoothness'].notna() & data_to_process['file_path'].notna()\n",
    "        ].copy()\n",
    "        data_filtered['file_path'] = data_filtered['file_path'].astype(str)\n",
    "\n",
    "        # Regex pattern to extract subject ID and the full acquisition string\n",
    "        # from the file_path, e.g., 'sub-XXXXX_..._acq-YYYYY_...'\n",
    "        path_parse_pattern = r'sub-(\\d+).*acq-([a-zA-Z0-9_.-]+)'\n",
    "        \n",
    "        initial_smoothness_rows = 0\n",
    "        for index, row in data_filtered.iterrows():\n",
    "            initial_smoothness_rows += 1 # Count rows that passed initial NaN filter\n",
    "            file_path_str = row['file_path']\n",
    "            smoothness_val = row['smoothness']\n",
    "\n",
    "            # Only attempt regex if the string is not 'nan' (from float NaNs) and is non-empty\n",
    "            if file_path_str != 'nan' and file_path_str.strip() != '':\n",
    "                match = re.search(path_parse_pattern, file_path_str)\n",
    "                if match:\n",
    "                    subject = match.group(1)\n",
    "                    acq_raw = match.group(2)\n",
    "                    \n",
    "                    # Consolidate acq name (e.g., 'mb1me4_echo-1' -> 'mb1me4')\n",
    "                    acq = get_consolidated_acq_name(acq_raw)\n",
    "                    \n",
    "                    # Only include if subject is in our list of non-'sp' subjects\n",
    "                    if subject in all_subjects_list:\n",
    "                        processed_records.append({\n",
    "                            'subject': subject,\n",
    "                            'acq': acq,\n",
    "                            'smoothness': smoothness_val\n",
    "                        })\n",
    "                # else: print(f\"Debug: No regex match for path: {file_path_str}\") # Uncomment for debugging\n",
    "            # else: print(f\"Debug: Skipping due to invalid file_path_str: '{file_path_str}'\") # Uncomment for debugging\n",
    "\n",
    "        if not processed_records:\n",
    "            print(\"  No valid smoothness records found after parsing and filtering.\")\n",
    "            return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "\n",
    "        data_parsed = pd.DataFrame(processed_records)\n",
    "        data_parsed['subject'] = data_parsed['subject'].astype(str)\n",
    "        data_parsed['acq'] = data_parsed['acq'].astype(str) # Ensure acq is string before next step\n",
    "\n",
    "        # Assign headcoil based on our defined HEADCOIL_64_SUBJECTS\n",
    "        data_parsed['headcoil'] = data_parsed['subject'].apply(lambda x: '64' if x in HEADCOIL_64_SUBJECTS else '20')\n",
    "        \n",
    "        # Select relevant columns before grouping\n",
    "        data_parsed = data_parsed[['subject', 'headcoil', 'acq', 'smoothness']].copy()\n",
    "        \n",
    "        # Group by subject, headcoil, and consolidated acquisition, then take the mean of smoothness.\n",
    "        # This handles potential multiple entries for the same (subject, acq) combination,\n",
    "        # ensuring a unique smoothness value per group.\n",
    "        initial_rows_before_grouping = len(data_parsed)\n",
    "        data_final = data_parsed.groupby(['subject', 'headcoil', 'acq'], as_index=False)['smoothness'].mean()\n",
    "        \n",
    "        print(f\"  Smoothness data records initially extracted: {initial_smoothness_rows}\")\n",
    "        print(f\"  Smoothness records after subject/acq/smoothness filtering: {len(data_parsed)}\")\n",
    "        print(f\"  Smoothness data records after de-duplication/averaging: {len(data_final)}\")\n",
    "        \n",
    "        print(f\"Loaded and filtered {len(data_final)} smoothness records.\")\n",
    "        return data_final\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Smoothness file not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # Re-raise the exception for full traceback during debugging if needed.\n",
    "        # raise\n",
    "        return pd.DataFrame(columns=['subject', 'headcoil', 'acq', 'smoothness'])\n",
    "\n",
    "# --- Load and Process Smoothness Data using the refined function ---\n",
    "smoothness_df = load_and_process_smoothness_data(csv_path, all_subjects)\n",
    "\n",
    "\n",
    "# --- Step 3: Create the final table for display and saving ---\n",
    "print(\"\\n--- Creating Final Smoothness Table ---\")\n",
    "\n",
    "if smoothness_df.empty:\n",
    "    print(\"Error: Smoothness data could not be loaded or processed. Cannot create table.\")\n",
    "else:\n",
    "    # Filter to include only the desired multi-echo acquisitions\n",
    "    smoothness_filtered_for_display = smoothness_df[\n",
    "        smoothness_df['acq'].isin(MULTI_ECHO_ACQS)\n",
    "    ].copy()\n",
    "\n",
    "    # Ensure all subjects from 'all_subjects' are represented, even if they have no smoothness data\n",
    "    # for the selected acquisitions.\n",
    "    # Create a base DataFrame with all relevant subject-acq combinations.\n",
    "    base_display_combinations = pd.MultiIndex.from_product(\n",
    "        [all_subjects, MULTI_ECHO_ACQS], \n",
    "        names=['subject', 'acq']\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Merge the filtered smoothness data onto this base, ensuring all subjects are present.\n",
    "    # This will introduce NaNs for subjects/acqs where data is missing.\n",
    "    final_display_df = pd.merge(\n",
    "        base_display_combinations,\n",
    "        smoothness_filtered_for_display[['subject', 'acq', 'smoothness']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Pivot the table to get acquisitions as columns and subjects as rows\n",
    "    smoothness_pivot = final_display_df.pivot_table(\n",
    "        values='smoothness',\n",
    "        index='subject',\n",
    "        columns='acq',\n",
    "        aggfunc='first' # 'first' is fine here as we already handled duplicates with .mean() earlier\n",
    "    )\n",
    "\n",
    "    # Reindex columns to ensure desired order of multi-echo acquisitions\n",
    "    smoothness_pivot = smoothness_pivot.reindex(columns=MULTI_ECHO_ACQS)\n",
    "    \n",
    "    # Get headcoil information for all_subjects and merge (should be consistent)\n",
    "    headcoil_info = pd.DataFrame({\n",
    "        'subject': all_subjects,\n",
    "        'headcoil': [('64' if s in HEADCOIL_64_SUBJECTS else '20') for s in all_subjects]\n",
    "    }).set_index('subject')\n",
    "\n",
    "    # Merge headcoil info onto the pivoted table\n",
    "    smoothness_pivot = smoothness_pivot.merge(headcoil_info, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Reorder columns: headcoil first, then the acq types\n",
    "    cols_order = ['headcoil'] + MULTI_ECHO_ACQS\n",
    "    smoothness_pivot = smoothness_pivot[cols_order].sort_index()\n",
    "\n",
    "    # Round smoothness values for display\n",
    "    smoothness_pivot[MULTI_ECHO_ACQS] = smoothness_pivot[MULTI_ECHO_ACQS].round(3)\n",
    "\n",
    "    print(f\"\\nFinal Smoothness Table (Multi-echo Acquisitions Only, All Subjects):\")\n",
    "    # Set display options to show entire DataFrame\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(smoothness_pivot.to_string())\n",
    "    # Reset options to default\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    # Save the generated table to CSV\n",
    "    try:\n",
    "        smoothness_pivot.to_csv(SMOOTHNESS_OUTPUT_CSV)\n",
    "        print(f\"\\nSmoothness table saved to '{SMOOTHNESS_OUTPUT_CSV}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving smoothness table to CSV: {e}\")\n",
    "\n",
    "    # Print a summary of NaNs for these specific acquisitions\n",
    "    print(f\"\\nSummary of missing Smoothness data for multi-echo acquisitions:\")\n",
    "    for acq in MULTI_ECHO_ACQS:\n",
    "        missing_count = smoothness_pivot[acq].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_subs = smoothness_pivot[smoothness_pivot[acq].isna()].index.tolist()\n",
    "            print(f\"  {acq}: {missing_count} subjects missing data: {missing_subs}\")\n",
    "        else:\n",
    "            print(f\"  {acq}: No missing data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SMOOTHNESS TABLE GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f99a1-8c8c-4b94-9548-028eeaca8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING GROUP-LEVEL DESIGN MATRICES FOR RANDOMISE\")\n",
    "print(\"  (Using Consolidated FD_MEAN and Pre-Processed Smoothness Table)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "\n",
    "# Subject list path\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-openneuro.txt\")\n",
    "\n",
    "# Corrected path for the FD TSV file (original raw source)\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "# Path to the regenerated missing FD mean values\n",
    "RECALCULATED_FD_PATH = os.path.join(BASE_CODE_DIR, 'missing_fd_mean_recalculated.csv')\n",
    "\n",
    "# UPDATED: Path to the new, pre-processed smoothness table CSV\n",
    "SMOOTHNESS_TABLE_PATH = os.path.join(BASE_CODE_DIR, 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "# Define headcoil assignments (from previous kernel)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define acquisition types for which to generate dataframes (these are the consolidated ones)\n",
    "ACQ_TYPES = [\"mb1me4\", \"mb3me4\", \"mb6me4\"] # These are the *target* acq types after consolidation\n",
    "\n",
    "# Output directory for design matrices\n",
    "DESIGN_MATRIX_OUTPUT_DIR = os.path.join(BASE_CODE_DIR, 'design_matrices')\n",
    "os.makedirs(DESIGN_MATRIX_OUTPUT_DIR, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"FD TSV data path (original source): {FD_TSV_PATH}\")\n",
    "print(f\"Recalculated FD data path: {RECALCULATED_FD_PATH}\")\n",
    "print(f\"Smoothness table path (NEW SOURCE): {SMOOTHNESS_TABLE_PATH}\")\n",
    "print(f\"Target acquisition types (for consolidation): {', '.join(ACQ_TYPES)}\")\n",
    "print(f\"Design matrices will be saved to: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load, Truncate, and Consolidate FD Mean Data ---\n",
    "print(\"\\n--- Processing FD Mean Data ---\")\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Filter by subjects in our master list (non-'sp' subjects) and strip 'sub-' for consistency\n",
    "fd_df_filtered_raw = fd_new_df_raw[fd_new_df_raw['Sub'].isin([f\"sub-{s}\" for s in all_subjects])].copy()\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['Sub'].str.replace('sub-', '')\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['subject'].astype(str)\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean for proper aggregation\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = pd.to_numeric(fd_df_filtered_raw['outlier_acq_Custom1'], errors='coerce')\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = fd_df_filtered_raw['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# Create a new 'consolidated_acq' column by stripping echo info\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return acq_str\n",
    "\n",
    "fd_df_filtered_raw['consolidated_acq'] = fd_df_filtered_raw['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules for consolidation\n",
    "aggregation_rules_fd = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any'\n",
    "}\n",
    "\n",
    "# --- Initialize consolidated_fd_df with all expected subject-acq combinations ---\n",
    "# This ensures that even if original data is missing for a combination, a row exists with NaN\n",
    "base_combinations_fd = pd.MultiIndex.from_product([all_subjects, ACQ_TYPES], names=['subject', 'acq']).to_frame(index=False)\n",
    "\n",
    "# Perform the grouping and aggregation on the raw data\n",
    "temp_consolidated_fd_df = fd_df_filtered_raw.groupby(['subject', 'consolidated_acq'], as_index=False).agg(aggregation_rules_fd)\n",
    "temp_consolidated_fd_df = temp_consolidated_fd_df.rename(columns={'consolidated_acq': 'acq'}) # Rename back\n",
    "\n",
    "# Merge with the base combinations to ensure all subject-acq pairs are present\n",
    "consolidated_fd_df = pd.merge(base_combinations_fd, temp_consolidated_fd_df, on=['subject', 'acq'], how='left')\n",
    "\n",
    "\n",
    "print(f\"Original FD data records (after initial subject filter): {len(fd_df_filtered_raw)}\")\n",
    "print(f\"Consolidated FD data records (after echo averaging): {len(temp_consolidated_fd_df)}\")\n",
    "print(f\"Final consolidated_fd_df records (including missing pairs): {len(consolidated_fd_df)}\")\n",
    "print(\"FD data consolidation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2a: Incorporate Recalculated Missing FD Mean Data ---\n",
    "print(\"\\n--- Incorporating Recalculated FD Mean Data ---\")\n",
    "try:\n",
    "    recalculated_fd_df = pd.read_csv(RECALCULATED_FD_PATH)\n",
    "    # Rename 'acquisition' column to 'acq' to match consolidated_fd_df\n",
    "    recalculated_fd_df = recalculated_fd_df.rename(columns={'acquisition': 'acq'})\n",
    "    \n",
    "    # Ensure correct data types for merging/updating\n",
    "    recalculated_fd_df['subject'] = recalculated_fd_df['subject'].astype(str)\n",
    "    recalculated_fd_df['acq'] = recalculated_fd_df['acq'].astype(str)\n",
    "    recalculated_fd_df['fd_mean_recalc'] = pd.to_numeric(recalculated_fd_df['fd_mean'], errors='coerce') \n",
    "\n",
    "    # Before update, count NaNs in fd_mean\n",
    "    nan_count_before_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df BEFORE update: {nan_count_before_update}\")\n",
    "\n",
    "    # Merge the recalculated data into the consolidated dataframe\n",
    "    # This adds a 'fd_mean_recalc' column.\n",
    "    consolidated_fd_df = pd.merge(\n",
    "        consolidated_fd_df,\n",
    "        recalculated_fd_df[['subject', 'acq', 'fd_mean_recalc']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs in the original 'fd_mean' column using values from 'fd_mean_recalc'\n",
    "    # This will only fill if 'fd_mean' is NaN AND 'fd_mean_recalc' is NOT NaN.\n",
    "    consolidated_fd_df['fd_mean'] = consolidated_fd_df['fd_mean'].fillna(\n",
    "        consolidated_fd_df['fd_mean_recalc']\n",
    "    )\n",
    "\n",
    "    # Drop the temporary 'fd_mean_recalc' column\n",
    "    consolidated_fd_df = consolidated_fd_df.drop(columns=['fd_mean_recalc'])\n",
    "\n",
    "    # After update, count NaNs in fd_mean\n",
    "    nan_count_after_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df AFTER update: {nan_count_after_update}\")\n",
    "\n",
    "    if nan_count_before_update > nan_count_after_update:\n",
    "        print(\"  Successfully filled some missing FD mean values from recalculated data.\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update == 0:\n",
    "        print(\"  No missing FD mean values were present to update (already complete).\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update > 0:\n",
    "        print(\"  Warning: Some missing FD mean values still persist. This means the recalculated data also contained NaNs or did not cover all missing entries.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Recalculated FD data file not found at {RECALCULATED_FD_PATH}. Skipping update.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error incorporating recalculated FD data: {e}. Skipping update.\")\n",
    "    # raise # Uncomment for detailed traceback during debugging if needed\n",
    "print(\"Recalculated FD mean data incorporation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Load Pre-Processed Smoothness Data (and Melt) ---\n",
    "print(\"\\n--- Processing Smoothness Data (from pre-processed table) ---\")\n",
    "def load_preprocessed_smoothness_data(csv_path, all_subjects_list, target_acqs_list):\n",
    "    try:\n",
    "        # Read the CSV. The 'subject' column is the index by default if not specified.\n",
    "        # Given the previous kernel output, 'subject' is likely the first column but not the index.\n",
    "        # Let's read it without index_col and explicitly set it later if needed.\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure 'subject' column is recognized correctly\n",
    "        if 'subject' not in data.columns and data.index.name == 'subject':\n",
    "            data = data.reset_index() # If 'subject' was the index, make it a column\n",
    "\n",
    "        # Ensure 'subject' and 'headcoil' are strings for consistency\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data['headcoil'] = data['headcoil'].astype(str)\n",
    "\n",
    "        # Melt the DataFrame from wide to long format\n",
    "        # The columns to melt are the target_acqs_list.\n",
    "        acq_columns_to_melt = [col for col in data.columns if col in target_acqs_list]\n",
    "        \n",
    "        if not acq_columns_to_melt:\n",
    "            print(\"  Warning: No target acquisition columns found in the smoothness table CSV for melting. Returning empty DataFrame for smoothness.\")\n",
    "            return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "            \n",
    "        data_long = pd.melt(\n",
    "            data,\n",
    "            id_vars=['subject', 'headcoil'],\n",
    "            value_vars=acq_columns_to_melt,\n",
    "            var_name='acq',\n",
    "            value_name='smoothness'\n",
    "        )\n",
    "\n",
    "        # Ensure 'smoothness' column is numeric, coercing errors to NaN\n",
    "        data_long['smoothness'] = pd.to_numeric(data_long['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter by our subjects (non-'sp')\n",
    "        data_long = data_long[data_long['subject'].isin(all_subjects_list)].copy()\n",
    "        \n",
    "        # Filter to include only the desired multi-echo acquisitions (redundant with melt, but safe check)\n",
    "        data_long = data_long[data_long['acq'].isin(target_acqs_list)].copy()\n",
    "        \n",
    "        print(f\"  Smoothness records loaded from '{os.path.basename(csv_path)}' and melted: {len(data_long)}\")\n",
    "        print(f\"Loaded and processed {len(data_long)} smoothness records from pre-processed table.\")\n",
    "        return data_long[['subject', 'acq', 'smoothness']] # Only return relevant columns for merge\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Pre-processed smoothness table not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading pre-processed smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # raise # Uncomment this to see the full traceback if the error persists.\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "\n",
    "# Call the updated function to load pre-processed smoothness data\n",
    "smoothness_df = load_preprocessed_smoothness_data(SMOOTHNESS_TABLE_PATH, all_subjects, ACQ_TYPES)\n",
    "\n",
    "\n",
    "# --- Step 4: Generate Headcoil Encoded Data ---\n",
    "print(\"\\n--- Generating Headcoil Data ---\")\n",
    "headcoil_map = {sub_id: 1 if sub_id in HEADCOIL_64_SUBJECTS else 0 for sub_id in all_subjects}\n",
    "headcoil_series = pd.Series(headcoil_map, name='headcoil_encoded').rename_axis('subject').reset_index()\n",
    "headcoil_series['subject'] = headcoil_series['subject'].astype(str)\n",
    "print(f\"Generated headcoil encoding for {len(headcoil_series)} subjects.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Create Design Matrices for Each Acquisition Type ---\n",
    "design_matrices = {}\n",
    "\n",
    "for acq_type in ACQ_TYPES:\n",
    "    print(f\"\\n--- Constructing Design Matrix for {acq_type} ---\")\n",
    "\n",
    "    # Initialize DataFrame with all subjects and the constant 'ones'\n",
    "    df = pd.DataFrame({'subject': all_subjects})\n",
    "    df['ones'] = 1\n",
    "\n",
    "    # Merge Consolidated FD Mean data for the current acq_type\n",
    "    fd_mean_filtered_for_acq = consolidated_fd_df[consolidated_fd_df['acq'] == acq_type][['subject', 'fd_mean']].copy()\n",
    "    df = pd.merge(df, fd_mean_filtered_for_acq, on='subject', how='left')\n",
    "\n",
    "    # Merge Headcoil data (constant for all acq types per subject)\n",
    "    df = pd.merge(df, headcoil_series, on='subject', how='left')\n",
    "\n",
    "    # Ensure correct column types, coercing errors to NaN\n",
    "    df['fd_mean'] = pd.to_numeric(df['fd_mean'], errors='coerce')\n",
    "    df['headcoil_encoded'] = pd.to_numeric(df['headcoil_encoded'], errors='coerce') # Should not have NaNs if all_subjects are covered\n",
    "\n",
    "    # --- De-meaning (FD & Headcoil) and Interaction Calculation ---\n",
    "    # Calculate means, ignoring NaNs. Pandas .mean() skips NaNs by default.\n",
    "    mean_fd_mean = df['fd_mean'].mean()\n",
    "    mean_headcoil_encoded = df['headcoil_encoded'].mean()\n",
    "\n",
    "    # De-mean individual columns. NaNs will remain NaN after subtraction.\n",
    "    df['fd_mean_demeaned'] = df['fd_mean'] - mean_fd_mean\n",
    "    df['headcoil_demeaned'] = df['headcoil_encoded'] - mean_headcoil_encoded\n",
    "\n",
    "    # Calculate raw interaction (will propagate NaNs from its components)\n",
    "    df['interaction_raw'] = df['fd_mean_demeaned'] * df['headcoil_demeaned']\n",
    "    \n",
    "    # De-mean the interaction term itself\n",
    "    mean_interaction_raw = df['interaction_raw'].mean()\n",
    "    df['fd_mean_x_headcoil_demeaned'] = df['interaction_raw'] - mean_interaction_raw\n",
    "\n",
    "\n",
    "    # --- NEW: Append Smoothness Data (after initial dataframe assembly) ---\n",
    "    if not smoothness_df.empty:\n",
    "        smoothness_filtered_for_acq = smoothness_df[smoothness_df['acq'] == acq_type][['subject', 'smoothness']].copy()\n",
    "        \n",
    "        # Merge this filtered smoothness data onto the main df\n",
    "        df = pd.merge(df, smoothness_filtered_for_acq, on='subject', how='left')\n",
    "        \n",
    "        # Ensure smoothness is numeric after merge\n",
    "        df['smoothness'] = pd.to_numeric(df['smoothness'], errors='coerce')\n",
    "\n",
    "        # De-mean smoothness\n",
    "        mean_smoothness = df['smoothness'].mean() # Calculate mean of merged smoothness\n",
    "        df['smoothness_demeaned'] = df['smoothness'] - mean_smoothness\n",
    "    else:\n",
    "        print(f\"  Warning: Pre-processed smoothness data is empty. 'smoothness' and 'smoothness_demeaned' columns will be all NaNs for {acq_type}.\")\n",
    "        df['smoothness'] = np.nan\n",
    "        df['smoothness_demeaned'] = np.nan # Ensure de-meaned column also exists as NaN\n",
    "\n",
    "\n",
    "    # Select and reorder final columns for the design matrix\n",
    "    final_cols = [\n",
    "        'subject',\n",
    "        'ones',\n",
    "        'fd_mean_demeaned',\n",
    "        'headcoil_demeaned',\n",
    "        'fd_mean_x_headcoil_demeaned',\n",
    "        'smoothness_demeaned'\n",
    "    ]\n",
    "    design_matrix_df = df[final_cols].copy()\n",
    "\n",
    "    # --- Drop rows with NaN in fd_mean_demeaned (as requested) ---\n",
    "    original_subject_count_before_fd_drop = len(design_matrix_df)\n",
    "    missing_fd_subjects_before_drop = design_matrix_df[design_matrix_df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    \n",
    "    if missing_fd_subjects_before_drop:\n",
    "        print(f\"  Subjects with missing FD data (before dropping): {len(missing_fd_subjects_before_drop)}\")\n",
    "        print(f\"  Listing missing FD subjects: {missing_fd_subjects_before_drop}\")\n",
    "        design_matrix_df = design_matrix_df.dropna(subset=['fd_mean_demeaned']).copy()\n",
    "        subjects_dropped_fd = original_subject_count_before_fd_drop - len(design_matrix_df)\n",
    "        print(f\"  Dropped {subjects_dropped_fd} subjects due to missing FD data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing FD data found in {acq_type} to drop.\")\n",
    "\n",
    "    # --- NEW: Drop rows with NaN in smoothness_demeaned ---\n",
    "    original_subject_count_after_fd_drop = len(design_matrix_df)\n",
    "    missing_smoothness_subjects_before_drop = design_matrix_df[design_matrix_df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_smoothness_subjects_before_drop:\n",
    "        print(f\"  Subjects with missing Smoothness data (before dropping): {len(missing_smoothness_subjects_before_drop)}\")\n",
    "        print(f\"  Listing missing Smoothness subjects: {missing_smoothness_subjects_before_drop}\")\n",
    "        design_matrix_df = design_matrix_df.dropna(subset=['smoothness_demeaned']).copy()\n",
    "        subjects_dropped_smoothness = original_subject_count_after_fd_drop - len(design_matrix_df)\n",
    "        print(f\"  Dropped {subjects_dropped_smoothness} subjects due to missing Smoothness data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing Smoothness data found in {acq_type} to drop.\")\n",
    "\n",
    "\n",
    "    # Store the DataFrame\n",
    "    design_matrices[acq_type] = design_matrix_df\n",
    "    print(f\"Design Matrix for {acq_type} created. Final number of subjects: {len(design_matrix_df)}\")\n",
    "    # Re-check NaNs after final drops (should be 0 for both)\n",
    "    print(f\"Subjects with missing FD data (after final drop): {design_matrices[acq_type]['fd_mean_demeaned'].isna().sum()}\")\n",
    "    print(f\"Subjects with missing Smoothness data (after final drop): {design_matrices[acq_type]['smoothness_demeaned'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# --- Display Sample Heads ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE HEADS OF GENERATED DESIGN MATRICES (AFTER ALL NaNs DROPPED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    print(f\"\\n--- Design Matrix for {acq_type} (First 10 rows) ---\")\n",
    "    # Display more rows for better inspection, adjust as needed\n",
    "    pd.set_option('display.max_rows', 10) # Show more rows for inspection\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df.head(10)) # Print only first 10 rows\n",
    "    pd.reset_option('display.max_rows') # Reset after printing\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Final Check on NaNs after Dropping ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CHECK: SUBJECTS WITH MISSING FD_MEAN AND SMOOTHNESS DATA (SHOULD BE ZERO AFTER DROP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    missing_fd_subjects_final = df[df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    missing_smoothness_subjects_final = df[df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_fd_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_fd_subjects_final)} subjects still have missing FD data:\")\n",
    "        print(f\"  {missing_fd_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing FD data (as expected).\")\n",
    "\n",
    "    if missing_smoothness_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_smoothness_subjects_final)} subjects still have missing Smoothness data:\")\n",
    "        print(f\"  {missing_smoothness_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing Smoothness data (as expected).\")\n",
    "\n",
    "# --- NEW: Save Design Matrices to CSV files ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"SAVING DESIGN MATRICES TO CSV FILES IN: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    output_filename = os.path.join(DESIGN_MATRIX_OUTPUT_DIR, f'design_matrix_{acq_type}.csv')\n",
    "    try:\n",
    "        # Save without the 'subject' column as index, just as data\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved '{output_filename}' with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "print(\"\\nAll design matrices generated, adjusted, and saved.\")\n",
    "print(\"================================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea02d4-cebb-41d0-b932-87f826e77157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame.applymap has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Series.str.extract has been deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"elementwise comparison failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'n_eff' parameter is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid escape sequence '\\s'\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING GROUP-LEVEL DESIGN MATRICES FOR RANDOMISE\")\n",
    "print(\"  (Using Consolidated FD_MEAN and Pre-Processed Smoothness Table)\")\n",
    "print(\"  *** Ensured proper de-meaning after all filtering ***\") # Highlight this change\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_CODE_DIR = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/code\")\n",
    "\n",
    "# Subject list path\n",
    "SUBLIST_PATH = os.path.join(BASE_CODE_DIR, \"sublist-openneuro.txt\")\n",
    "\n",
    "# Corrected path for the FD TSV file (original raw source)\n",
    "FD_TSV_PATH = os.path.expanduser(\"~/Documents/GitHub/multiecho-pilot/derivatives/Task-sharedreward_Level-Acq_Outlier-info_mriqc-0.16.1.tsv\")\n",
    "\n",
    "# Path to the regenerated missing FD mean values\n",
    "RECALCULATED_FD_PATH = os.path.join(BASE_CODE_DIR, 'missing_fd_mean_recalculated.csv')\n",
    "\n",
    "# UPDATED: Path to the new, pre-processed smoothness table CSV\n",
    "SMOOTHNESS_TABLE_PATH = os.path.join(BASE_CODE_DIR, 'smoothness_multi_echo_table.csv')\n",
    "\n",
    "\n",
    "# Define headcoil assignments (from previous kernel)\n",
    "HEADCOIL_64_SUBJECTS = [\n",
    "    \"10015\", \"10017\", \"10024\", \"10028\", \"10035\", \"10041\", \"10043\", \"10046\",\n",
    "    \"10054\", \"10059\", \"10069\", \"10074\", \"10078\", \"10080\", \"10085\", \"10094\",\n",
    "    \"10108\", \"10125\", \"10130\", \"10136\", \"10137\", \"10142\", \"10150\", \"10154\",\n",
    "    \"10186\", \"10188\", \"10221\"\n",
    "]\n",
    "\n",
    "# Define acquisition types for which to generate dataframes (these are the consolidated ones)\n",
    "ACQ_TYPES = [\"mb1me4\", \"mb3me4\", \"mb6me4\"] # These are the *target* acq types after consolidation\n",
    "\n",
    "# Output directory for design matrices\n",
    "DESIGN_MATRIX_OUTPUT_DIR = os.path.join(BASE_CODE_DIR, 'design_matrices')\n",
    "os.makedirs(DESIGN_MATRIX_OUTPUT_DIR, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "print(f\"Subject list path: {SUBLIST_PATH}\")\n",
    "print(f\"FD TSV data path (original source): {FD_TSV_PATH}\")\n",
    "print(f\"Recalculated FD data path: {RECALCULATED_FD_PATH}\")\n",
    "print(f\"Smoothness table path (NEW SOURCE): {SMOOTHNESS_TABLE_PATH}\")\n",
    "print(f\"Target acquisition types (for consolidation): {', '.join(ACQ_TYPES)}\")\n",
    "print(f\"Design matrices will be saved to: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- Step 1: Load and Prepare Master Subject List (and filter 'sp' subjects) ---\n",
    "try:\n",
    "    with open(SUBLIST_PATH, 'r') as f:\n",
    "        all_subjects_raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter out subjects with 'sp' suffix\n",
    "    all_subjects = [s for s in all_subjects_raw if not s.endswith('sp')]\n",
    "    all_subjects.sort() # Ensure ascending order\n",
    "    \n",
    "    print(f\"Loaded {len(all_subjects_raw)} subjects from {SUBLIST_PATH}.\")\n",
    "    print(f\"Filtered to {len(all_subjects)} subjects (excluding 'sp' suffix).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Subject list file not found at {SUBLIST_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading subject list: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Load, Truncate, and Consolidate FD Mean Data ---\n",
    "print(\"\\n--- Processing FD Mean Data ---\")\n",
    "fd_new_df_raw = None\n",
    "try:\n",
    "    fd_new_df_raw = pd.read_csv(FD_TSV_PATH, sep=r'\\s+')\n",
    "    print(f\"Loaded {len(fd_new_df_raw)} records from {FD_TSV_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New FD data file not found at {FD_TSV_PATH}. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading new FD data: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Filter by subjects in our master list (non-'sp' subjects) and strip 'sub-' for consistency\n",
    "fd_df_filtered_raw = fd_new_df_raw[fd_new_df_raw['Sub'].isin([f\"sub-{s}\" for s in all_subjects])].copy()\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['Sub'].str.replace('sub-', '')\n",
    "fd_df_filtered_raw['subject'] = fd_df_filtered_raw['subject'].astype(str)\n",
    "\n",
    "# Ensure 'outlier_acq_Custom1' is boolean for proper aggregation\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = pd.to_numeric(fd_df_filtered_raw['outlier_acq_Custom1'], errors='coerce')\n",
    "fd_df_filtered_raw['outlier_acq_Custom1'] = fd_df_filtered_raw['outlier_acq_Custom1'].fillna(False).astype(bool)\n",
    "\n",
    "# Create a new 'consolidated_acq' column by stripping echo info\n",
    "def get_consolidated_acq(acq_str):\n",
    "    match = re.match(r'(mb\\dme\\d+)(_echo-\\d+_part-mag)?', acq_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return acq_str\n",
    "\n",
    "fd_df_filtered_raw['consolidated_acq'] = fd_df_filtered_raw['acq'].apply(get_consolidated_acq)\n",
    "\n",
    "# Define aggregation rules for consolidation\n",
    "aggregation_rules_fd = {\n",
    "    'tsnr': 'mean',\n",
    "    'fd_mean': 'mean',\n",
    "    'outlier_acq_Custom1': 'any'\n",
    "}\n",
    "\n",
    "# --- Initialize consolidated_fd_df with all expected subject-acq combinations ---\n",
    "# This ensures that even if original data is missing for a combination, a row exists with NaN\n",
    "base_combinations_fd = pd.MultiIndex.from_product([all_subjects, ACQ_TYPES], names=['subject', 'acq']).to_frame(index=False)\n",
    "\n",
    "# Perform the grouping and aggregation on the raw data\n",
    "temp_consolidated_fd_df = fd_df_filtered_raw.groupby(['subject', 'consolidated_acq'], as_index=False).agg(aggregation_rules_fd)\n",
    "temp_consolidated_fd_df = temp_consolidated_fd_df.rename(columns={'consolidated_acq': 'acq'}) # Rename back\n",
    "\n",
    "# Merge with the base combinations to ensure all subject-acq pairs are present\n",
    "consolidated_fd_df = pd.merge(base_combinations_fd, temp_consolidated_fd_df, on=['subject', 'acq'], how='left')\n",
    "\n",
    "\n",
    "print(f\"Original FD data records (after initial subject filter): {len(fd_df_filtered_raw)}\")\n",
    "print(f\"Consolidated FD data records (after echo averaging): {len(temp_consolidated_fd_df)}\")\n",
    "print(f\"Final consolidated_fd_df records (including missing pairs): {len(consolidated_fd_df)}\")\n",
    "print(\"FD data consolidation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2a: Incorporate Recalculated Missing FD Mean Data ---\n",
    "print(\"\\n--- Incorporating Recalculated FD Mean Data ---\")\n",
    "try:\n",
    "    recalculated_fd_df = pd.read_csv(RECALCULATED_FD_PATH)\n",
    "    # Rename 'acquisition' column to 'acq' to match consolidated_fd_df\n",
    "    recalculated_fd_df = recalculated_fd_df.rename(columns={'acquisition': 'acq'})\n",
    "    \n",
    "    # Ensure correct data types for merging/updating\n",
    "    recalculated_fd_df['subject'] = recalculated_fd_df['subject'].astype(str)\n",
    "    recalculated_fd_df['acq'] = recalculated_fd_df['acq'].astype(str)\n",
    "    recalculated_fd_df['fd_mean_recalc'] = pd.to_numeric(recalculated_fd_df['fd_mean'], errors='coerce') \n",
    "\n",
    "    # Before update, count NaNs in fd_mean\n",
    "    nan_count_before_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df BEFORE update: {nan_count_before_update}\")\n",
    "\n",
    "    # Merge the recalculated data into the consolidated dataframe\n",
    "    # This adds a 'fd_mean_recalc' column.\n",
    "    consolidated_fd_df = pd.merge(\n",
    "        consolidated_fd_df,\n",
    "        recalculated_fd_df[['subject', 'acq', 'fd_mean_recalc']],\n",
    "        on=['subject', 'acq'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs in the original 'fd_mean' column using values from 'fd_mean_recalc'\n",
    "    # This will only fill if 'fd_mean' is NaN AND 'fd_mean_recalc' is NOT NaN.\n",
    "    consolidated_fd_df['fd_mean'] = consolidated_fd_df['fd_mean'].fillna(\n",
    "        consolidated_fd_df['fd_mean_recalc']\n",
    "    )\n",
    "\n",
    "    # Drop the temporary 'fd_mean_recalc' column\n",
    "    consolidated_fd_df = consolidated_fd_df.drop(columns=['fd_mean_recalc'])\n",
    "\n",
    "    # After update, count NaNs in fd_mean\n",
    "    nan_count_after_update = consolidated_fd_df['fd_mean'].isna().sum()\n",
    "    print(f\"  FD data NaNs in consolidated_fd_df AFTER update: {nan_count_after_update}\")\n",
    "\n",
    "    if nan_count_before_update > nan_count_after_update:\n",
    "        print(\"  Successfully filled some missing FD mean values from recalculated data.\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update == 0:\n",
    "        print(\"  No missing FD mean values were present to update (already complete).\")\n",
    "    elif nan_count_before_update == nan_count_after_update and nan_count_before_update > 0:\n",
    "        print(\"  Warning: Some missing FD mean values still persist. This means the recalculated data also contained NaNs or did not cover all missing entries.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Recalculated FD data file not found at {RECALCULATED_FD_PATH}. Skipping update.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error incorporating recalculated FD data: {e}. Skipping update.\")\n",
    "    # raise # Uncomment for detailed traceback during debugging if needed\n",
    "print(\"Recalculated FD mean data incorporation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Load Pre-Processed Smoothness Data (and Melt) ---\n",
    "print(\"\\n--- Processing Smoothness Data (from pre-processed table) ---\")\n",
    "def load_preprocessed_smoothness_data(csv_path, all_subjects_list, target_acqs_list):\n",
    "    try:\n",
    "        # Read the CSV. The 'subject' column is the index by default if not specified.\n",
    "        # Given the previous kernel output, 'subject' is likely the first column but not the index.\n",
    "        # Let's read it without index_col and explicitly set it later if needed.\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Ensure 'subject' column is recognized correctly\n",
    "        if 'subject' not in data.columns and data.index.name == 'subject':\n",
    "            data = data.reset_index() # If 'subject' was the index, make it a column\n",
    "\n",
    "        # Ensure 'subject' and 'headcoil' are strings\n",
    "        data['subject'] = data['subject'].astype(str)\n",
    "        data['headcoil'] = data['headcoil'].astype(str) # Keep headcoil as string\n",
    "\n",
    "        # Melt the DataFrame from wide to long format\n",
    "        # The columns to melt are the target_acqs_list.\n",
    "        acq_columns_to_melt = [col for col in data.columns if col in target_acqs_list]\n",
    "        \n",
    "        if not acq_columns_to_melt:\n",
    "            print(\"  Warning: No target acquisition columns found in the smoothness table CSV for melting. Returning empty DataFrame for smoothness.\")\n",
    "            return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "            \n",
    "        data_long = pd.melt(\n",
    "            data,\n",
    "            id_vars=['subject', 'headcoil'],\n",
    "            value_vars=acq_columns_to_melt,\n",
    "            var_name='acq',\n",
    "            value_name='smoothness'\n",
    "        )\n",
    "\n",
    "        # Ensure 'smoothness' column is numeric, coercing errors to NaN\n",
    "        data_long['smoothness'] = pd.to_numeric(data_long['smoothness'], errors='coerce')\n",
    "\n",
    "        # Filter by our subjects (non-'sp')\n",
    "        data_long = data_long[data_long['subject'].isin(all_subjects_list)].copy()\n",
    "        \n",
    "        # Filter to include only the desired multi-echo acquisitions (redundant with melt, but safe check)\n",
    "        data_long = data_long[data_long['acq'].isin(target_acqs_list)].copy()\n",
    "        \n",
    "        print(f\"  Smoothness records loaded from '{os.path.basename(csv_path)}' and melted: {len(data_long)}\")\n",
    "        print(f\"Loaded and processed {len(data_long)} smoothness records from pre-processed table.\")\n",
    "        return data_long[['subject', 'acq', 'smoothness']] # Only return relevant columns for merge\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Pre-processed smoothness table not found at {csv_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading pre-processed smoothness data: {str(e)}. Returning empty DataFrame.\")\n",
    "        # raise # Uncomment this to see the full traceback if the error persists.\n",
    "        return pd.DataFrame(columns=['subject', 'acq', 'smoothness'])\n",
    "\n",
    "# Call the updated function to load pre-processed smoothness data\n",
    "smoothness_df = load_preprocessed_smoothness_data(SMOOTHNESS_TABLE_PATH, all_subjects, ACQ_TYPES)\n",
    "\n",
    "\n",
    "# --- Step 4: Generate Headcoil Encoded Data ---\n",
    "print(\"\\n--- Generating Headcoil Data ---\")\n",
    "headcoil_map = {sub_id: 1 if sub_id in HEADCOIL_64_SUBJECTS else 0 for sub_id in all_subjects}\n",
    "headcoil_series = pd.Series(headcoil_map, name='headcoil_encoded').rename_axis('subject').reset_index()\n",
    "headcoil_series['subject'] = headcoil_series['subject'].astype(str)\n",
    "print(f\"Generated headcoil encoding for {len(headcoil_series)} subjects.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Create Design Matrices for Each Acquisition Type ---\n",
    "design_matrices = {}\n",
    "\n",
    "for acq_type in ACQ_TYPES:\n",
    "    print(f\"\\n--- Constructing Design Matrix for {acq_type} ---\")\n",
    "\n",
    "    # Initialize DataFrame with all subjects and the constant 'ones'\n",
    "    df = pd.DataFrame({'subject': all_subjects})\n",
    "    df['ones'] = 1\n",
    "\n",
    "    # Merge Consolidated FD Mean data for the current acq_type\n",
    "    fd_mean_filtered_for_acq = consolidated_fd_df[consolidated_fd_df['acq'] == acq_type][['subject', 'fd_mean']].copy()\n",
    "    df = pd.merge(df, fd_mean_filtered_for_acq, on='subject', how='left')\n",
    "\n",
    "    # Merge Headcoil data (constant for all acq types per subject)\n",
    "    df = pd.merge(df, headcoil_series, on='subject', how='left')\n",
    "\n",
    "    # Ensure raw columns are numeric, coercing errors to NaN\n",
    "    df['fd_mean'] = pd.to_numeric(df['fd_mean'], errors='coerce')\n",
    "    df['headcoil_encoded'] = pd.to_numeric(df['headcoil_encoded'], errors='coerce')\n",
    "\n",
    "    # --- Append Smoothness Data (after initial dataframe assembly) ---\n",
    "    if not smoothness_df.empty:\n",
    "        smoothness_filtered_for_acq = smoothness_df[smoothness_df['acq'] == acq_type][['subject', 'smoothness']].copy()\n",
    "        \n",
    "        # Merge this filtered smoothness data onto the main df\n",
    "        df = pd.merge(df, smoothness_filtered_for_acq, on='subject', how='left')\n",
    "        \n",
    "        # Ensure smoothness is numeric after merge\n",
    "        df['smoothness'] = pd.to_numeric(df['smoothness'], errors='coerce')\n",
    "\n",
    "    else:\n",
    "        print(f\"  Warning: Pre-processed smoothness data is empty. 'smoothness' column will be all NaNs for {acq_type}.\")\n",
    "        df['smoothness'] = np.nan # Ensure column exists as NaN\n",
    "\n",
    "\n",
    "    # --- First, drop rows with NaN in fd_mean_demeaned (using raw 'fd_mean' for dropping) ---\n",
    "    # We are dropping before de-meaning to ensure means are calculated on the final set of subjects\n",
    "    original_subject_count_before_drops = len(df)\n",
    "    \n",
    "    # Identify subjects with missing FD data for reporting\n",
    "    # Note: We are now dropping based on the raw `fd_mean` before de-meaning for a clean set\n",
    "    missing_fd_subjects_to_drop = df[df['fd_mean'].isna()]['subject'].tolist()\n",
    "    \n",
    "    if missing_fd_subjects_to_drop:\n",
    "        print(f\"  Subjects with missing FD data (will be dropped): {len(missing_fd_subjects_to_drop)}\")\n",
    "        print(f\"  Listing missing FD subjects: {missing_fd_subjects_to_drop}\")\n",
    "        df = df.dropna(subset=['fd_mean']).copy() # Drop NaNs in the raw fd_mean\n",
    "        subjects_dropped_fd = original_subject_count_before_drops - len(df)\n",
    "        print(f\"  Dropped {subjects_dropped_fd} subjects due to missing FD data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing FD data found in {acq_type} to drop.\")\n",
    "\n",
    "\n",
    "    # --- NEW: Second, drop rows with NaN in smoothness (using raw 'smoothness' for dropping) ---\n",
    "    original_subject_count_after_fd_drop = len(df)\n",
    "    missing_smoothness_subjects_to_drop = df[df['smoothness'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_smoothness_subjects_to_drop:\n",
    "        print(f\"  Subjects with missing Smoothness data (will be dropped): {len(missing_smoothness_subjects_to_drop)}\")\n",
    "        print(f\"  Listing missing Smoothness subjects: {missing_smoothness_subjects_to_drop}\")\n",
    "        df = df.dropna(subset=['smoothness']).copy() # Drop NaNs in the raw smoothness\n",
    "        subjects_dropped_smoothness = original_subject_count_after_fd_drop - len(df)\n",
    "        print(f\"  Dropped {subjects_dropped_smoothness} subjects due to missing Smoothness data.\")\n",
    "    else:\n",
    "        print(f\"  No subjects with missing Smoothness data found in {acq_type} to drop.\")\n",
    "\n",
    "    # Now that we have the final set of subjects without NaNs for main regressors, perform de-meaning.\n",
    "    # --- IMPORTANT: Re-calculate means on the *final* filtered DataFrame ---\n",
    "    mean_fd_mean = df['fd_mean'].mean()\n",
    "    mean_headcoil_encoded = df['headcoil_encoded'].mean()\n",
    "    mean_smoothness = df['smoothness'].mean()\n",
    "\n",
    "    # De-mean individual columns. Since NaNs were dropped, these should now be fully numerical.\n",
    "    df['fd_mean_demeaned'] = df['fd_mean'] - mean_fd_mean\n",
    "    df['headcoil_demeaned'] = df['headcoil_encoded'] - mean_headcoil_encoded\n",
    "    df['smoothness_demeaned'] = df['smoothness'] - mean_smoothness\n",
    "\n",
    "    # Calculate raw interaction (will propagate NaNs from its components)\n",
    "    df['interaction_raw'] = df['fd_mean_demeaned'] * df['headcoil_demeaned']\n",
    "    \n",
    "    # De-mean the interaction term itself\n",
    "    mean_interaction_raw = df['interaction_raw'].mean()\n",
    "    df['fd_mean_x_headcoil_demeaned'] = df['interaction_raw'] - mean_interaction_raw\n",
    "\n",
    "    # Select and reorder final columns for the design matrix\n",
    "    final_cols = [\n",
    "        'subject',\n",
    "        'ones',\n",
    "        'fd_mean_demeaned',\n",
    "        'headcoil_demeaned',\n",
    "        'fd_mean_x_headcoil_demeaned',\n",
    "        'smoothness_demeaned'\n",
    "    ]\n",
    "    design_matrix_df = df[final_cols].copy()\n",
    "\n",
    "    # Store the DataFrame\n",
    "    design_matrices[acq_type] = design_matrix_df\n",
    "    print(f\"Design Matrix for {acq_type} created. Final number of subjects: {len(design_matrix_df)}\")\n",
    "    # Re-check NaNs after final drops (should be 0 for both)\n",
    "    print(f\"Subjects with missing FD data (after final drop): {design_matrices[acq_type]['fd_mean_demeaned'].isna().sum()}\")\n",
    "    print(f\"Subjects with missing Smoothness data (after final drop): {design_matrices[acq_type]['smoothness_demeaned'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# --- Display Sample Heads ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE HEADS OF GENERATED DESIGN MATRICES (AFTER ALL NaNs DROPPED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    print(f\"\\n--- Design Matrix for {acq_type} (First 10 rows) ---\")\n",
    "    # Display more rows for better inspection, adjust as needed\n",
    "    pd.set_option('display.max_rows', 10) # Show more rows for inspection\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df.head(10)) # Print only first 10 rows\n",
    "    pd.reset_option('display.max_rows') # Reset after printing\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Final Check on NaNs after Dropping ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CHECK: SUBJECTS WITH MISSING FD_MEAN AND SMOOTHNESS DATA (SHOULD BE ZERO AFTER DROP)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    missing_fd_subjects_final = df[df['fd_mean_demeaned'].isna()]['subject'].tolist()\n",
    "    missing_smoothness_subjects_final = df[df['smoothness_demeaned'].isna()]['subject'].tolist()\n",
    "\n",
    "    if missing_fd_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_fd_subjects_final)} subjects still have missing FD data:\")\n",
    "        print(f\"  {missing_fd_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing FD data (as expected).\")\n",
    "\n",
    "    if missing_smoothness_subjects_final:\n",
    "        print(f\"For {acq_type}: WARNING! {len(missing_smoothness_subjects_final)} subjects still have missing Smoothness data:\")\n",
    "        print(f\"  {missing_smoothness_subjects_final}\")\n",
    "    else:\n",
    "        print(f\"For {acq_type}: No subjects with missing Smoothness data (as expected).\")\n",
    "\n",
    "# --- Save Design Matrices to CSV files ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"SAVING DESIGN MATRICES TO CSV FILES IN: {DESIGN_MATRIX_OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for acq_type, df in design_matrices.items():\n",
    "    output_filename = os.path.join(DESIGN_MATRIX_OUTPUT_DIR, f'design_matrix_{acq_type}.csv')\n",
    "    try:\n",
    "        # Save without the 'subject' column as index, just as data\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved '{output_filename}' with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "print(\"\\nAll design matrices generated, adjusted, and saved.\")\n",
    "print(\"================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddc9b5-4454-4c52-ada4-e3cb7b20b504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
